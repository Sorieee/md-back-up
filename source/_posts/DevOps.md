---
title: DevOps
date: 2021-06-10 10:57:43
tags:


---

# 持续交付: 发布可靠软件的系统方法-Start

## 引言

​	部署流水线以持续集成过程为其理论基石，从本质上讲，它是采纳持续集成原理后的自然结果。

​	部署流水线的目标有三个。首先，它让软件构建、部署、测试和发布过程对所有人可见，促进了合作。其次，它改善了反馈，以便在整个过程中，我们能够更早地发现并解决问题。最后，它使团队能够通过一个完全自动化的过程在任意环境上部署和发布软件的任意版本。

## 一些常见的发布反模式

### 反模式: 手工部署软件

​	这种反模式的特征如下。

* 有一份非常详尽的文档，该文档描述了执行步骤及每个步骤中易出错的地方。
* 以手工测试来确认该应用程序是否运行正确。
* 在发布当天开发团队频繁地接到电话，客户要求解释部署为何会出错。
* 在发布时，常常会修正一些在发布过程中发现的问题。
* 如果是集群环境部署，常常发现在集群中各环境的配置都不相同，比如应用服务器的连接池设置不同或文件系统有不同的目录结构等。
* 发布过程需要较长的时间（超过几分钟）。
* 布结果不可预测，常常不得不回滚或遇到不可预见的问题。
* 发布之后凌晨两点还睡眼惺忪地坐在显示器前，绞尽脑汁想着怎么让刚刚部署的应用程序能够正常工作。



​	相反，随着时间的推移，部署应该走向完全自动化，即对于那些负责将应用程序部署到开发环境、测试环境或生产环境的人来说，应该只需要做两件事：（1）挑选版本及需要部署的环境，（2）按一下“部署”按钮。对于套装软件的发布来说，还应该有一个创建安装程序的自动化过程。

* 如果部署过程没有完全自动化，每次部署时都会发生错误。唯一的问题就是“该问题严重与否”而已。即便使用良好的部署测试，有些错误也很难追查。
* 如果部署过程不是自动化的，那么它就既不可重复也不可靠，就会在调试部署错误的过程中浪费很多时间。
* 手动部署流程不得不被写在文档里。可是文档维护是一项复杂而费时的任务，它涉及多人之间的协作，因此文档通常要么是不完整的，要么就是未及时更新的，而把一套自动化部署脚本作为文档，它就永远是最新且完整的，否则就无法进行部署工作了。
* 自动部署本质上也是鼓励协作的，因为所有内容都在一个脚本里，一览无遗。要读懂文档通常需要读者具备一定的知识水平。然而在现实中，文档通常只是为执行部署者写的备忘录，是难以被他人理解的。
* 以上几点引起的一个必然结果：手工部署过程依赖于部署专家。如果专家去度假或离职了，那你就有麻烦了。
* 尽管手工部署枯燥且极具重复性，但仍需要有相当程度的专业知识。若要求专家做这些无聊、重复，但有技术要求的任务则必定会出现各种我们可以预料到的人为失误，同时失眠，酗酒这种问题也会接踵而至。然而自动化部署可以把那些成本高昂的资深高技术人员从过度工作中解放出来，让他们投身于更高价值的工作活动当中。
* 对手工部署过程进行测试的唯一方法就是原封不动地做一次（或者几次）。这往往费时，还会造成高昂的金钱成本，而测试自动化的部署过程却是既便宜又容易。
* 另外，还有一种说法：自动化过程不如手工过程的可审计性好。我们对这个观点感到很疑惑。对于一个手工过程来说，没人能确保其执行者会非常严格地遵循文档完成操作。只有自动化过程是完全可审核的。有什么会比一个可工作的部署脚本更容易被审核的呢？
* 每个人都应该使用自动化部署过程，而且它应该是软件部署的唯一方式。这个准则可以确保：在需要部署时，部署脚本就能完成工作。在本书中我们会提到多个原则，而其中之一就是“使用相同的脚本将软件部署到各种环境上”。如果使用相同的脚本将软件部署到各类环境中，那么在发布当天需要向生产环境进行部署时，这个脚本已经被验证过成百上千次了。如果发布时出现任何问题的话，你可以百分百地确定是该环境的具体配置问题，而不是这个脚本的问题。

### 反模式: 开发完成之后才向类生产环境部署

​	在这一模式下，当软件被第一次部署到类生产环境（比如试运行环境）时，就是大部分开发工作完成时，至少是开发团队认为“该软件开发完成了”。这种模式中，经常出现下面这些情况。

* 如果测试人员一直参与了在此之前的过程，那么他们已在开发机器上对软件进行了测试。
* 只有在向试运行环境部署时，运维人员才第一次接触到这个新应用程序。在某些组织中，通常是由独立的运维团队负责将应用程序部署到试运行环境和生产环境。在这种工作方式下，运维人员只有在产品被发布到生产环境时才第一次见到这个软件。
* 有可能由于类生产环境非常昂贵，所以权限控制严格，操作人员自己无权对该环境进行操作，也有可能环境没有按时准备好，甚至也可能根本没人去准备环境。
* 开发团队将正确的安装程序、配置文件、数据库迁移脚本和部署文档一同交给那些真正执行部署任务的人员，而所有这些都没有在类生产环境或试运行环境中进行过测试
* 开发团队和真正执行部署任务的人员之间的协作非常少。



​	以下这些事情会使与发布相关的问题恶化。

* 假如一个应用程序是全新开发的，那么第一次将它部署到试运行环境时，可能会非常棘手。
* 发布周期越长，开发团队在部署前作出错误假设的时间就越长，修复这些问题的时间也就越长。
* 交付过程被划分到开发、DBA、运维、测试等部门的那些大型组织中，各部门之间的协作成本可能会非常高，有时甚至会将发布过程拖上“地狱列车”。此时为了完成某个部署任务（更糟糕的情况是，为了解决部署过程中出现的问题），开发人员、测试人员和运维人员总是高举着问题单（不断地互发电子邮件）。
* 开发环境与生产环境差异性越大，开发过程中所做的那些假设与现实之间的差距就越大。虽然很难量化，但我敢说，如果在Windows系统上开发软件，而最终要部署在Solaris集群上，那么你会遇到很多意想不到的事情。
* 如果应用程序是由用户自行安装的（你可能没有太多权限来对用户的环境进行操作），或者其中的某些组件不在企业控制范围之内，此时可能需要很多额外的测试工作。



​	我们的对策就是将测试、部署和发布活动也纳入到开发过程中，让它们成为开发流程正常的一部分。这样的话，当准备好进行系统发布时就几乎很少或不会有风险了，因为你已经在很多种环境，甚至类生产环境中重复过很多次，也就相当于测试过很多次了。

### 反模式：生产环境的手工配置管理

* 多次部署到试运行环境都非常成功，但当部署到生产环境时就失败。
* 集群中各节点的行为有所不同。例如，与其他节点相比，某个节点所承担的负载少一些，或者处理请求的时间花得多一些。
* 运维团队需要较长时间为每次发布准备环境。
* 系统无法回滚到之前部署的某个配置，这些配置包括操作系统、应用服务器、关系型数据库管理系统、Web服务器或其他基础设施设置。
* 不知道从什么时候起，集群中的某些服务器所用的操作系统、第三方基础设施、依赖库的版本或补丁级别就不同了。
* 直接修改生产环境上的配置来改变系统配置。



​	本书描述的关键实践之一就是配置管理，其责任之一就是让你能够重复地创建那些你开发的应用程序所依赖的每个基础设施。这意味着操作系统、补丁级别、操作系统配置、应用程序所依赖的其他软件及其配置、基础设施的配置等都应该处于受控状态。你应该具有重建生产环境的能力，最好是能通过自动化的方式重建生产环境。虚拟化技术在这一点上可能对你有所帮助。

​	你应该完全掌握生产环境中的任何信息。这意味着生产环境中的每次变更都应该被记录下来，而且做到今后可以查阅。部署失败经常是因为某个人在上次部署时为生产环境打了补丁，但却没有将这个修改记录下来。实际上，不应该允许手工改变测试环境、试运行环境和生产环境，而只允许通过自动化过程来改变这些环境。

​	应用软件之间通常会有一些依赖关系。我们应该很容易知道当前发布的是软件的哪个版本。

​	发布可能是一件令人兴奋的事情，也可能变成一件累人而又沉闷的工作。几乎在每次发布的最后都会有一些变更，比如修改数据库的登录账户或者更新所用外部服务的URL。我们应该使用某种方法来引入此类变更，以便这些变更可以被记录并测试。这里我们再次强调一下，自动化是关键。变更首先应该被提交到版本控制系统中，然后通过某个自动化过程对生产环境进行更新。

## 如何实现目标

​	我们来调整一下目标，即找到可以以一种高效、快速、可靠的方式交付高质量且有价值的软件的方法。

* 自动化。如果构建、部署、测试和发布流程不是自动化的，那它就是不可重复的。由于软件本身、系统配置、环境以及发布过程的不同，每次做完这些活动以后，其结果可能都会有所不同。由于每个步骤都是手工操作，所以出错的机会很大，而且无法确切地知道具体都做了什么。这意味着整个发布过程无法得到应有的控制来确保高质量。常常说软件发布像是一种艺术，但事实上，它应该是一种工程学科。
* 频繁做。如果能够做到频繁发布，每个发布版本之间的差异会很小。这会大大减少与发布相关的风险，且更容易回滚。频繁发布也会加快反馈速度，而客户也需要它。本书很多内容都聚焦于如何尽快得到对软件及其相关配置所做变化的反馈，这包括其环境、部署过程及数据等。



​	对于频繁地自动化发布来说，反馈是至关重要的。下面关于反馈的三个标准是很有用的：

* 无论什么样的修改都应该触发反馈流程；
* 反馈应该尽快发出；
* 交付团队必须接收反馈，并依据它作出相应的行动。

### 每次修改都应该触发反馈流程

​	一个可工作的软件可分成以下几个部分：可执行的代码、配置信息、运行环境和数据。如果其中任何一部分发生了变化，都可能导致软件的行为发生变化。所以我们要能够控制这四部分，并确保任何修改都会被验证。

​	什么是反馈流程？它是指完全以自动化方式尽可能地测试每一次变更。根据系统的不同，测试会有所不同，但通常至少包括下面的检测。

* 建可执行代码的流程必须是能奏效的。这用于验证源代码是否符合语法。
* 软件的单元测试必须是成功的。这可以检查应用程序的行为是否与期望相同。
* 软件应该满足一定的质量标准，比如测试覆盖率以及其他与技术相关的度量项。
* 软件的功能验收测试必须是成功的。这可以检查应用是否满足业务验收条件，交付了所期望的业务价值。
* 软件的非功能测试必须是成功的。这可以检查应用程序是否满足用户对性能、有效性、安全性等方面的要求。
* 软件必须通过了探索性测试，并给客户以及部分用户做过演示。这些通常在一个手工测试环境上完成。此时，产品负责人可能认为软件功能还有缺失，我们自己也可能发现需要修复的缺陷，还要为其写自动化测试来避免回归测试。



### 必须尽快接收反馈

​	实现这样的部署流水线是需要大量资源的，尤其是当有了全面的自动化测试套件之后。部署流水线的关键目的之一就是对人力资源利用率的优化：我们希望将人力释放出来做更有价值的工作，将那些重复性的体力活交给机器来做。

​	对于整个流水线中的提交（commit）阶段，其测试应具有如下特征。

* 运行速度快。
* 尽可能全面，即75%左右的代码库覆盖率。只有这样，这些测试通过以后，我们才对自己写的软件比较有信心。
* 如果有测试失败的话，就表明应用程序有严重问题，无论如何都不能发布。也就是说，像检查界面元素的颜色是否正确这类测试不应该包含在这个测试集合当中。
* 尽可能做到环境中立。这个环境没必要和生产环境一模一样，可以相对简单廉价一些。

​	

相对而言，提交阶段之后的测试一般有如下这些特点。

* 它们通常运行更慢一些，所以适合于并行执行。
* 即使某些测试有可能失败，但在某种场合下，我们还是会发布应用程序。比如某个即将发布的版本有一个不稳定的修复，会导致其性能低于预先定义的标准，但有时我们还是会决定发布这个版本。
* 它们的运行环境应该尽可能与生产环境相同。除了测试功能以外，它同时还会对部署过程以及对生产环境的任何修改进行测试。



​	这种方法的基础之一就是快速的反馈。为了确保对变更的快速反馈，我们就要注意开发软件的流程，特别是如何使用版本控制系统和如何组织代码。开发人员应该频繁提交代码到版本控制系统中，像管理大规模团队或分布式团队那样，将代码分成多个组件。

### 交付团队必须接收反馈并作出反应

​	想要能够根据反馈来调整行动，就要对信息进行广播。使用一个大且可视的仪表盘（并非一定要电子的），或者其他通知机制对于确保反馈送达到每一个人是极为重要的。这个仪表盘应该随处可见，而且至少每个团队的屋中都应放置一个。当然，如果最后没有引发什么改进行动，反馈也就没有什么用了。因此，这就要求纪律性和计划性。当需要采取行动时，整个团队有责任停下他们手中的事情，来决定接下来采取哪些行动。在完成此事之后，团队才能继续自己的工作

### 这个流程可以推广吗

​	略

## 收效

### 授权团队

​	我们常常看到在不同的环境中运行着不同的版本，而不同角色的人工作在其上。能够轻松地将任意版本的软件部署到任意环境的能力能带来很多好处。

* 测试人员可以选择性地部署较旧的版本，以验证新版本上的功能变化。
* 技术支持人员可以自己部署某个已发布的版本，用于重现缺陷。
* 技术支持人员可以自己部署某个已发布的版本，用于重现缺陷。
* 发布方式也变成一键式的了。

### 减少错误

​	我们可能从方方面面将错误引入到软件中。最初委托制作这个软件的人就可能出错，比如提出错误的需求。需求分析人员可能将需求理解错了，开发人员也可能写出了到处都是缺陷的程序，而我们在这里要说的错误是指由不良好的配置管理引入到生产环境的错误。我们将在第2章详细阐述配置管理。现在，让我们想一下到底需要哪些东西才可以让一个应用程序正确地工作，当然肯定需要正确版本的代码，除此之外呢？我们还需要数据库模式（schema）的正确版本、负载均衡器的正确配置信息、应用程序所依赖的Web服务（比如用于查阅价格的Web服务）的正确URL等。当我们说配置管理时，指的是让你识别并控制一组完整信息的流程与机制，这些信息包括每个字节和比特。

### 缓解压力

​		现在，让我们来设想一下。如果接下来的发布只需要单击一下按钮，而且只需要等上几分钟，甚至几秒钟内就可以完成。另外，假如发生了非常糟糕的事情，你只要花上相同的几分钟或几秒钟的时间就可以把刚部署的内容恢复到从前的老样子。再大胆地设想一下，假如你的软件发布周期总是很短，那么当前生产环境中的版本与新版本之间的差异应该非常小。如果上述设想都是事实的话，那么发布的风险一定会大大降低，那种将职业生涯压注在发布是否成功的不爽感觉也将大大减少。

### 部署的灵活性	

​	只要需要，就可以让软件运行在任何环境中”的能力使我们和客户对我们随时管理所有版本发布过程充满信心。

### 多加练习，使其完美

​	只有一种环境可以有多变性，那就是开发环境。开发人员应该在自己的开发环境中自行生成二进制文件，而不需要在别处构建生成。所以，对这种开发环境的部署流程要求太严格是没有必要的。虽然我们能够做到在开发人员的开发机器上也以同样的方式部署软件，但实际上对开发环境的部署没有必要严格要求

## 候选发布版本

​	大多数软件发布方法都是在其流程的最后阶段才能识别出可以发布的那些版本。当说到与跟踪（tracking）相关的工作时，这是有些意义的。在写作本书时，Wikipedia上对开发阶段的描述中将“候选发布版本”作为这一流程中的一个步骤进行了说明，如图1-2所示。我们的观点则稍有不同。

![](https://pic.imgdb.cn/item/60c1a418844ef46bb289ead2.jpg)

​	根据我们的经验，直到开发阶段之后才做测试的话，无疑会降低应用程序的质量。最好还是在缺陷被引入时，就发现并将其解决。发现得越晚，修复的成本越高。开发人员已经不记得他们是在实现哪个功能时把缺陷引入的，而这个功能很可能已经发生了变化。直到最后才做测试，这通常意味着没有足够的时间真正地修复缺陷，或者只能修复其中很少的一部分缺陷。因此，我们想尽早地发现并修正这些缺陷，最好是在将其提交到代码库之前。

**每次提交代码都可能产生一个可发布的版本**

​	我们应该频繁做集成，事实上应该在每次提交修改后都做集成。持续集成这个实践将频繁集成发挥到了极至，而“持续集成”转变了软件开发过程。持续集成会及时检测到任何一次破坏已有系统或者不满足客户验收测试的提交。一旦发生这种情况，团队就立刻去修复问题（这是持续集成的首要规则）。如果能够坚持这个实践，那么软件会一直处于可用状态。假如测试足够全面，且运行测试的环境与生产环境足够相近（甚至相同）的话，那么可以说，你的软件一直处于可发布状态。

##  软件交付的原则

### 为软件的发布创建一个可重复且可靠的过程

​	可重复性和可靠性来自于以下两个原则：

（1）几乎将所有事情自动化；

（2）将构建、部署、测试和发布软件所需的东西全部纳入到版本控制管理之中。

归根结底，软件部署包括三件事：

* 提供并管理你的软件所需要的运行环境，这包括硬件配置、所依赖的软件、基础设施以及所需的外部服务；
* 将你的应用程序的正确版本安装在其之上；
* 配置你的应用程序，包括它所需要的任何数据以及状态。

### 将几乎所有事情自动化

​	看上去自动化发布流程是一个令人怯步的工作，而手工完成这些事情显得更容易一些。如果我们只需要做一次这样的工作，通过手工执行的确非常容易，但如果需要执行这个流程数十次的话，就不是那么容易的事了，而且很可能在第三次或第四次的时候就感觉不那么容易了。

### 把所有的东西都纳入版本控制

​	将构建、部署、测试和发布的整个过程中所需的东西全部保存在某种形式的版本存储库中，包括需求文档、测试脚本、自动化测试用例、网络配置脚本、部署脚本、数据库创建、升级、回滚和初始化脚本、应用程序所依赖的软件集合的配置脚本、库文件、工具链以及技术文档等。所有这些内容都应该受到版本控制，与每次构建结果相关的版本都应可以识别。也就是说，这些变更集（change set）都应该有唯一标识，比如构建号、版本控制库中的版本号。

### 提前并频繁地做让你感到痛苦的事

​	这是最通用的原则，也是最有启发性的。在软件交付这个领域，它可能是最有用的一个启发式原则，我们所说的一切都可以归结到这一点上。集成通常是一个非常痛苦的过程。如果你的项目也是如此，那么就应该在每次有人提交代码后立刻进行集成，而且应该从项目一开始就这么做。如果测试是发布之前最痛苦的事情，那么就别拖到最后，而是应从项目一开始就不断地进行测试。

### 内建质量

​	越早发现缺陷，修复它们的成本越低。如果在没有提交代码到版本控制之前，我们就能发现并修复缺陷的话，代价是最小的。

​	“内建质量”还有另外两个推论

* 测试不是一个阶段，当然也不应该开发结束之后才开始。如果把测试留在最后，那就为时晚矣，因为可能根本没有时间修复那些刚被发现的问题。
* 测试也不纯粹或主要是测试人员的领域。交付团队的每个人都应该对应用程序的质量负责。

### “DONE”意味着“已发布”

​	我们认为，一个特性只有交到用户手中才能算“DONE”。这是持续部署实践背后的动机之一。

​	对于一些敏捷交付团队来说，“DONE”意味着软件已经部署到生产环境上。对于软件项目来说，这是一种理想状态。将其作为衡量是否完成的标准，并不总是合适的。对于那些第一次发布的软件系统来说，它可能需要一段时间才能达到“让外部用户真正从该软件身上获益”的状态。因此，我们可以暂且退让一步，只要某个功能在类生产环境上向客户代表做过演示，并且客户代表试用之后就认为是完成了。

​	根本没有“已经完成了80%”这一说法。任何事情要么是完成了，要么就是没完成。我们可以估计尚未完成的某件工作还需要多少工作量，但仅仅是估计而已。

### 交付过程是每个成员的责任

​	很多项目都是开发者开发后将困难转交给测试者，而测试者又在发布时将困难转嫁到运维团队。当出现问题时，人们花费大量的时间来修复错误，并用同等的时间来互相指责。

​	假如你工作于小规模团队或相对独立的部门，也许对发布软件所需的资源有绝对的控制能力。如果是这样，当然非常好啦。假如不是这样的话，你就要有思想准备，很可能需要长期的艰苦工作才能打破不同角色之间的壁垒。

​	这是DevOps运动的核心原则之一。DevOps运动的焦点和我们这本书的目标一致：为了更加快速且可靠地交付有价值的软件，鼓励所有参与软件交付整个过程中的人进行更好的协作。

### 持续改进

​	应用程序的首次发布只是其生命周期中的第一个阶段。随着应用程序的演进，更多的发布将会接踵而来。更重要的是，你的交付过程应该随之不断演进。

​	在交付过程中，整个团队应该定期地坐在一起，召开回顾会议，反思一下在过去一段时间里哪些方面做得比较好，应该继续保持，哪些方面做得不太好，需要改进，并讨论一下如何改进。每个改进点都应该有一个人负责跟踪，确保相应的改进活动能够被执行。当下一次团队坐在一起时，他们应该向大家汇报这些活动的结果。这就是众所周知的戴明环：计划-执行-检查-处理（PDCA）。

​	关键在于组织中的每个人都要参与到这个过程当中。如果只在自己所在角色的内部进行反馈环，而不是在整个团队范围内进行的话，就必将产生一种“顽疾”：以整体优化为代价的局部优化，最终导致互相指责。

# 配置管理

## 引言

​	假如项目中有良好的配置管理策略，那么你对下列所有问题的回答都应该是“YES”。

* 你能否完全再现你所需要的任何环境（这里的环境包括操作系统的版本及其补丁级别、网络配置、软件组合，以及部署在其上的软件应用及其配置）？
* 你能很轻松地对上述内容进行增量式修改，并将修改部署到任意一种或所有环境中吗？
* 你能否很容易地看到已被部署到某个具体环境中的某次修改，并能追溯到修改源，知道是谁做的修改，什么时候做的修改吗？
* 你能满足所有必须遵守的规程章则吗？
* 是否每个团队成员都能很容易地得到他们所需要的信息，并进行必要的修改呢？这个配置管理策略是否会妨碍高效交付，导致周期时间增加，反馈减少呢？



在本章中，我们将讨论三个问题。

* 为管理应用程序的构建、部署、测试和发布过程做好准备。我们从两个方面解决这个问题：对所有内容进行版本控制；管理依赖关系。
* 管理应用软件的配置信息。
* 整个环境的配置管理，这包括应用程序所依赖的软件、硬件和基础设施。另外还有环境管理背后的原则，包括操作系统、应用服务器、数据库和其他COTS（商业现货）软件。

## 使用版本控制

​	本质上来讲，版本控制系统的目的有两个。首先，它要保留每个文件的所有版本的历史信息，并使之易于查找。这种系统还提供一种基于元数据（这些元数据用于描述数据的存储信息）的访问方式，使元数据与某个单个文件或文件集合相链接。其次，它让分布式团队（无论是空间上不在一起，还是不同的时区）可以愉快地协作。

* 对于我们开发的应用软件，某个特定的版本是由哪些文件和配置组成的？如何再现一份与生产环境一模一样的软硬件环境？
* 什么时候修改了什么内容，是谁修改的，以及为什么要修改？因此，我们很容易知道应用软件在何时出了错，出错的过程，甚至出错的原因。

### 对所有内容进行版本控制

​	每个与所开发的软件相关的产物都应被置于版本控制之下。开发人员不但要用它来管理和控制源代码，还要把测试代码、数据库脚本、构建和部署脚本、文档、库文件和应用软件所用的配置文件都纳入到版本控制之中，甚至把编译器以及工具集等也放在里面，以便让新加入项目的成员可以很容易地从零开始工作。

​	为了重新搭建测试环境和生产环境，将所有必需的信息保存起来也是很重要的。这里必需的信息包括应用程序所需的支撑软件的配置信息、构成对应系统环境的操作系统配置信息、DNS区域文件和防火墙配置等。你至少要将那些用于重新创建应用程序的安装文件和安装环境所必需的所有信息保存在版本控制存储库之中。

​	我们的目标是能够随时获取软件在整个生命周期中任意时间点的文件状态。这样我们就可以选择从开发环境至生产环境整个环节中的任意时间点，并将系统恢复到该时间点的状态。我们甚至可以把开发团队所需的开发环境配置也置于版本控制中，如此一来，团队中的每个成员都能够轻松使用完全相同的设置。分析人员应该把需求文档保存到版本控制存储库中。测试人员也应该将自己的测试脚本和过程保存在版本控制存储库中。项目经理则应该将发布计划、进度表和风险日志也保存在这里。总之，每个成员都应该将与项目相关的任何文件及其修订状态保存在版本控制存储库之中。

​	除了存储源代码和配置信息，很多项目还将其应用服务器、编译器、虚拟机以及其他相关工具的二进制镜像也放在版本控制库中。

​	但我们并不推荐将源代码编译后得到的二进制文件也纳入到版本控制中，有以下几个理由。首先，它们通常比较大，而且（与编译程序不同）会让存储所需要的空间快速膨胀，因为我们每次签入代码，在编译和自动提交测试通过后，都会生成新的二进制文件。其次，如果有自动化构建系统，那么只要重新运行构建脚本，就可以利用源代码重新生成需要的二进制文件。这样的话，根本没有必要把这类二进制产物放在版本控制库中。请注意，我们并不推荐在同一个自动化构建过程中进行重复编译。因为如果需要二进制产物的话，我们只要通过构建系统把源代码再重新打包生成一次就可以了。最后，我们使用修订版本号来标识产品的版本。如果我们把构建生成的二进制文件也储存在版本控制库中，那么在存储库中的一个版本就会有两个不同的源，一个是源代码，另一个是二进制文件。尽管看上去这有点儿含糊，但创建部署流水线（本书的主要议题之一）时就显得极为重要了。

**版本控制：“删除”的自由**

​	版本控制库中包含每个文件的每一个版本，它的好处就是：可以随时删除你认为不必要的文件。只要有版本控制系统，对于“是否可以删除这个文件？”这个问题，你可以轻松地回答“Yes”。如果事实证明你的删除决定是错的，只要从早期版本中把它再找回来就行了。

​	这种“自由删除”是维护大型配置集合向前迈进的重要一步。保证大型团队能高效工作的关键就在于一致性和良好的组织性。“打破陈规”的能力使团队可以勇敢地尝试新的想法或实现方式，提高代码质量。

### 频繁提交代码到主干

​	首先，只有频繁提交代码，你才能享受版本控制所带来的众多好处，比如能够轻松地回滚到最近某个无错误的版本。

​	如果你频繁提交，其他人可以看到你的修改且可与之交互，你也可以清楚地知道你的修改是否破坏了应用程序，而且每次合并工作的工作量会一直很小，易于管理。

​	有些人解决这个两难问题的方法是，在版本控制系统中为新功能建立单独的分支。到某个时间点后，如果这些修改的质量令人满意，就将其合并到主干。这类似于“两阶段提交”。实际上，有些版本控制系统就是以这种方式工作的。

​	然而，我们对这样的做法持反对意见，除非是第14章提到的那三种例外情况。在这一点上有一些争议，尤其是在使用ClearCase以及相似工具的用户中。我们认为，这种方法存在以下几个问题。

* 它违背了持续集成的宗旨，因为创建分支的做法推迟了新功能的整合，只有当该分支被合并时才可能发现集成问题。
* 如果多个开发者同时分别创建了多个分支，问题会成指数增加，而合并过程也会极其复杂。
* 尽管有一些好用的工具有自动合并功能，但它们无法解决语义冲突。例如，某人在一个分支上重命名了一个方法，而另一个人在另一分支上对该方法增加了一次调用。
* 它让重构代码库变得非常困难，因为分支往往涉及多个文件，会让合并变得更加困难。



​	一个更好的解决方案是尽量使用增量方式开发新功能，并频繁且有规律地向版本控制系统提交代码。这会让软件能一直保持在集成以后的可工作状态。而且，你的软件会一直被测试，因为每次提交代码时，持续集成服务器就会从代码主干上运行自动测试。这会减小因重构引起的大规模合并导致冲突的可能性，确保集成问题能够被及时发现，此时修复这些问题的成本很低，从而提高软件开发质量。

​	为了确保提交代码时不破坏已有的应用程序，有两个实践非常有效。

* 一是在提交代码之前运行测试套件。这个测试套件应该是一个快速运转（一般少于10分钟）且相对比较全面的测试集合，以验证你没有引入明显的回归缺陷。很多持续集成服务器都提供名为“预测试提交”（pretested commit）的功能，让你在提交之前可以在类生产环境中执行这些测试。
* 增量式引入变化。我们建议每完成一个小功能或一次重构之后就提交代码。如果能正确地使用这一技术，你每天最少可以提交一次，通常能达到每天提交多次。如果你还未习惯于这种技术的话，肯定会以为是“天方夜谭”。但我们向你保证，这种技术能够带来相当高效的软件交付过程。

### 使用意义明显的提交注释

​	当构建失败以后，你知道是谁破坏了构建，以及他为什么破坏了构建。当然，这并不是唯一原因。很多时候，提交人没有写足够的描述信息，其原因通常是由于正在抓紧解决某个非常复杂的问题。我们可能常常遇到下面的场景。

* 你发现了一个缺陷，结果追溯到一行相当晦涩的代码。
* 你通过查看版本控制系统的日志，查找放入这行代码的人，以及他是什么时候放入的。
* 可是，放入这行代码的人去度假或者回家了，而他写的提交注释只有简单的几个字，即“已修复令人费解的缺陷”。
* 为了修复这个缺陷，你修改了这行晦涩代码。
* 但是却把其他功能破坏了。
* 你只能再花几个小时的时间，让软件恢复到可工作状态。

## 依赖管理

### 外部库文件管理

​	那么是否一定要把外部依赖库文件放在版本控制库中呢？其实，放与不放，各有利弊。如果放了，那我们更容易将软件的版本与正确的库文件版本相关联，但它也可能使源代码库的体积更大，并且签出时间也会变长

### 组件管理

​	将整个应用软件分成一系列的组件进行开发（小型应用除外）是个不错的实践。这能让某些变更的影响范围比较小，从而减少回归缺陷。另外，它还有利于重用，使大项目的开发过程更加高效

## 软件配置管理

### 配置与灵活性

​	灵活性也是有代价的。

​	对于软件灵活性的期望常常导致一种反模式，即“终极配置”，而这种反模式常被表述为对一个软件项目的需求。如果做得好，它没有什么坏处，但是如果搞不好的话，它会毁了一个项目。

​	根据我们的经验，“修改配置信息的风险要比修改代码的风险低”这句话就是个错觉。就拿“停止一个正在运行的应用系统”这个需求来说，通过修改代码或修改配置都很容易办到。如果使用修改源代码的方式，可以有多种方式来保证质量，比如编译器会帮我们查语法错误，自动化测试可以拦截很多其他方面的错误。然而，大多数配置信息是没有格式检查，且未经测试的。在大多数系统中，没有什么机制能阻止我们将一个URI“http://www.asciimation.co.nz/”改为“this is not a validURI”。大多数系统只有在运行时，才能发现这样的更改，此时用户不是惊喜地看到ASCII版的Star Wars，而是看到一堆系统异常报告，因为URI这个类无法解析“this is not a valid URI”。

在构建高度可配置的软件的道路上有很多陷阱，而最糟糕的可能莫过于下面这些。

* 经常导致分析瘫痪，即问题看上去很严重，而且很棘手，以至于团队花费很多时间思考如何解决它，但最终还是无法解决。
* 系统配置工作变得非常复杂，以至于抵消了其在灵活性上带来的好处。更有甚者，可能在配置灵活性上花费的成本与定制开发的成本相当。



​	可配置的软件并不总是像它看起来那么便宜。更好的方法几乎总是先专注于提供具有高价值且可配置程度较低的功能，然后在真正需要时再添加可配置选项。

*  在生成二进制文件时，构建脚本可以在构建时引入相关的配置，并将其写入新生成的二进制文件。
*  在打包时将配置信息一同打包到软件中，比如在创建程序集，以及打包ear或gem时。
*  在安装部署软件程序时，部署脚本或安装程序可以获取必要的配置信息，或者直接要求用户输入这些配置信息。
*  软件在启动或运行时可获取配置。



​	一般来说，我们并不赞同在构建或打包时就将配置信息植入的做法，而是应使用相同二进制安装包向所有的环境中部署，以确保这个发布的软件就是那个被测试过的软件。根据这一个原则，我们可以推出：在相临的两次部署之间，任何变更都应该作为配置项被捕获和记录，而不应该在编译或打包时植入。

### 配置的分类

* 在生成二进制文件时，构建脚本可以在构建时引入相关的配置，并将其写入新生成的二进制文件。
* 在打包时将配置信息一同打包到软件中，比如在创建程序集，以及打包ear或gem时
* 在安装部署软件程序时，部署脚本或安装程序可以获取必要的配置信息，或者直接要求用户输入这些配置信息。
* 软件在启动或运行时可获取配置。



​	一般来说，我们并不赞同在构建或打包时就将配置信息植入的做法，而是应使用相同二进制安装包向所有的环境中部署，以确保这个发布的软件就是那个被测试过的软件。根据这一个原则，我们可以推出：在相临的两次部署之间，任何变更都应该作为配置项被捕获和记录，而不应该在编译或打包时植入。

​	通常来说，能够在部署时对软件进行配置是非常重要的，这样就可以告诉应用程序在哪儿能找到所需服务，比如数据库、邮件服务器或外部系统。比如，当应用程序运行时的配置信息被存储在数据库中，你可能要在部署应用程序时将数据库的连接参数传入，使应用程序启动时可以从数据库中取到这些信息。

​	如果你有权限完全控制生产环境，就通常能让部署脚本自行获取这些配置并提供给应用。对于套装软件来说，安装包中通常都有默认的配置信息。做软件测试时，我们仍需要用某种方法在部署过程中修改某些配置信息。

​	当然，我们还可能要在启动或运行应用程序时修改某些配置。在系统启动时，我们可以通过命令参数或环境变量等形式提供配置信息。另外，你还可以使用同样的机制来做运行时的配置，比如注册表设置、数据库、配置文件，或者使用外部配置服务（比如通过SOAP或REST风格的接口访问）。

### 应用程序的配置管理

​	在管理应用程序的配置这个问题上，需要回答三个问题。

（1）如何描述配置信息？

（2）部署脚本如何存取这些配置信息？

（3）在环境、应用程序，以及应用程序各版本之间，每个配置信息有什么不同？



​	小提示：不要把密码签入到版本控制系统中，也不要把它硬编码到应用程序中。要是让运维人员知道你这么做，一定会让你卷铺盖走人的。所以，别给他们这样的机会。如果你坚持要将密码存在某处而不是自己记住的话，可以试着把它加密后放在用户主目录下。

​	这种方法的另一种极糟的使用方式是，将应用程序某一层上的密码保存在需要访问它的那层代码或文件系统中。实际上，用户在部署时应该每次都手工输入密码。对于多层应用系统来说，有多种方式来处理验证问题。比如，你可以使用证书、目录服务，或者一个单点登录系统。

**获取配置信息**

​	管理配置最有效的方法是让所有的应用程序通过一个中央服务系统得到它们所需要的配置信息。

**为配置项建模**

​	每个配置都是一个元组，所以应用程序的配置信息由一系列的元组构成。然而，这些元组及其值取决于三方面，即应用程序、该应用程序的版本、该版本所运行的环境（例如开发环境、用户验收测试环境、性能测试环境、试运行环境或生产环境）。

​	下面列举了一些在对配置信息建模时需要考虑的用例。

* 新增一个环境（比如一个新的开发工作站，或性能测试环境）。在这种情况下你要能为这个配置应用的新环境指定一套新的配置信息。
* 创建应用程序的一个新版本，通常需要添加一些配置设置，删除一些过时的配置设置。此时应该确保在部署新版本时，可以使用新的配置设置，但是一旦需要回滚时，还能够使用旧版本的配置设置。
* 将新版本从一个环境迁移到另一个环境，比如从测试环境挪到试运行环境。此时应该确保新环境上的新配置项都有效，而且为其设置了正确的值。
* 重定向到一个数据库服务器。应该只需要简单地修改所有配置设置，就能让它指向新的数据库服务器。
* 通过虚拟化技术管理环境。应该能够使用虚拟技术管理工具创建某种指定的环境，并且配置好所有的虚拟机。你也许需要将这种虚拟环境中的配置信息作为某特定版本的应用软件在虚拟环境中的标准配置信息。

**系统配置的测试**

​	与应用程序和构建脚本一样，配置设置也需要测试。对于系统配置测试来说，包括以下两部分。

​	一是要保证配置设置中对外部服务的引用是良好的。比如，作为部署脚本的一部分，我们要确保消息总线（messaging bus）在配置信息中所指定的地址已启动并运行，并确保应用程序所用的模拟订单执行服务在功能测试环境中能够正常工作。最起码，要保证能够与所有的外部服务相连通。如果应用程序所依赖的任何部分没有准备好，部署或安装脚本都应该报错，这相当于配置设置的冒烟测试。

​	二是当应用程序一旦安装好，就要在其上运行一些冒烟测试，以验证它运行正常。对于系统配置的测试，我们只要测试与配置有关的功能就可以了。在理想情况下，一旦测试结果与预期不符，这些测试应该能够自动停止软件的运行，并显示安装或部署失败。

### 跨应用的配置管理

​	大中型组织中，通常会同时管理很多应用程序，而软件配置管理的复杂性也会大大增加。这类组织中一般都会有遗留系统，而且很可能某个遗留系统的配置项让人很难搞得清楚明白。这种情况下，最重要的任务之一就是，要为每个应用程序维护一份所有配置选项的索引表，记录这些配置保存在什么地方，它们的生命周期是多长，以及如何修改它们。

​	如果应用程序之间有依赖关系，部署有先后次序的话，实时存取配置信息的能力就特别重要。很容易因配置信息设置不当而浪费很多时间，甚至导致整套服务无法正常运行，而这类问题是极难诊断的。

### 管理配置信息的原则

* 在应用程序的生命周期中，我们应该在什么时候注入哪类配置信息。是在打包的时候，还是在部署或安装的时候？是在软件启动时，还是在运行时？要与系统运维和支持团队一同讨论，看看他们有什么样的需求。
* 应该总是通过自动化的过程将配置项从保存配置信息的存储库中取出并设置好，这样就能很容易地掌握不同环境中的配置信息了。
* 配置系统应该能依据应用、应用软件的版本、将要部署的环境，为打包、安装以及部署脚本提供不同的配置值。每个人都应该能够非常容易地看到当前软件的某个特定版本部署到各种环境上的具体配置信息。
* 对每个配置项都应用明确的命名习惯，避免使用晦涩难懂的名称，使其他人不需要说明手册就能明白这些配置项的含义
* DRY（Don't Repeat Yourself）原则。定义好配置中的每个元素，使每个配置元素在整个系统中都是唯一的，其含义绝不与其他元素重叠。
* 最少化，即配置信息应尽可能简单且集中。除非有要求或必须使用，否则不要新增配置项。
* 避免对配置信息的过分设计，应尽可能简单。
* 保测试已覆盖到部署或安装时的配置操作。检查应用程序所依赖的其他服务是否有效，使用冒烟测试来诊断依赖于配置项的相关功能是否都能正常工作。

## 环境管理

​	没有哪个应用程序是孤岛。每个应用程序都依赖于硬件、软件、基础设施以及外部系统才能正常工作。

 	环境的配置和应用程序的配置同样重要。例如，如果应用程序需要用到消息总线，那么只有正确配置了这个消息总线，应用程序才能正常工作。操作系统的配置也同样重要。

​	这里把不良环境管理可能带来的问题总结如下。

* 配置信息的集合非常大；
* 一丁点变化就能让整个应用坏掉，或者严重降低它的性能。
* 一旦系统出现问题，需要资深人员花费不确定的时间来找到问题根源并修复它。
* 很难准确地再现那些手工配置的环境，因此给测试验证带来很大困难。
* 很难维护一个不使用配置信息的环境，因此维护这种环境下的行为也很难，尤其是不同的节点有不同的配置时。



​	重现环境的能力是非常必要的，原因如下。

* 可以避免知识遗失问题。比如某人离职且无法与他联系上，但只有他明白某个配置项所代表的意思。一旦这类配置项不能正常工作，通常都意味着较长的停机时间。这是一个很大却不必要的风险。
* 修复某个环境可能需要花费数小时的时间。所以，我们最好能在可预见的时间里重建环境，并将它恢复到某个已知的正常状态下。
* 创建一个和生产环境相同的测试环境是非常必要的。对于软件配置而言，测试环境应该和生产环境一模一样。这样，配置问题更容易被在早期发现。



​	需要考虑的环境配置信息如下：

* 环境中各种各样的操作系统，包括其版本、补丁级别以及配置设置；
* 应用程序所依赖的需要安装到每个环境中的软件包，以及这些软件包的具体版本和配置；
* 应用程序正常工作所必需的网络拓扑结构；
* 应用程序所依赖的所有外部服务，以及这些服务的版本和配置信息；
* 现有的数据以及其他相关信息（比如生产数据库）。



​	其实高效配置管理策略的两个基本原则是：

（1）将二进制文件与配置信息分离；

（2）将所有的配置信息保存在一处。如果应用了这两个基本原则，你就能将“在系统不停机的情况下，创建新环境、升级系统部分功能或增加新的配置项”等工作变成一个简单的自动化过程。

​	当评估第三方产品或服务时，应该问自己如下问题。

* 我们可以自行部署它吗？
* 可以自行部署它吗？
* 如何使它适应我们的自动化部署策略？

### 环境管理的工具

​	在以自动化方式管理操作系统配置的工具中，Puppet和CfEngine是两个代表。使用这些工具，你能以声明方式来定义一些事情，如哪些用户可以登录你的服务器，应该安装什么软件，而这些定义可以保存在版本控制库中。运行在系统中的代理（agent）会从版本控制库中取出最新的配置，更新操作系统以及安装在其之上的软件。对于应用了这些工具的系统来说，根本没必要登录到服务器上去操作，所有的修改都可能通过版本控制系统来发起，因而你也能够得到每次变化的完整记录，即谁在什么时候做了什么样的修改。

​	虚拟化技术也可以提高环境管理过程的效率。不必利用自动化过程从无到有地创建一个新环境，你可以轻易地得到一份环境副本，并把它作为一个基线保存起来。

### 变更过程管理

​	最后要强调的是，对环境的变更过程进行管理是必要的。应该严格控制生产环境，未经组织内部正式的变更管理过程，任何人不得对其进行修改。

​	如果配置管理流程比较好的话，对于下面的问题，你的回答都应该是肯定的。

* 是否仅依靠保存于版本控制系统中的数据（除了生产数据），就可以从无到有重建生产系统？
* 是否可以将应用程序回滚到以前某个正确的状态下？
* 是否能确保在测试、试运行和正式上线时以同样的方式创建部署环境？



​	如果回答是否定的，那么你的组织正处于风险之中。我们建议为下面的内容制定出一个保存基线和控制变更的策略:

* 应用程序的源代码、构建脚本、测试、文档、需求、数据库脚本、代码库以及配置文件；
* 用于开发、测试和运维的工具集；
* 用于开发、测试和生产运行的所有环境；
* 与应用程序相关的整个软件栈，包括二进制代码及相关配置；
* 在应用程序的整个生产周期（包括构建、部署、测试以及运维）的任意一种环境上，与该应用程序相关联的配置。

# 持续集成

## 引言

​	持续集成背后的思想是：既然经常对代码库进行集成对我们有好处，为什么不随时做集成呢？就集成而言，“随时”意思是指每当有人提交代码到版本控制库时。

## 实现持续集成

### 准备工作

​	在开始做持续集成之前，你需要做三件事情。

**版本控制**

​	与项目相关的所有内容都必须提交到一个版本控制库中，包括产品代码、测试代码、数据库脚本、构建与部署脚本，以及所有用于创建、安装、运行和测试该应用程序的东西。

**自动化构建**

​	你要能在命令行中启动构建过程。无论是通过命令行程序启动IDE来构建应用程序，然后再运行测试，还是使用多个复杂的构建脚本通过互相调用的方式来完成都行，但无论采用哪种机制，必须满足如下条件：人和计算机都能通过命令行自动执行应用的构建、测试以及部署过程。

* 要能在持续集成环境中以自动化的方式来执行整个构建过程，以便出现问题时能够审计。
* 应将构建脚本与代码库同等对待。应该对它进行测试，并不断地重构，以使它保持整洁且容易理解，而集成开发环境自动生成的构建过程基本上无法做到这一点。项目越复杂，这项工作就越重要。
* 使理解、维护和调试构建过程更容易，并有利于和运维人员更好地协作。

**团队共识**

​	持续集成不是一种工具，而是一种实践。它需要开发团队能够给予一定的投入并遵守一些准则，需要每个人都能以小步增量的方式频繁地将修改后的代码提交到主干上，并一致认同“修复破坏应用程序的任意修改是最高优先级的任务”。

### 一个基本的持续集成系统

​	现在的持续集成工具其安装和运行都极其简单。有几个开源工具可供选择，比如Hudson和受人尊敬的CruiseControl家族（CruiseControl、CruiseControl.NET和CruiseControl.rb）。其中，Hudson和CruiseControl.rb的启动和运行尤其简单。CruiseControl.rb是很轻量级的，而且掌握一些Ruby知识的人很容易对它进行扩展。Hudson的插件很多，这使它可以与构建和部署领域中的很多工具集成。

​	在此书编写之际，还有两种商业化持续集成服务器为小团队提供了免费版本，它们是ThoughtWorks Studios开发的Go以及JetBrains的TeamCity。其他流行的商业化持续集成服务器还包括Atlassian的Bamboo和Zutubi的Pulse。高端的发布管理以及构建加速系统还有UrbanCode的AntHillPro、ElectricCloud的ElectricCommander，以及IBM的BuildForge，它们都可以用于简单的持续集成。还有很多其他产品，完整列表可参见CI feature matrix[插图]。

​	第一次在持续集成工具上执行构建时，你很可能发现在运行持续集成工具的机器上缺少一些必需的软件和设置。这是一个独一无二的学习机会，请将接下来你所做的工作全部记录下来，并放在自己项目的知识共享库中。你应该花上一些时间将应用程序所依赖的所有软件和配置项提交到版本控制系统中，并将重建全新环境的整个活动变成一个自动化的过程。

​	一旦准备好要提交最新修改代码时，请遵循如下步骤。

* 查看一下是否有构建正在运行。如果有的话，你要等它运行完。如果它失败了，你要与团队中的其他人一起将其修复，然后再提交自己的代码。
* 一旦构建完成且测试全部通过，就从版本控制库中将该版本的代码更新到自己的开发环境上
* 在自己的开发机上执行构建脚本，运行测试，以确保在你机器上的所有代码都工作正常。当然你也可以利用持续集成工具中的个人构建功能来完成这一步骤。
* 如果本地构建成功，就将你的代码提交到版本控制库中。
* 然后等待包含你的这次提交的构建结果。
* 如果这次构建失败了，就停下手中做的事，在自己的开发机上立即修复这个问题，然后再转到步骤（3）。
* 如果这次构建成功，你可以小小地庆祝一下，并开始下一项任务。

## 持续集成的前提条件

### 频繁提交

​	对于持续集成来说，我们最重要的工作就是频繁提交代码到版本控制库。每天至少应该提交几次代码。

### 创建全面的自动化测试套件

​	如果没有一系列全面的自动化测试，那么构建成功只意味着应用程序能够编译并组装在一起。虽然对于某些团队来说，这已经是非常大的一个进步了，但是，假如能够有一定程度的自动化测试，会让你更有信心说：“我们的应用程序是可以工作的。”

​	单元测试用于单独测试应用程序中某些小单元的行为（比如一个方法、一个函数，或一小组方法或函数之间的交互）。

​	组件测试用于测试应用程序中几个组件的行为。与单元测试一样，它通常不必启动整个应用程序，但有可能需要连接数据库、访问文件系统或其他外部系统或接口（这些可以使用“桩”，即stub技术）。

​	验收测试的目的是验证应用程序是否满足业务需求所定义的验收条件，包括应用程序提供的功能，以及其他特定需求，比如容量、有效性、安全性等。验收测试最好采用将整个应用程序运行于类生产环境的运作方式。当然，验收测试的运行时间也较长。一个验收测试套件连续运行一整天是很平常的事儿。

###  保持较短的构建和测试过程

* 大家在提交代码之前不愿意在本地环境进行全量构建和运行测试，导致构建失败的几率越来越大。
* 持续集成过程需要花太长时间，从而导致再次运行构建时，该构建会包含很多次提交，所以很难确定到底是哪次提交破坏了本次构建。
* 大家提交的频率会变少，因为每运行一次构建和测试，都要坐在那儿等上一阵子。



​	理想情况下，提交前的预编译和测试过程，以及持续集成服务器上的编译和测试过程应该都能在几分钟内结束。我们认为，十分钟是一个极限了，最好是在五分钟以内，九十秒内完成是最理想的。十分钟对于那些惯于操作小项目的人来说，应该算是比较长的时间了，但对于那些经历过需要花数小时的编译的老前辈来说，却是非常短的时间。这段时间长度应该恰好能泡杯茶，快速聊几句，看一眼邮件，或伸展一下身体。

​	有很多技术可以帮助你减少构建时间。首先要考虑的事情是让测试执行得更快。

​	有时候需要将测试分成几个阶段，首先将其分成两个阶段。第一个阶段用于编译软件，运行所有类级别的单元测试，并创建用于部署的二进制文件。这个阶段叫做“提交阶段”。

​	第二个阶段应该利用第一个阶段所生成的二进制文件进行验收测试、集成测试。假如你有性能测试的话，也要一并运行。利用现代持续集成工具，很容易创建这种分阶段的构建流程，它们能够同时运行多个任务，并将运行结果收集在一起，以便很容易看到运行状态和结果。

​	另外，有时候把一个简单的冒烟测试套件加入到提交阶段，也是非常有用的。这个冒烟测试套件应该执行一些简单的验收和集成测试，用于确保最常见的功能没有被破坏。假如这些基本功能被破坏了，就能得到很快的反馈。

### 管理开发工作区

​	对于保证开发人员的开发效率与明晰思路来说，开发环境的管理是特别重要的。当开发人员刚开始新任务时，应该总是从一个已知正确的状态开始。他们应该能够运行构建、执行自动化测试，以及在其可控的环境上部署其开发的应用程序，通常是在他们自己的开发机上。只有在特殊的情况下，才应使用共享环境开发。在本地开发环境上运行应用程序时，应确保所使用的自动化过程与持续集成环境中的一致，与测试环境中也是一样的，且生产环境中也是一样的。

## 使用持续集成软件

​	持续集成工具最基本的功能就是轮询版本控制系统，查看是否有新的版本提交，如果有的话，则签出最新版本的软件，运行构建脚本来编译应用程序，再运行测试，最后将运行结果告知你。

### 基本操作

​	本质上，持续集成软件包括两个部分。第一部分是一个一直运行的进程，它每隔一定的时间就执行一个简单的工作流程。第二部分就是提供展现这个流程运行结果的视图，通知你构建和测试成功与否，让你可以找到测试报告，拿到生成的安装文件等。

### 铃声和口哨

​	这种可视化的唯一缺点就是，如果开发团队和客户在一起工作的话（对于大多数敏捷项目来说，的确是这样的），构建失败（流程中很自然的一部分）可能被认为是应用程序质量存在问题的信号。事实也正是如此，每次构建失败都表明发现了一个问题，但如果没有发现的话，它就会被带到生产环境中。然而，有时候很难向客户解释“为什么构建总是失败”。我们曾遇到过好几次这种状况，其中有一次构建失败持续了很长时间，期间我们与客户进行了一些艰难的对话，但唯一能做的事情就是让它高度可视化，并努力工作，向客户解释这样做的好处。当然，最佳解决方案是努力工作，让构建一直成功。

​	你还可以在构建过程中对源代码进行一些分析工作，包括分析测试覆盖率、重复代码、是否符合编码标准、圈复杂度，以及其他一些健康指标，并将结果显示在每个构建的总结报告中。你也可以运行一些程序来生成与代码相对应的对象模型图或数据库结构图。所有这些都是可视化的一部分。

​	持续集成前身:

* 每日构建。
* 增加自动化测试。
* rolling builds”过程，即持续不断地运行构建过程，而不是在夜间定时执行批处理过程。

## 必不可少的实践

### 构建失败之后不要提交新代码

​	续集成的第一忌就是明知构建已经失败了，还向版本控制库中提交新代码。如果构建失败，开发人员应该尽快找出失败的原因，并修复它。

### 提交前在本地运行所有的提交测试，或者让持续集成服务器完成此事

​	正如之前提过的，我们希望每次提交都可以产生一个可发布的候选版本。任何人以任何形式公布某个东西之前，都会检查一下自己的工作成果，而候选版本也是一个发行物，所以每次提交前也要做一下检查。

​	我们希望提交过程是一件轻量级的事儿，这样就可以每隔二十分钟左右提交一次了，但它也应该是一件非常严肃的事儿，这样在每次提交之前，我们都会停下来，仔细想一想是否应该提交。提交前在本地运行一次提交测试，就是做一下健全性检查（sanity check）。它也让我们能确信新增的代码的确是按期望的方式运行的。

​	你可能会问：“为什么在提交前还要运行本地提交测试呢？这样的话，我们的编译和提交测试不是要运行两次了吗？”这么做，有两个理由。

* 如果在你根据版本控制进行更新之前，其他人已经向版本控制库中提交了新代码，那么你的变更与那些新代码合并后，可能会导致测试失败。如果你自己先在本地更新代码并运行提交测试的话，假如有问题，就会在本地提前发现，提前修复，从而不会令持续集成服务器上的构建失败，不至于影响其他人及时提交。
* 在提交时经常犯的错误是，忘记提交那些刚刚新增加的东西到存储库中。如果遵守这个流程的话，当本地构建成功，而持续集成系统中的提交阶段失败了的话，那么你就知道要么是由于别人与你同时提交了代码，要么就是你遗漏了一部分类或配置文件没有提交到版本控制系统中。



​	很多现代持续集成服务器还提供这样一种功能，名字叫做预测试提交（pretestedcommit），也称为个人构建（personal build）或试飞构建（preflight build）。使用这种特性，就不必自己进行提交，持续集成服务器将拿到你的本地变更，把它放在构建网格中运行提交测试。一旦构建成功通过，持续集成服务器就替你将变更提交到版本控制库中。如果构建失败的话，它会通知你哪里出错了。

​	Pulse、TeamCity和 ElectricCommander这三种持续集成服务器都已经提供了这个功能。如果使用分布式版本控制系统的话，这个实践就更容易了，因为你可以将代码存储到自己的本地代码控制库中，而无需提交到团队的中央版本控制库中。通过这种方式，一旦个人构建失败的话，很容易通过创建补丁的方式将自己提交的修改搁置，恢复到你刚刚提交到持续集成服务器的那个版本上，将构建修复，再把补丁放上去。

### 等提交测试通过后再继续工作

​	在提交代码时，做出了这一代码的开发人员应该监视这个构建过程，直到该提交通过了编译和提交测试之后，他们才能开始做新任务。在这短短几分钟的提交阶段结束之前，他们不应该离开去吃午饭或开会，而应密切注意构建过程并在提交阶段完成的几秒钟内了解其结果。

### 回家之前，构建必须处于成功状态

​	不建议你工作到很晚来修复失败的构建，而是希望你有规律地尽早提交代码，给自己足够的时间处理可能出现的问题。或者，你可以第二天再提交。很多有经验的开发人员在下班前一小时内不再提交代码，而是把它作为第二天早上的第一件事情。

​	如果位于印度的团队破坏构建后就回家了，那么位于伦敦的团队一整天的工作都会受到极大影响。同样，如果位于伦敦的团队做了同样的事，那么位于美国的同事可能在接下来的八小时之内一直在他们的阴影下工作。

### 时刻准备着回滚到前一个版本

​	如果某次提交失败了，无论采取什么样的行动，最重要的是尽快让一切再次正常运转起来。如果无法快速修复问题，无论什么原因，我们都应该将它回滚到版本控制库中前一个可工作的版本上，之后再在本地环境中修复它。

### 在回滚之前要规定一个修复时间

​	建立一个团队规则：如果因某次提交而导致构建失败，必须在十分钟之内修复它。如果在十分钟内还没有找到解决方案的话，就将其回滚到版本控制系统中前一个好的版本。如果团队能够忍受，有时候也可以延长一段时间来修复它。

### 不要将失败的测试注释掉

​	一旦你决定执行前面所说的规则，有些开发人员常常为了能够提交代码，而将那些失败的测试注释掉。这种冲动是可以理解的，但却是无法被容忍的一种错误行为。那些已经成功运行了一段时间的测试失败时，失败的原因可能很难找。

### 为自己导致的问题负责

​	假如提交代码后，你写的测试都通过了，但其他人的测试失败了，构建结果还是会失败。通常这意味着，你引入了一个回归缺陷。你有责任修复因自己的修改导致失败的那些测试。在持续集成环境中这是理所当然的，但可惜的是，在很多项目中事实并不是这样的。

### 测试驱动的开发

​	关于测试驱动开发的话题超出了本书的范围。但值得注意的是，和所有其他此类实践一样，测试驱动开发也需要纪律性和实效性。在这里我们向读者推荐两本相关的书藉：Steve Freeman和Nat Pryce合著的Growing Object-Oriented Software,Guided by Tests,以及Gerard Meszaros写的xUnit Test Patterns:RefactoringTest Code。

## 推荐的实践

### 极限编程开发实践

​	对于任何团队，即使不采用其他实践，只用持续集成也会给项目开发带来很大改善，而若与其他实践相结合的话，它的作用会更大。尤其是，除了测试驱动开发和我们前面讲到的代码集体所有权，你还应该考虑把重构作为高效软件开发的基石。

​	重构是指通过一系列小的增量式修改来改善代码结构，而不会改变软件的外部行为。通过持续集成和测试驱动开发可以确保这些修改不会改变系统的行为，从而使重构成为可能。这样，你的团队就可以自由自在地修改代码，即使偶尔涉及较大范围的代码修改，也不用担心它会破坏系统了。这个实践也让频繁提交成为了可能，即开发人员在每次做了一个小的增量式修改后就提交代码。

### 若违背架构原则，就让构建失败

​	开发人员有时很容易忘记系统架构的一些原则。我们曾经使用过一种手段来解决这个问题，那就是写一些提交时测试，用于证明这些原则没有被破坏。

### 若测试运行变慢，就让构建失败

​	持续集成需要小步频繁提交。如果提交测试要运行很长时间的话，这种长时间的等待会严重损害团队的生产效率，他们将花费很长的时间等待构建和测试过程完成。而且，这样也无法做到频繁提交，结果会导致团队成员开始把每次要提交的内容都存在本地，而每多增加一次本地保存就会增加一些复杂性，同时也增加了与版本控制库的代码出现合并冲突的可能性，增加了引入错误的几率，最终可能导致测试失败。所有这些最终都会导致生产率下降。

​	为了让开发团队注意到快速测试的重要性，可以这样做：当某个测试运行超过一定时间后，就让这次提交测试失败。我们在上一个项目中使用的这一时间是两秒。

### 若有编译警告或代码风格问题，就让测试失败

​	编译器发出警告时，通常理由都足够充分。我们曾经用过一个比较成功的策略，即只要有编译警告，就让构建失败，但我们的开发团队常常把它叫做“纳粹代码”。这在某些场合可能有点儿苛刻，但作为强迫写好代码的一种实践，还是很有效的。你可以通过添加代码检查尽可能地强化这一技术。

​	我们成功使用过很多关于代码质量检查的开源工具，如下所示：

* Simian是一种可以识别大多数流行语言（包括纯文本）中重复代码的工具。
* JDepend是针对Java的免费版本，它有一个．NET的商业版本NDepend，它们拥有大量对设计质量进行评估的实用（和不太实用）的度量指标。
* CheckStyle可以对“烂代码”做一些检查，比如工具类中的公共构造函数、嵌套的代码块和比较长的代码行。它也能找到缺陷和安全漏洞的一些常见根源。它还很容易被扩展。FxCop是它的．NET版本。
* FindBugs是一个Java软件，它是CheckStyle的替代品，有一些相似的校验功能。

## 分布式团队

### 对流程的影响

​		对在同一时区内的分布式团队来说，持续集成流程基本是一样的。当然，你无法以实物的形式使用提交令牌。虽然有些持续集成服务器支持虚拟令牌，但它不具有人性化特点，所以当你提醒某人去修复构建时，容易导致大家的抵触心理。同时，类似“个人构建”这种功能会变得更加有用。但总地来说，流程是一样的。

​	对分布在不同时区的分布式团队来说，就需要多处理一些事情啦。如果在旧金山的团队在破坏构建以后回家了，那么，这对北京的团队可能就是个严重的阻碍。因为当旧金山的团队下班后，北京才刚上班。尽管流程没有什么变化，但不良影响会被放大。

### 集中式持续集成

​	一些功能更强大的持续集成服务器提供像“集中管理构建网格”和“高级授权机制”这种功能，用于把持续集成作为一个集中式服务，为大型分布式团队提供服务。这样的服务器让团队很容易建立自服务式的持续集成服务，而不需要自己管理硬件。它也会让运维团队将持续集成作为集中式服务，统筹服务器资源，管理持续集成和测试环境的配置，以确保这些环境的一致性以及与生产环境的相似性，还能巩固一些好的实践，比如第三方库的配置管理，预安装一些工具（用于收集代码覆盖率和质量的统一度量数据。最终，我们可以做到项目之间的统一度量数据的收集和监控，为管理者和交付团队提供程序级的代码质量监控方式。

​	虚拟化技术可以与集中式持续集成服务很好地结合，只需要单击一下按钮就能利用已保存好的基线镜像重建一个新的虚拟机。利用虚拟化技术，可以为开发团队提供一键式搭建新环境这样的自服务功能。这也可以确保构建和部署一直运行在一致的基线版的环境中。	

### 技术问题

​	当分布于世界各地的团队之间网络状况不佳时，依据选择的不同版本控制系统，团队间共享版本控制系统、构建和测试资源的做法有时候也会有很多麻烦。在持续集成运转良好时，整个团队都会有规律地提交代码。这意味着，与版本控制系统之间的交互通常保持在一个较高的合理水平上。由于提交和更新比较频繁，虽然每次交互通常都较小（甚至可以用字节来计算），劣质的通信仍会严重拖生产效率的后腿。因此，加大投入在各开发中心之间建立起足够高带宽的通信机制是非常必要的。考虑将集中式的版本控制库迁到某种分布式版本控制系统（比如Git或Mercurial）也是不错的选择。闻名知意，即使无法连接到主服务器，分布式版本控制系统也能让大家提交代码。

### 替代方法

​	如果由于某些不可克服的原因，无法再增加投入在开发中心间建立更高带宽的通信机制，各地团队还可以使用本地持续集成和测试系统（当然这不太理想），甚至在某些极端情况下，不得不用本地的版本控制系统。我们并不建议使用这种方法，但这种情况在现实中还是很有可能的。所以，我们要尽一切可能避免使用这种方法。这种方法在时间和人力上的成本都很高，而且根本无法做到团队间的共享访问和控制。

​	对于分布式团队来说，主要有两种方式来解决本地化版本控制系统的存取问题：一是将应用程序分成多个组件；二是使用那些分布式或支持多主库拓扑结构的版本控制系统。

## 分布式版本控制系统

​	DVCS（Distributed Version Control System，分布式版本控制系统）的兴起是团队合作方式的革命性改进。很多开源项目曾经使用电子邮件或论坛发帖的方式来提交补丁，而像Git和Mercurial这种工具让开发人员之间、团队之间以及分支与合并工作流时的打补丁变得极其简单。DVCS使你能够离线工作、本地提交，或在将修改提交给其他人之前把这些代码搁置起来或对其做rebase操作。DVCS的核心特性是每个仓库都包括项目的完整历史，这意味着除了团队约定之外，仓库是没有权限控制功能的。所以，与集中式系统相比，DVCS引入了一个中间层：在本地工作区的修改必须先提交到本地库，然后才能推送到其他仓库，而更新本地工作区时，必须先从其他仓库中将代码更新到本地库。

​	持续集成的以上这些替代方案可以创建高质量可工作的软件。然而，这必须满足以下条件才能成为事实。

* 有一个成员比较少，但都非常有经验提交团队。他们可以取每个补丁、照管自动化测试并确保软件的质量。
* 频繁地从分支上取被修改过的代码，以避免由于积累太多的代码使变更很难合并。如果发布的时间计划非常严格，则这个条件就非常重要，因为人们倾向于临在近发布时刻再合并，而此时的合并是极其痛苦的——这正是持续集成要解决的问题。
* 相对较少的核心开发人员，可能有一个贡献频率较低但人员较多的社区作为补充。这会让合并具有可追溯性。

## 小结

​	总之，一个好的持续集成系统是基石，在此之上你可以构建更多的基础设施：

* 一个巨大的可视化指示器，用于显示构建系统所收集到的信息，以提供高质量的反馈；
* 结果报告系统，以及针对自己测试团队的安装包；
* 为项目经理提供关于应用程序质量的数据的提供程序；
* 使用部署流水线，可以将其延展到生产环境，为测试人员和运维团队提供一键式部署系统。

# 测试策略的实现

## 引言

​	戴明14条之一就是：“停止依赖于大批量检查来保证质量的做法。改进过程，从一开始就将质量内嵌于产品之中。”

​	测试策略的设计主要是识别和评估项目风险的优先级，以及决定采用哪些行动来缓解风险的一个过程。好的测试策略会带来很多积极作用。测试会建立我们的信心，使我们相信软件可按预期正常运行。也就是说，软件的缺陷较少，技术支持所需的成本较低，客户认可度较高。测试还为开发流程提供了一种约束机制，鼓励团队采用一些好的开发实践。一个全面的自动化测试套件甚至可以提供最完整和最及时的应用软件说明文档，这个文档不仅是说明系统应该如何运行的需求规范，还能证明这个软件系统的确是按照需求来运行的。

## 测试的分类

![](https://pic.imgdb.cn/item/60c9b645844ef46bb227e617.jpg)

### 业务导向且支持开发过程的测试

​	这一象限的测试通常称作功能测试或验收测试。验收测试确保用户故事的验收条件得到满足。在开发一个用户故事之前，就应该写好验收测试，采取完美的自动化形式。

​	像验收条件一样，验收测试可以测试系统特性的方方面面，包括其功能（functionality）、容量（capacity）、易用性（usability）、安全性（security）、可变性（modifiability）和可用性（availability）等。关注于功能正确性的验收测试称作功能验收测试，而非功能验收测试归于图中的第四象限。如果对于功能与非功能测试有模糊认识且常常搞不清它们的区别，请参见技术导向且评估项目的象限。

​	时新的自动化功能测试工具，比如 Cucumber、JBehave、Concordion以及Twist，都旨在把测试脚本与实现分离，以达到这种理想状态，并提供某种机制方便地将二者进行同步。在这种方式下，由用户来写测试脚本是可能的，而开发人员和测试人员则要致力于实现这些测试脚本。

**自动化验收测试**

自动化验收测试有很多很有价值的特性。

* 它加快了反馈速度，因为开发人员可以通过运行自动化测试，来确认是否完成了一个特定需求，而不用去问测试人员。
* 它减少了测试人员的工作负荷。
* 它让测试人员集中精力做探索性测试和高价值的活动，而不是被无聊的重复性工作所累。
* 这些验收测试也是一组回归测试套件。当开发大型应用或者在大规模团队中工作时，由于采用了框架或许多模块，对应用某一部分的更改很可能会影响其余特性，所以这一点尤其重要。
* 就像行为驱动开发（BDD）所建议的那样，使用人类可读的测试以及测试套件名，我们就可以从这些测试中自动生成需求说明文档。像Cucumber和Twist这样的工具，就是为让分析人员可以把需求写成可执行的测试脚本而设计的。这种方法的好处在于通过验收测试生成的需求文档从来都不会过时，因为每次构建都会自动生成它。



​	自动化验收测试的维护成本可能很高。如果写得不好，它们会使交付团队付出极大的维护成本。由于这个原因，有些人不建议创建大而复杂的自动化测试集合，比如James Shore[插图][dsyXYv]就持这种观点。然而，通过使用正确的工具，并遵循好的实践原则，完全可以大大降低创建并维护自动化验收测试的成本，从而令收益大于付出。

​	同样需要记住的是，并不是所有的东西都需要自动化。对于某些方面的测试来说，用手工方法做更好。易用性测试及界面一致性等方面很难通过自动化测试来验证。尽管有时候测试人员会将自动操作作为探索性测试的一部分，比如初始化环境、准备测试数据等，但探索性测试不可能被完全自动化。很多情况下，手工测试就足够了，甚至优于自动化测试。总之，我们倾向于将自动化验收测试限于完全覆盖Happy Path的行为，并仅覆盖其他一些极其重要的部分。

​	更多关于何时做自动化的内容，参见BrianMarick的文章“When Should a Test Be Automated? ”[90NC1y]。

​	然而，大多数界面测试工具与界面本身总是紧紧耦合在一起，其后果就是，一旦界面改变了（哪怕是一点儿），测试也会被破坏。这会导致很多的假阳性，因为你会经常遇到这种情况，即测试被破坏的原因并不是应用功能不正确，而只是由于某个复选框的名字被修改了。在这种情况下，仅将这些测试与应用程序同步就会消耗相当多的时间，但却不会交付任何价值。最好不断地问自己这样一个问题：“我的验收测试有多少次是由于真正的缺陷才失败的，有多少次是因为需求的变更才失败的？”

​	有几种方法来解决这个问题。一种方法是在测试与用户界面之间增加一个抽象层，以便减少因用户界面变更而导致的工作量。另一种方法是通过公共API来运行这些验收测试，这些API就在用户界面层之下，而且用户界面也会使用这些API来执行真正的操作（当然，这就要求你的UI层不应该包含业务逻辑）。我们并不是说不需要用户界面测试了，而是说可以将用户界面本身的测试减少到最低限度，而不是减少对业务逻辑的测试。这样，验收测试套件可以直接验证业务逻辑。

### 技术导向且支持开发过程的测试

​	这些自动化测试单独由开发人员创建并维护。这些自动化测试单独由开发人员创建并维护。有三种测试属于这一分类：单元测试、组件测试和部署测试。

​	单元测试不应该访问数据库、使用文件系统、与外部系统交互。或者说，单元测试不应该有系统组件之间的交互。这会让单元测试运行非常快，因此可以得到更早的反馈，了解自己的修改是否破坏了现有的任何功能。这些测试也应该覆盖系统中每个代码分支路径（最少达到80%）。

​	然而，为了获得高速度，也有一些代价，即可能会错过应用系统不同部分之间交互时产生的一些缺陷。比如，通常某些对象（面向对象编程中的概念）或者应用程序数据的生命周期是非常不同的。此时，只有当对更大范围的代码进行测试时才能发现一些缺陷，这些缺陷出现的原因是你没有正确处理某些数据或对象的生命周期。

### 业务导向且评价项目的测试

​	这类手工测试可以验证我们实际交付给用户的应用软件是否符合其期望。这并不只是验证应用是否满足需求规格说明，还验证需求规格说明的正确性。我们从来没有接触或听说过哪个项目的需求规格说明在开发项目之前就已经写得非常完美。不可避免地，每当在现实生活中有用户试用一个应用，他们就会发现这个应用还有改进的空间。用户会破坏一些东西，因为他们会尝试执行从前没有人执行过的一系列操作。用户也会抱怨，认为应用程序应该能更好地帮助他们完成他们要经常做的工作。他们可能会从应用软件里得到一些启发，发现某种新功能能帮助他们更好地完成工作。软件开发是一个很自然的迭代过程，它建立在一个有效的反馈环之上，而我们却骗自己是否有其他方式来预见它。

​	一种非常重要的面向业务且评价项目的测试是演示。在每个迭代结束时敏捷开发团队都向用户演示其开发完成的新功能。

​	探索性测试被James Bach描述为一种手工测试，他说：“执行测试的同时，测试人员会积极地控制测试的设计并利用测试时获得的信息设计新的更好的测试。”

​	易用性测试是为了验证用户是否能很容易地使用该应用软件完成工作。在开发过程当中很容易发现问题，甚至那些定义软件需求的非技术人员也能轻易发现问题。因此，易用性测试是验证应用程序是否能交付价值给用户的最终测试。

### 技术导向且评价项目的测试

​	验收测试分为两类：功能测试和非功能测试。非功能测试是指除功能之外的系统其他方面的质量，比如容量、可用性、安全性等。

​	这类测试（用于检查这类验收条件是否都被满足了）和运行这类测试的工具可能与特定于功能验收条件的测试和工具有很大不同。这类测试常常需要很多的资源，比如需要比较特殊的环境来运行测试，并且可能需要专业知识来建立和实现测试，另外它们还通常需要花更长时间来运行（无论这些测试是否是自动化测试）。因此，这类测试的实现一般会比较靠后。即使所有非功能测试都被自动化了，与功能验收测试相比，其运行频率也会更低一些，而且很可能是在部署流水线的最后阶段进行。

### 测试替身

​	自动化测试的一个关键是在运行时用一个模拟对象来代替系统中的一部分。这样，应用程序中被测试的那部分与系统其他部分之间的交互可以被严格地掌控，从而更容易确定应用程序中这一特定部分的行为。这样的模拟对象常常就是mock、stub和dummy等。

* 哑对象（dummy object）是指那些被传递但不被真正使用的对象。通常这些哑对象只是用于添充参数列表。
* 假对象（fake object）是可以真正使用的实现，但是通常会利用一些捷径，所以不适合在生产环境中使用。一个很好的例子是内存数据库。
* 桩（stub）是在测试中为每个调用提供一个封装好的响应，它通常不会对测试之外的请求进行响应，只用于测试。
* spy是一种可记录一些关于它们如何被调用的信息的桩。这种形式的桩可能是记录它发出去了多少个消息的一个电子邮件服务。
* 模拟对象（mock）是一种在编程时就设定了它预期要接收的调用。如果收到了未预期的调用，它们会抛出异常，并且还会在验证时被检查是否收到了它们所预期的所有调用。

## 现实中的情况与应对策略

### 新项目

​	新项目有机会实现我们在本书中所描述的理想国。此时，变化的成本比较低，通过建立一些相对简单的基本规则，并创建一些相对简单的测试基础设施，就可以很顺利地开始你的持续集成之旅。在这种情况下，最重要的事情就是一开始就要写自动化验收测试。为了能做到这一点，你需要：

* 选择技术平台和测试工具；
* 建立一个简单的自动化构建；
* 制定遵守INVEST原则[即独立的（Independent）、可协商的（Negotiable）、有价值的（Valuable）、可估计的（Estimable）、小的（Small）且可测试的（Testable）]的用户故事[ddVMFH]及考虑其验收条件。

然后就可以严格遵守下面的流程：

* 客户、分析师和测试人员定义验收条件；
* 测试人员和开发人员一起基于验收条件实现验收测试的自动化；
* 开发人员编码来满足验收条件；
* 只要有自动化测试失败，无论是单元测试、组件测试还是验收测试，开发人员都应该把它定为高优先级并修复它。



​	当然，必须让团队的每个人（包括客户和项目经理在内）都接受这种做法。我们曾看到过一些项目取消这种做法，因为客户觉得写自动化验收测试花费了太多的时间。假如客户真的愿意以牺牲自动化验收测试套件的质量为代价达到快速将软件推向市场的目标，那么，作出这样的决定也无可厚非。当然，其后果也应该非常明显啦。

### 项目进行中

​	引入自动化测试最好的方式是选择应用程序中那些最常见、最重要且高价值的用例为起点。这就需要与客户沟通，以便清楚地识别真正的业务价值是什么，然后使用测试来做回归，以防止功能被破坏。基于这些沟通，你应该能把那些Happy Path的测试自动化，用于覆盖高价值的场景。

### 遗留系统

​	这种遗留系统的特点在于：代码通常没有标准组件化，结构比较差。所以修改系统某部分的代码却影响了另一部分代码的事情经常发生。此时，通常比较有效的策略是在测试结束后仔细地验证系统的状态。如果时间来得及，你可以再测试一下这个用户故事的Alternate Path。最后，你还可以写更多的验收测试来检查一些异常条件，或防御一些常见的失效模式（failure mode），或防止不良的副作用。

​	切记，只写那些有价值的自动化测试就行。基本上，可以将应用程序分成两部分。一部分是实现系统功能的具体代码，另一部分则是在这些代码之下，为实现系统功能提供支撑的框架代码。

### 集成测试

​	你可以利用写一般验收测试的方式来写集成测试。通常来说，集成测试应该在两种上下文中运行：首先是被测试的应用程序使用其真正依赖的外部系统来运行时，或者是使用由外部服务供应商所提供的替代系统；其次是应用程序运行于你自己创建的一个测试用具（test harness）之上，而且这些测试用具也是代码库的一部分。

* 在测试环境中使用一个“防火墙”将该应用程序与外部系统隔离开来，而在开发过程中，越早这么做越好。当外部系统不可用时，这也是测试应用程序行为的一个好方法
* 在应用程序中使用一组配置信息，让其与外部系统的模拟版本进行交互。



​	在理想情况下，服务提供商会提供一个复制版的测试服务，除了性能以外，它可以提供与真正的服务完全相同的行为。你可以在此之上进行测试。然而，在现实世界中，你常常需要开发一个测试用具。比如当：

* 外部系统还没有开发完成，但接口已经提前定义好了（此时你需要有心理准备，因为这些接口很可能会发生变化）
* 外部系统已经开发完了，但是还不能为了测试而部署它，或者用于测试目的的外部系统运行太慢，或缺陷太多，无法支持正常自动化测试的运行；
* 虽然有测试系统，但它的响应具有不确定性，从而导致无法对自动化测试结果进行验证（比如，某个股票市场的实时数据）；
* 外部系统很难安装或者需要通过用户界面进行手工干预；
* 自动化持续集成系统需要承担的工作量太大且其所需要的服务水平太高，远不是一个仅用于做手工探索性测试的轻量级测试环境所能承受或提供的。



每当增加一个外部系统集成点时，项目风险就会增加，集成风险如下。

* 测试服务是否准备好了？它是否能正常运行？
* 外部服务供应商是否有足够的资源和人力来回答我们遇到的问题、修改缺陷，添加我们提出的一些定制化功能？
* 我们是否能直接访问真实的生产环境，以便验证外部系统是否满足我们的容量要求或可用性要求？
* 外部服务提供的API是否很容易与我们自己开发应用软件时所采用的技术进行集成，我们的团队是否需要某些专业技能才能使用这些API？
* 是否要编写并维护我们自己的测试服务？
* 当外部系统的响应与我们所期望的行为不一致时，我们自己的应用程序是否能够正确地处理？

## 流程

​	如果团队成员之间的沟通不畅，写验收测试的成本可能很高，甚至成为一种乏味的体力活。很多项目依靠测试人员来检查收到的需求，遍历所有可能的场景，并设计复杂的测试脚本，作为后续工作的参照。这个流程的产物[插图]会让客户进行审批。批准后，测试人员就会依此来测试。

​	我们可以在该流程中的几个点做一些极简单的优化。最好的解决方案就是在每个迭代开始时，召集所有的项目干系人开个会。假如没有做迭代式开发，那么就在某个用户故事开始开发的前一周召开这样的会议。让客户、分析人员、测试人员坐在一起，找到最高优先级的测试场景。像Cucumber、JBehave、Concordion和Twist这类工具让你能在一个文本编辑器中用自然语言写验收条件，然后再写代码让这些验收条件变成可执行的测试，并且如果对这些测试代码进行重构，它们也会更新相应的测试规范。

​	另一种方法是为测试创立一种DSL（Domain-SpecificLanguage，领域专属语言），并用这种DSL来书写验收条件。

​	这些验收测试以及测试目标的简短描述就可以成为开发人员开发用户故事的起点。测试人员和开发人员在开发前应该尽早一起讨论这些验收测试。这会让开发人员更好地了解用户故事，并理解最重要的场景是什么样的。与开发完用户故事之后再沟通相比，这会大大减少开发人员和测试人员之间的反馈循环，有助于减小遗漏功能的几率，并有助于减少缺陷。

**管理待修复缺陷列表**

​	如果已经有一个待修复缺陷列表了，那么非常重要的一件事情就是将其可视化，让开发团队的每个人都认识到缩短待修复缺陷列表的责任。尤其当构建常常失败时，仅仅显示验收测试成功与否是不够的，还要显示测试通过的数量、失败的数量以及被忽略掉的测试数量，而且要放在比较显眼的位置。这样，可以让团队都关注这些问题。

​	还一种处理缺陷的方法，那就是像对待功能特性一样来对待缺陷。毕竟，修复缺陷和开发新功能一样，都需要花时间和精力。因此，客户可以将某个缺陷与要开发的新功能进行对比，得出它们的相对优先级。比如，一个出现概率很小的缺陷，只会影响少量用户，而且还有一个已知的临时解决方案，那么修复它的重要性可能要低于那些可以为用户带来收入的新功能。至少，我们可以把缺陷分为严重（critical）、阻塞（blocker）、中（medium）和低（low）四个级别。要想找到更全面的评估方法，我们可能还要考虑缺陷发生的频率，对用户的影响是什么，以及是否有临时解决方案等。

​	根据这种分类方式，就能在待修复缺陷列表中根据优先级将缺陷与用户故事按相同方式来排序，并可将二者一起放置。这样，除了可以避免“这是新功能，还是缺陷”的争论以外，还能一眼就看清楚还有多少工作要做，并相应地调整其优先级。低优先级的缺陷将被放在待修复缺陷列表中靠后的位置，就像对待低优先级的用户故事一样。客户也常常会选择不修复某些缺陷。因此，将缺陷和新特性一起放在待修复缺陷列表中也是管理它们的一种合乎逻辑的方法。

# 部署流水线解析

## 引言

​	持续集成的主要关注对象是开发团队。持续集成系统的输出通常作为手工测试流程和后续发布流程的输入。在软件的发布过程中，很多浪费来自于测试和运维环节。例如，我们常常看到：

* 构建和运维团队的人员一直在等待说明文档或缺陷修复；
* 测试人员等待“好的”版本构建出来；
* 在新功能开发完成几周之后，开发团队才能收到缺陷报告；
* 开发快完成时，才发现当前的软件架构无法满足该系统的一些非功能需求。

## 什么是部署流水线

​	从某种抽象层次上讲，部署流水线是指软件从版本控制库到用户手中这一过程的自动化表现形式。对软件的每次变更都会经历一个复杂流程才能发布。这一流程包括构建软件，以及后续一系列不同阶段的测试与部署，而这些活动通常都需要多人或者多个团队之间的协作。部署流水线是对这一流程的建模，在持续集成和发布管理工具上，它体现为支持查看并控制整个流程，包括每次变更从被提交到版本控制库开始，直到通过各类测试和部署，再到发布给用户的过程。

![](https://pic.imgdb.cn/item/60ced454844ef46bb2a2f923.jpg)

![](https://pic.imgdb.cn/item/60ced4ee844ef46bb2a83ac5.jpg)

​	随着某个构建逐步通过每个测试阶段，我们对它的信心也在不断提高。当然，我们在每个阶段上花在环境方面的资源也在不断增加，即越往后的阶段，其环境与生产环境越相似，其目的就是在这个过程中尽早发现那些不满足发布条件的构建版本，并尽快将失败根源反馈给团队。

![](https://pic.imgdb.cn/item/60ced50d844ef46bb2a944c6.jpg)

​	使用这种模式的话，有些非常重要的积极影响。首先，它可以有效地阻止那些没有经过充分测试或不满足功能需求的版本进入生产环境，也能避免回归缺陷，尤其是对于那些需要紧急修复并部署到生产环境的情况（因为和其他变更一样，这种紧急修复版本也需要走同样的流程）。根据我们的经验，最新发布的软件由于系统组件和其环境之间的未预期交互导致出现故障的事情是很常见的，比如使用了新的网络拓扑结构，或者生产环境的服务器在配置方面有些许不同。部署流水线的纪律会缓解这种现象。

​	其次，当部署和产品发布都被自动化之后，这些活动就变成快速、可重复且可靠的了。一旦被自动化，发布工作会变得非常容易，以至于会变成一件“平常”事，即只要你愿意，就可以做频繁发布。另外，如果支持自动安全回滚，发布风险也会大大降低，那么频繁发布就更不成问题了。一旦具有这种能力，发布就根本不会有什么风险了。最不济也就是引入一个严重缺陷，可这时只要回滚到之前没有缺陷的那个版本，然后在线下修复这个缺陷就可以了，没什么大不了的，详见第10章。

​	为了达到这种令人羡慕的状态，我们必须把那些用于证明某些版本满足业务要求的测试集合进行自动化。而且，我们还要把测试环境、试运行环境和生产环境上的部署过程自动化，这样可以避免那些手工密集型的易出错的步骤。对于很多系统来说，可能还需要其他形式的测试或者阶段，但对所有项目有一些阶段是共同具有的。

* 提交阶段是从技术角度上断言整个系统是可以工作的。这个阶段会进行编译，运行一套自动化测试（主要是单元级别的测试），并进行代码分析。
* 自动化验收测试阶段是从功能和非功能角度上断言整个系统是可以工作的，即从系统行为上看，它满足用户的需要并且符合客户的需求规范。
* 手工测试阶段用于断言系统是可用的，满足了它的系统要求，试图发现那些自动化测试未能捕获的缺陷，并验证系统是否为用户提供了价值。这一阶段通常包括探索性测试、集成环境上的测试以及UAT（User AcceptanceTesting，用户验收测试）。
* 发布阶段旨在将软件交付给用户，既可能是以套装软件的形式，也可能是直接将其部署到生产环境，或试运行环境（这里的试运行环境是指和生产环境相同的测试环境）。

**最基本的部署流水线**

![](https://pic.imgdb.cn/item/60ced82f844ef46bb2c2d63d.jpg)

​	这个流程的起点是开发人员向版本控制库提交代码。此时，持续集成管理系统对这次提交作出响应，触发该流水线的一个实例。第一个（提交）阶段会编译代码，运行单元测试，执行代码分析，创建软件二进制包。如果所有的单元测试都通过了，并且代码符合编码标准，就将可执行代码打包成可执行文件，并放到一个制品库（artifact repository）中。时新的持续集成服务器都提供保存这种过程产物的功能，并让用户和流水线的后续阶段能以某种非常简便的方式获取并使用。另外，还有很多像Nexus和Artifactory这样的工具可帮助管理这类过程产物。在提交阶段，你也许还会执行另外一些任务，比如为验收测试准备测试数据库。时新的持续集成服务器都支持通过构建网格并行执行这些任务。

​	第二个阶段通常由运行时间较长的自动化验收测试组成。因此，持续集成服务器最好支持将测试分成多组的做法，以便在构建网络中并行执行任务，这样会提高执行效率，使你更快地得到反馈（通常要在一两个小时之内返回结果）。这个阶段应该是流水线中第一个阶段成功完成以后自动触发的。

​	在此之后，部署流水线可能会有分支出现，这样就可以将该构建版本独立部署到多个不同的环境中，比如部署到用户验收测试环境、容量测试环境和生产环境。通常情况下，我们并不需要在验收测试阶段成功之后直接自动触发这些阶段。相反，我们希望让测试人员或运维人员可以做到自服务，即自己手工选择需要的某个版本，并将其部署到相应的环境中。为了做到这一点，需要有一个自动化部署脚本来执行这种部署过程。测试人员应当能够看到需要手工测试的所有构建版本，以及它们的状态（即已通过前两个阶段测试的版本，以及每个版本包含哪些修改，提交注释写了什么等）。之后单击一个按钮，运行相应的部署脚本将选定的构建版本部署到选定的环境上。

​	最后，一定要记住，我们所做的这一切都是为了尽快得到反馈。为了加速这个反馈循环，就必须能够看到每个环境中都部署了哪个版本，每个构建版本在流水线中处于哪个阶段。

![](https://pic.imgdb.cn/item/60ced8f2844ef46bb2c8f51b.jpg)

## 部署流水线的相关实践

### 只生成一次二进制包

​	很多构建系统将版本控制库中的源代码作为多个步骤中最权威的源，不同上下文中会重复编译这个源，比如在提交时、做验收测试时或做容量测试时。而且，在每个不同的环境上部署时都要重新编译一次。但是，对于同一份源代码，每次都重新编译的话，会引入“编译结果不一致”的风险。在后续阶段里，其编译器的版本可能与提交阶段所用版本不一致。对于第三方库，你可能会不小心使用了本未打算使用的版本。甚至编译器的配置都会对应用程序的行为产生影响。

>一种相关的反模式就是一直使用源代码，而不是二进制包。关于这种反模式更详细的讨论，请参见14.5.2节中的“ClearCase与从源重建反模式”。

​	这种反模式违反了两个重要原则。第一个原则就是“保证部署流水线的高效性，使团队尽早得到反馈”。重复编译违反了这一原则，因为编译需要花时间，在大型软件系统中进行的编译尤其如此。第二原则就是“始终在已知可靠的基础上进行构建”。被部署到生产环境中的二进制包应该与通过前面验收测试流程的二进制包是完全一样的。在很多实际使用的流水线里，每次生成二进制包时，都会存储其散列，并在后续每个阶段中利用这个散列对二进制包进行验证。

​	总之，二进制包应该只在构建流水线的提交阶段生成一次。这些二进制包应该保存在文件系统的某个位置上，让流水线的后续阶段能够轻松地访问到这个位置，但要注意不要放在版本控制库中，因为它只是一个版本的衍生品，并不是原生态的定义。大多数持续集成服务器能处理这类事情，而且会执行一些关键性的记录操作，让你能追溯到版本控制库中与之相关联的某次代码提交上。没有必要花太多时间和精力对这些二进制包进行备份，因为应该可以在版本控制库中的某个正确版本上，通过运行自动化构建精确地重新生成这个二进制包。

**为什么二进制包应该具有环境无关性**

​	我们认为，为每个环境都创建一个二进制包是一种不好的做法。尽管这种方法比较常见，但的确存在几个严重的缺点，不利于部署的灵活性、方便性和系统的可维护性，而有些工具恰恰鼓励你这么干。

​	如果构建系统是按这种方式组织的，它们很快就会变得非常复杂，最终会导致不得不利用某些特殊手法处理不同部署环境里的差异和特殊行为。我们曾遇到过这样一个项目，其构建系统非常复杂，以至于需要一个由五个人组成的全职团队来维护它。最终，通过重新组织构建流程，我们把与环境相关的配置和与环境无关的二进制包相分离，将他们从“火坑”中拯救了出来。

​	这种构建系统会将原本很简单的事情（比如向一个集群中增加一台服务器）搞得非常复杂，结果导致发布流程变得非常脆弱且成本很高。如果你的构建过程创建的二进制包只能在某些特定机器中运行，那现在就开始计划重新组织它吧！

### 对不同环境采用同一部署方式

​	为了确保构建和部署流程被有效测试，在各种环境中使用相同流程对软件进行部署是非常必要的，这些环境即包括开发人员或分析人员的工作站，也包括测试环境和生产环境。

​	能够使用相同的脚本向开发环境和生产环境部署，是避免“它在我的机器上可以工作”病症的法宝 [c29ETR]。如果能做到这种程度的话，那么在版本即将发布前，部署流程就已经在其他环境中测试过数百次了。这是我们所知道的缓解软件发布风险的最好方法之一。

相反，如果使用同一个脚本在所有的环境上进行部署，那么当在某个环境上部署失败时，就可以确定其原因一定来自以下三个方面：

* 与该环境相关的配置文件中，某项配置有问题；
* 基础设施或应用程序所依赖的某个服务有问题；
* 环境本身的配置有问题。

### 对部署进行冒烟测试

​	当做应用程序部署时，你应该用一个自动化脚本做一下冒烟测试，用来确保应用程序已经正常启动并运行了。这个测试应该非常简单，比如只要启动应用程序，检查一下，能看到主页面，并在主页面上能看到正确的内容就行了。这个冒烟测试还应该检查一下应用程序所依赖的服务是否都已经启动，并且正常运行了，比如数据库、消息总线或外部服务等。

### 向生产环境的副本中部署

​	很多团队实际部署应用上线时可能遇到的另一个主要问题是，生产环境与他们的开发环境或测试环境有非常大的差异。为了对系统上线充满信心，你要尽可能在与生产环境相似的环境中进行测试和持续集成。

​	另外，要想确保所有的环境都一样，需要有很多纪律保障良好的配置管理实践。你要确保：

* 基础设施是相同的，比如网络拓扑和防火墙的配置等；
* 操作系统的配置（包括补丁版本）都是相同的；
* 应用程序所用的软件栈是相同的；
* 应用程序的数据处于一个已知且有效的状态。系统升级过程中需要进行的数据迁移是部署活动的一个痛点，我们将在第12章讲这个问题。

### 每次变更都要立即在流水线中传递

​	在持续集成出现之前，很多项目都有一个各阶段的执行时间表，比如每小时构建一次，每天晚上运行一次验收测试，每个周末运行一次容量测试。部署流水线则使用了不同的方式：每次提交都要触发第一个阶段的执行，后续阶段在第一个阶段成功结束后，立即被触发。当然，假如某些阶段需要花较长的时间，而开发人员（尤其是在大型团队中）的提交又非常频繁，就很难做到这一点了。

![](https://pic.imgdb.cn/item/60cedfa2844ef46bb2010f34.jpg)

​	另一种构建策略是，一旦代码构建和单元测试结束，持续集成系统就去检查版本库中是否有新的提交。如果有的话，就将最近还没有构建过的所有变更全部拿来进行构建，即对版本4进行构建。假设这次构建和单元测试失败了，那么构建系统是无法知道究竟是哪个版本（版本3还是版本4）引起的，但开发人员自己可以很容易发现问题在哪儿。

### 只要有环节失败，就停止整个流水线

​	为了达到本书所描述的目标（迅速、可重复且可靠的发布），对于团队来说，最重要的是要接受这样的思想：每次提交代码到版本控制系统中后，都能够构建成功并通过所有的测试。对于整个部署流水线来说，都适用这一要求。假如在某个环境上的某次部署失败了，整个团队就要对这次失败负责，应该停下手头的工作，把它修复后再做其他事情。

## 提交阶段

在提交阶段，我们需要做以下几件事。这些任务通常作为一个工作集合运行在构建网格上（大多数持续集成服务器都提供类似功能），这样，提交阶段就能够在一个可接受的时间之内完成（最好在五分钟之内完成，最多不能超过十分钟）。一般来说，提交阶段包含以下步骤：

* 编译代码（如果所用开发语言需要的话）；
* 运行一套提交测试；
* 为后续阶段创建二进制包；
* 执行代码分析来检查代码的健康状况；
* 为后续阶段做准备工作，比如准备一下后续测试所用的数据库。

测试非功能特性（比如容量）可能比较困难，但仍旧可以通过一些分析工具，收集一些关于当前代码库的测试覆盖率、可维护性以及安全漏洞方面的信息。为这些度量项设定一个阈值，并像对待测试一样，一旦不满足阈值条件，就让提交阶段失败。比较有用的度量项包括：

* 测试覆盖率（如果提交测试只覆盖了代码库的5%，那么这些测试发挥不了太大的作用）；
* 重复代码的数量；
* 圈复杂度（cyclomatic complexity）；
* 输入耦合度（afferent coupling）和输出耦合度（efferent coupling）；
* 编译警告的数量；
* 代码风格。

**提交阶段最佳实践**

​	在理想情况下（无限的处理能力和无限的网络带宽），我们希望开发人员能够一直等到所有测试（甚至是手工测试）全部通过，这样一旦出现问题，就可以马上修复。然而，这并不现实，因为部署流水线的后续阶段（自动化验收测试、容量测试和手工验收测试）都需要相对较长的时间。这也是规范测试流程的一个理由，因为当缺陷还比较容易修复时，尽快得到反馈是非常重要的，而不应花更大的代价得到全面的反馈。

## 自动化验收测试之门

**为什么仅有单元测试是不够的**

​	单元测试过了，但是系统可能运行不起来。

**自动化验收测试最佳实践**

​	仔细考虑应用程序所要被部署到的生产环境是非常重要的。如果生产环境能完全在开发团队的控制之中，那么这个开发团队真的很幸运。此时，只要在这一环境的副本上运行验收测试就可以了。如果生产环境非常复杂或者非常昂贵，我们可能就要使用它的简化版了，比如仅使用两个中间件服务器，尽管生产环境中可能会有很多个。如果应用程序对外部服务有依赖，可以使用测试替身来模拟所依赖的外部基础设施。

## 后续的测试阶段

### 手工测试

​	在迭代开发过程中，验收测试之后一定会有一些手工的探索性测试、易用性测试和演示。在此之前，开发人员可能已经向分析师和测试人员演示了应用程序的功能，但一定是在自动化测试通过之后。在这个过程中，测试人员所扮演的角色并不是回归测试该系统，而是首先通过手工证明验收条件已被满足，从而确保这些验收测试的确是验证了系统行为。

### 非功能测试

​	根据我们的经验，如果需要的话，完全可以在部署流水线中创建一个阶段，用于运行这些自动化的非功能测试。

## 发布准备

​	缓解这类风险非常简单，只要把这个发布环节视为部署流水线的一个自然结果就行。实际上，我们只需要：

* 让参与项目交付过程的人共同创建并维护一个发布计划（包括开发人员和测试人员，以及运维人员，基础设施和支持人员）；
* 通过尽可能多的自动化过程最小化人为错误发生的可能性，并从最容易出错的环节开始实现自动化；
* 在类生产环境中经常做发布流程演练，这样就可以对这个流程及其所使用的技术进行调试；
* 如果事情并没有按计划执行，要有撤销某次发布的能力；
* 作为升级和撤销过程的一部分，制定配置迁移和数据迁移的策略。

### 自动部署与发布

​	对生产环境的控制权越小，遇到意外情况的可能性就越大。因此，无论何时发布软件系统，我们都希望有完全的控制权。然而，这里至少有两方面的约束。首先，对于很多应用程序来说，你根本不能完全控制应用程序所在的运行环境。对于由用户自行安装的软件（比如游戏或者办公软件）来说，这一点是必然的。通常解决这个问题的办法就是选择一些具有代表性的目标环境，并分别在这些样本环境上执行自动化验收测试套件。这样就能通过收集结果数据发现哪些测试在哪些平台上无法正常运行了。

​	第二个约束就是，人们通常认为为了达到完全控制环境所付出的成本会高于因此得到的收益。然而，事实常常恰好相反。生产环境中的大多数问题往往是由不充分的控制导致的。正如我们在第11章中所讲的，生产环境应该是完全受控的，即对生产环境的任何修改都应该通过自动化过程来完成。这不仅包括应用程序的部署，还包括对配置、软件栈、网络拓扑以及状态的所有修改。只有在这种方式下，我们才可能对它们进行可靠地审计和问题诊断，并在可预计的时间内修复它们。随着系统复杂性的增加，不同类型服务器的增多，以及不断提高的性能需求，我们就更需要这种程度的控制力。

### 变更的撤销

​	传统上，人们对新版本的发布常常存在着恐惧心理，原因有两个。一是害怕引入问题，因为手工的软件发布过程很可能引入难以发现的人为错误，或者部署手册本身就隐藏着某个错误。二是担心由于发布过程中的一个问题或新版本的某个缺陷，使你原来承诺的发布失败。无论是哪种情况，你的唯一希望就是足够聪明且非常迅速地解决这个问题。

​	我们可以通过每天练习发布多次来证明自动化部署系统是可以工作的，这样就可以缓解第一种问题。对于第二个问题，可以准备一个撤销策略。最糟的情况也就是回滚到发布之前的状态，这样你就有足够的时间评估刚发现的问题，并找到一个合理的解决方案。

​	最好的撤销策略是在发布新版本时，让旧版本仍旧处于可用状态，并在发布后保持一段时间。这是我们将在第10章讨论的一些部署模式的基础。对于很简单的应用程序来说，这是可以做到的（忽略数据和配置信息的迁移），只要把每个版本都放在一个单独的目标中，再使用符号链接指向当前版本就行了。最复杂的情况就是在部署和撤销中涉及生产数据的迁移。

​	另一种比较好的撤销策略是从头开始重新部署旧版本。为此，与部署流水线中的其他环境一样，我们就应当能通过单击按钮的方式来发布已通过所有测试阶段的任意一个版本。某些软件完全可以达到这种理想状态，甚至那些数据量相当大的系统也可以做到这一点，而对另外一些系统来说，即使不差钱儿，对于某些个别的变更，提供这种具有版本无关性的撤销也是相当耗时的。无论怎样，有目标、有理想总是好的，因为它为每个项目都设定了一个努力方向。即使在某些方面做得不够好，但你的方法越接近这种理想状态，部署就会越容易。

​	撤销流程绝不应该与部署流程、增量部署流程或回滚流程有什么不同。然而，这些流程可能很少被测试，所以也就不可靠。

### 在成功的基础上构建

​	当一个候选发布版本能够部署到生产环境时，我们就确信：

* 代码可以编译；
* 代码能够按开发人员的预期运行，因为它通过了单元测试；
* 系统能够满足分析人员或用户预期，因为它通过了所有的验收测试；
* 基础设施的配置和基线环境被恰当地管理了，因为应用程序在模拟的生产环境上通过了测试；
* 系统所有的正确组件都就绪了，因为它是可以部署的；
* 部署脚本也是可以工作的，因为在该版本到这一阶段之前，部署脚本至少在开发环境中用过一次，在验收测试阶段用过一次，在测试环境中用过一次；
* 我们需要部署的所有内容都在版本控制库中，而且不需要手工干预，因为我们已经部署这个系统好几次了。

## 实现一个部署流水线

​	接下来，我们将描述如何从无到有，建立一个完整流水线的策略。一般来说，步骤是这样的：

* 对价值流建模，并创建一个可工作的简单框架；
* 将构建和部署流程自动化；
* 将单元测试和代码分析自动化；
* 将验收测试自动化；
* 将发布自动化。

### 对价值流进行建模并创建简单的可工作框架

​	第一步就是画出从提交到发布整个过程的价值流图。如果项目已经建好并开始运行，你在半个小时内就能画完。然后和参与其中的每个人聊一下，记录下流程中的每个步骤，包括对经历时间（elapsed time）和增值时间（value-added time）的最佳估计值。如果是还没有启动的新项目，就要先设计一个合适的价值流，可以在同一组织中找个与你的项目相似的项目，思考它的价值流，也可以从最简单的价值流开始，即第一个阶段是提交阶段，用来构建应用程序并运行基本的度量和单元测试，第二个阶段用来运行验收测试，第三个阶段用来向类生产环境部署应用，以便用它来做演示。

​	一旦有了价值流图，就可以用持续集成和发布管理工具对流程建模了。如果所用工具不支持直接对价值流建模的话，可以使用“项目[插图]间依赖”来模拟它。首先，这些项目应该什么也不做，而只是作为可以被依次触发的占位符。如果是使用“最简单模型”，每当有人提交代码到版本控制系统时，就应该触发提交阶段。当提交阶段通过以后，验收测试阶段就应该被自动触发，并使用提交阶段刚刚创建的二进制包。为手工测试或发布应用而向类生产环境部署二进制包的阶段，都应该会要求你具有通过单击按钮来选择到底部署哪个版本的能力，而这种能力通常都需要授权。

### 构建和部署过程的自动化

​	每当有人提交后，持续集成服务器就应执行构建——使用3.2节所列出的某个工具。持续集成服务器应该监视版本控制系统，每当发现有新提交的代码时，就签出或更新源代码，运行自动化构建流程，并将生成的二进制包放在文件系统的某个地方，使整个团队都能通过持续集成服务器的用户界面获取。

​	一旦持续构建流程建立并运行起来了，接下来就要做自动化部署了。首先，要找到能够部署应用程序的机器。对于刚启动的新项目，用持续集成服务器所在的机器也行。如果项目已比较成熟，可能就需要找几台专用机器了。这些环境可以称作试运行环境或者用户验收测试（UAT）环境（这在各组织中的叫法不同）。

​	部署活动可能包含：（1）为应用程序打包，而如果应用程序的不同组件需要部署在不同的机器上，就要分别打包；（2）安装和配置过程应该实现自动化；（3）写自动化部署测试脚本来验证部署是否成功了。部署流程的可靠性是非常重要的，因为它是自动化验收测试的前提条件。

### 自动化单元测试和代码分析

​	流水线的验收测试阶段可以重用向测试环境部署的脚本。唯一的不同之处就是在冒烟测试之后，就要启动验收测试框架，并在结束之后，为进行分析收集所有的测试结果报告。另外，最好也保存一下应用程序的运行日志文件。如果应用程序有图形用户界面的话，也可以在验收测试运行时使用一个像Vnc2swf这样的软件来进行屏幕录像，这对于诊断问题比较有用。

### 部署流水线的演进

​	我们发现，每个价值流图和流水线中几乎都有上面描述的步骤。通常这些是自动化的第一个目标。随着项目越来越复杂，价值流图也会演进。另外，对于流水线来说，还有两个常见的外延：组件和分支。大型应用程序最好由多个组件拼装而成。在这样的项目中，每个组件都应该有一个对应的“迷你流水线”，然后再用一个流水线把所有组件拼装在一起，并运行整个验收测试集（包括自动化的非功能测试），然后再部署到测试环境、试运行环境和生产环境中。

## 度量

​	反馈是所有软件交付流程的核心。改善反馈的最佳方法是缩短反馈周期，并让结果可视化。你应该持续度量，并把度量结果以一种让人无法回避的方式传播出去，比如使用张贴在墙上的海报或者用一个专门的计算机显示器以大号粗体字显示结果，这些设备就是信息辐射器。

​	对于软件交付过程来说，最重要的全局度量指标就是周期时间（cycle time）。它指的是从决定要做某个特性开始，直到把这个特性交付给用户的这段时间。正如Mary Poppendieck所问的那样：“你所在的组织中，如果仅仅修改一行代码，需要多长时间才能把它部署到生产环境中？你们是否以一种可重复且可靠的方式做这类事情？”[插图]这个指标很难度量，因为它涉及软件交付过程中的很多环节（从分析到开发，直至发布）。然而，这个指标比其他任何度量项都更能反映软件交付过程的真实情况。

​	一旦知道了应用程序的周期时间，就能找到最佳办法来缩短它。你可以利用约束理论，按照下面的流程来做优化。

（1）识别系统中的约束，也就是构建、测试、部署和发布这个流程中的瓶颈。随便举个例子，比如手工测试部分。

（2）确保供应，即确保最大限度地提高流程中这部分的产出。在我们的例子中（手工测试），就是保证总是有用户故事在等待手工测试，并确保手工测试所需的资源不会被其他工作占用。

（3）根据这一约束调整其他环节的产出，即其他资源都不会100%满负荷工作。比如，开发人员全力开发用户故事时，等待测试的用户故事会越积越多。因此，只要开发人员开发用户故事的速度能及时供应手工测试就可以了，其他时间他们可以写些自动化测试来捕获缺陷，这样测试人员就不需要在手工测试上花太长时间了。

（4）为约束环节扩容。如果周期时间还是太长（换句话说，第（2）步和第（3）步都没有什么太多的帮助），就要向该瓶颈环节增加资源了，比如聘用更多的测试人员，或者在自动化测试方面投入更多的精力。

（5）理顺约束环节并重复上述步骤，即在系统中找到下一个约束，并重复第（1）步。



​	尽管周期时间是软件交付中最重要的度量项，但还有一些其他度量项可以对问题起到警报作用。这些度量项如下所示。

* 自动化测试覆盖率。
* 代码库的某些特征，比如重复代码量、圈复杂度、输入耦合度、输出耦合度、代码风格问题等。
* 缺陷的数量。
* 交付速度，即团队交付可工作、已测试过并可以使用的代码的速率。
* 每天提交到版本控制库的次数。
* 每天构建的次数。
* 每天构建失败的次数。
* 每次构建所花的时间，包括自动化测试的时间。



​	如何呈献这些度量项是值得斟酌的。上面这些报告会产生很多数据，而如何解析这些数据就是一门艺术。比如程序经理可能想在一个项目健康报告中以非常简单的红黄绿交通信号灯方式看到已分析的聚合数据，而不是看到一页又一页的报告。相比之下，一个团队中资深的软件工程师会希望看到更详细的情况，但也不会乐意查看多页的报告。我们的同事Julias Shaw创建了一个叫做Panopticode的项目，可以在Java代码上运行一系列这样的报告，生成丰富的密集的可视化报告（如图5-8所示），使你一眼就能知道代码库是否存在问题，以及问题在哪儿。我们所要强调的是，一定要创建一个聚合所有信息，并且人脑可以直接通过其无比的模式匹配能力识别流程或代码库中问题的可视化报告。

![](https://pic.imgdb.cn/item/60ceeddb844ef46bb28d25db.jpg)

## 小结

略

# 构建与部署的脚本化

## 引言

​	一旦项目变得复杂了，比如有多个组件或者不太常见的打包方式，你就需要撸起袖子，动手构建脚本了。

​	自动化部署则稍微麻烦一点儿。向测试环境和生产环境部署软件的过程不可能是“复制一个二进制文件到生产环境，然后就坐在那里等着就了事儿”那么简单。大多数情况下，它需要一系列的步骤，比如配置应用程序、初始化数据、配置基础设施、操作系统和中间件，以及安装所需的模拟外部系统等。项目越复杂，这样的步骤就越多，所需时间越长，而且（如果没有自动化的话）就越容易出错。

## 构建工具概览

​	例如，假如你想运行测试，就需要编译自己的代码和测试，并设置测试数据，以及编译与初始化环境相关的所有东西。图6-1显示了一个依赖网络的简单例子。

![](https://pic.imgdb.cn/item/60d00083844ef46bb256e8ed.jpg)

​	每个任务都包括两点内容，一是它做什么，二是它依赖于什么。在每个构建工具中都会对这两点进行建模。

​	构建工具会遍历整个网络，调用（但并不一定执行）每个任务。

​	面向产品的构建工具将状态以时间戳的形式保存在每个任务执行后生成的文件中（SCons使用MD5签名）。这在编译C或C++程序时非常好，因为Make会保证只编译那些自上次构建后发生过修改的源代码文件。在大型项目中，这种特性（称为增量式构建）会比全量构建节省数小时。在C/C++项目中，通常编译会花较长的时间，因为编译器会做很多优化代码的工作。对于运行于虚拟机上的语言来说，编译器只创建字节码就行了，虚拟机运行时（JIT）编译器会在运行时进行这种优化。

​	下面，我们简单总结一下当前流行的构建工具。本书网站[dzMeNE]上有很多使用这些技术的构建脚本的例子和参考。

### Make

​	Make和它的变种仍旧活跃在系统开发领域。它是一种强大的产品导向的构建工具，能在单次构建中追踪依赖关系，还能只构建那些受到本次修改影响的组件。

​	Make也有很多缺点。随着应用程序复杂程度和组件之间依赖关系的增加，这种复杂性会让Make变得越来越难以调试。

​	在某些情况下，空白字符的影响非常大，所以很容易在Makefile中引入一些难以发现的缺陷。比如在一个命令脚本中，那些传给shell的命令必须有一个制表符在前面。如果相反地使用了空格，这个脚本就无法正常工作了。

​	Make的另一个缺点是，它依赖于shell做所有的事情。结果，Makefile就不得不与操作系统绑定在一起了。（的确，很多工作就由Make周边的一堆工具来承担，以便构建脚本可以在UNIX的多种变种系统中运行。）由于Makefile是一种外部的DSL（Domain-Specific Language，领域特定语言），并不提供对核心系统的扩展能力（除非定义新的规则）。在无法使用Make的内部数据结构的前提下，所有的扩展都必须重建公共解决方案。

​	现在很多C/C++的项目中，开发人员更倾向于使用SCons，而不是Make。SCons本身和它的构建文件都是用Python写的，这让它成为了比Make更强大和更适用的工具。它有很多非常有用的特性，比如支持Windows和并行构建。

## Ant

​	Java社区也先后经历了几种解决方案，先是将Make本身移到Java上。与此同时，XML作为构建结构化文档的方便方法开始崭露头角。二者的融合就产生了Apache的构建工具Ant。

​	Ant是一个任务导向的构建工具。Ant的运行时组件也是用Java写的，但Ant脚本是用XML书写的一种外部DSL。这种结合使Ant具有了强大的跨平台能力。它也是极其灵活和强大的系统，因为Ant的任务几乎可以让你做任何想做的事情。

​	然而，Ant也有几个缺点。

* 你要用XML写构建脚本，可XML的脚本既不简洁，又不易阅读。
* Ant是一个贫血领域模型。任务上没有真正的领域概念，所以要花大量的时间为编译、生成Jar、运行测试等编写样板文件。
* Ant是声明式语言，而非命令式语言，但提供了少量的命令式标签（比如糟糕可怕的<antcall>）给用户使用。
* 关于Ant 任务，它没法回答类似下面这样的问题，比如“运行了多少个测试”和“它们花了多长时间”。你能做的就是找一个工具把这些信息输出到命令行窗口中，然后对其进行解析，或者写一些Java代码做个钩子，放在Ant中。
* 尽管Ant通过import和macrode任务支持重用，但对新手用户来说，它们很难理解。



​	由于这些局限性，Ant文件会很长，也很难重构（数千行的Ant文件很常见）。当使用Ant时，非常值得一读的文章就是ThoughtWorks公司的咨询师JulianSimpson写的“Refactoring Ant Build Files”，它发表在了ThoughtWorks文集《软件开发沉思录》中。

### NAnt 与 MSBuild

​	略

### Maven

​	Maven通过为Java项目的代码组织结构定义一些假设前提，形成一个比较复杂的模型，试图以此消除Ant文件中大量的样板文件。这种流行的“惯例胜于配置”（convention over configuration）的原则意味着，只要项目按Maven指定的方式进行组织，它就几乎能用一条命令执行所有的构建、部署、测试和发布任务，却不用写很多行的XML。这包括为项目创建网站，用来默认宿主应用软件的所有Javadoc。

​	Maven另一个重要的特性是，它能自动管理Java库和项目间的依赖，而这正是大型Java项目的一个痛点。Maven还支持一种复杂且严格的软件分区方案，使你能将复杂的解决方案分解成较小的组件。

​	Maven的问题有三个。首先，如果项目没有按Maven规定的结构和生命周期来组织的话，你很难（甚至不可能）使用Maven。当然，在一些组织中，这也可能被认为是一种特点，它迫使开发团队根据Maven的规范组织项目结构。对于缺乏开发经验或有很多项目的组织，这是一件好事，但如果想要做一些“打破常规”的事（比如在执行测试之前加载一些定制测试数据），你就要颠覆Maven的生命周期和领域模型，而这个过程相当痛苦，而且难以维护，但通常是不可避免的。Ant比Maven灵活得多。

​	Maven的第二个问题是，它也需要用XML写的外部DSL。也就是说，为了扩展它，你要写代码。尽管写Maven插件并不很复杂，但绝对不可能在几分钟之内搞定。你要学习Mojos、插件描述符，以及Maven所用的控制反转（inversion-of-control）框架。幸运的是，Maven有很多插件，对于一般的Java项目，你几乎能找到所有想要的插件。

​	Maven的第三个问题是，在默认配置中，它是自更新的。Maven的内核非常小，为了让自己能够工作，它要从因特网上下载它自己的插件。Maven每次运行时都会尝试更新自己，而这种插件的自动升降级有可能导致不可预期的失败。更严重的结果是，你很可能无法重现某次构建。与之相关的一个问题是，Maven的库和依赖管理功能允许在多个项目之间使用组件的快照。如果使用这种快照依赖的话，就更难重现某次构建了。

​	对于某些团队来说，Maven的约束可能过于严格了，或者需要很多精力才能将项目整理成符合Maven的规定的结构。所以，他们宁可使用Ant。最近，出现了叫做Ivy的工具，它可以在多个组件之间管理库文件和依赖，而不需要使用Maven。将它与Ant结合使用，在某种程度上，可以得到与使用Maven一样的效果。

### Rake

​	可是XML令这些语言很难编写、阅读、维护和扩展。主流的Ruby构建工具Rake作为一个试验品出现了，它是否能够通过在Ruby中创建内部DSL来轻松完成Make的相应功能呢？答案是肯定的。Rake和Make一样是产品导向的工具，但也可以用作任务导向的工具。

​	像Make一样，Rake只能理解任务和依赖。然而，由于Rake脚本是纯Ruby的，所以你可以用Ruby的API来执行任何任务。因此，用Rake可以轻松写出强大且与平台无关的构建文件，因为你能使用通用编程语言的所有本地化功能。

​	Rake也有两个不便之处：首先，要确保在你的平台上装有适当的Ruby运行时环境（作为最方便最可靠的平台，JRuby势头强劲）；其次，要组合使用RubyGems。

### Buildr

​	Rake的简单和强大令“构建脚本应该用一个真正的编程语言编写”有了一个令人信服的理由。新一代构建工具，比如Buildr、Gradle和 Gantt都使用了这种方式。它们都以内部DSL的形式构建软件。然而，它们试图让复杂的依赖管理和多项目构建变得简单。

​	如果刚开始一个Java项目，或是想找Ant或Maven的替代品，我们强烈推荐Buildr，如果你喜欢Groovy中的DSL，就用Gradle吧。

### Psake

​	略

## 构建部署脚本化的原则与实践

### 为部署流水线的每个阶段创建脚本

​	我们是DDD（Domain-Driven Design，领域驱动设计）的忠实粉丝，所以，在我们设计的任何软件中都会使用这一技术，对于设计构建脚本也不例外。如果想让构建脚本的结构清晰地表达构建流程，这可能有点儿不切实际。使用这种方法，我们可以确保在维护构建部署系统和最小化组件间依赖的过程中，还能令脚本具有良好的结构。幸运的是，部署流水线提供了一种优秀的组织原则，可使构建脚本间的职责清晰明确。

​	当项目刚开始时，可以将部署流水线中的每个操作都放在同一个脚本文件中，即使是那些还没有被自动化的步骤，也可以有对应的哑操作。但是，一旦脚本变得太长，就要将它们分成独立的脚本，让部署流水线中的每个阶段分别使用单独的脚本。这样，一个提交阶段的脚本就可以完成编译、打包、运行提交测试套件和执行代码静态分析的工作。功能验收测试脚本会调用部署工具，将应用程序部署到适当环境中，并准备相关数据，之后再运行验收测试。你还可再用一个脚本运行任何非功能测试，比如压力测试和安全测试。

### 使用恰当的技术部署应用程序

​	在典型的部署流水线里，提交阶段之后的大多数阶段（比如自动化的验收测试阶段和用户验收测试阶段）都需要把应用程序部署到类生产环境中，所以部署自动化也是非常关键的。然而，在做自动化部署工作时，应该使用恰当的工具，而不是通用脚本语言（除非部署流程十分简单）。几乎每种中间件都有相应的工具来配置和部署它，那就使用它们吧。

​	最重要的是，开发人员（至少可以在他们自己的开发机器上）、测试人员和运维人员都要做应用程序的部署工作。因此，他们要共同判定如何部署应用程序。这件事也要在项目一开始就做。

**运维人员和开发人员必须合作规划部署流程**

​	部署脚本应该能够完成应用程序的安装和升级任务。在部署之前，它要能够关闭当前运行的版本，而且既支持在当前的数据库上升级，又能够从头创建数据库。

### 使用同样的脚本向所有环境部署

​	使用同样的流程部署应用程序到每个环境是非常必要的，这样就能确保构建和部署流程能经过有效测试。也就是说，“使用同样的脚本部署每个环境”和“环境配置信息的不同（比如服务URI或IP地址）”这两件事应该分开管理，即将配置信息从脚本中分离出来，并将其保存在版本控制库中。

​	（1）构建和部署脚本在开发机器和类生产环境上都能运行；

​	（2）开发人员使用这些脚本进行所有的构建和部署活动。对于并行构建系统来说，很容易变成“只有开发人员使用这些脚本”，但这就丢失了可以令构建部署脚本保持灵活性、很好地被重构和测试的关键因素。如果应用程序还依赖于公司内部开发的其他组件，就要确保能很方便地将其正确版本（已知与我们的应用程序相匹配的版本）放到开发机器上，这时Maven和Ivy这样的工具就能够派上用场。

### 使用操作系统自带的包管理工具

​	如果只有一种目标操作系统，或者一组相似的操作系统，我们强烈推荐使用操作系统自身的包管理技术把需要部署的文件打包在一起。例如，Debian和Ubuntu都使用Debian的包管理系统，RedHat、SuSE和很多其他Linux发行版都使用RedHat包管理系统，Windows用户可以使用Microsoft Installer系统等。所有这些包管理系统都相对容易使用，并有很好的工具支持。

​	如果在部署过程中需要把文件放在多个文件夹中或向注册表中增加键，那么就用一个包管理系统完成这样的任务吧。这会带来很多好处，不但令应用程序的维护变得非常简单，而且在部署流程中就可以使用像Puppet、CfEngine和Marimba这样的环境管理工具。只要将包放到组织级的代码库中，让这些工具来安装正确版本的包就可以了，就像让它们安装Apache的正确版本一样。假如要把不同的文件安装到不同的机器上（比如当使用N层架构时），你就可为每一层或每一类机器分别创建一个安装包。对二进制文件进行打包的工作也应该是部署流水线中需要实现自动化的部分。	

​	当然，并不是所有的部署都能用这种方式来管理。比如商业化中间件服务器就经常需要使用特定的工具来执行部署。此时，就必须使用混合方法了。准备好所有并不需要那些特定工具的东西，然后再使用这些特定工具执行部署过程的后续部分就行了。

当然，并不是所有的部署都能用这种方式来管理。比如商业化中间件服务器就经常需要使用特定的工具来执行部署。此时，就必须使用混合方法了。准备好所有并不需要那些特定工具的东西，然后再使用这些特定工具执行部署过程的后续部分就行了。

### 确保部署流程是幂等的（Idempotent）

​	无论开始部署时目标环境处于何种状态，部署流程应该总是令目标环境达到同样（正确）的状态，并以之为结束点。

​	做到这一点的最简单方法就是，将已知状态良好的基线环境作为起点，要么是通过自动化，要么是通过虚拟化方式准备好的。这里所说的环境包括所有需要用到的中间件，以及让应用程序能正常工作的任何软硬件。然后，部署流程可以获取指定的应用程序版本，并使用（对于中间件来说）适当的部署工具将其部署到该环境中。

​	对于这一原则，也有一些例外情况。首先，对于集群系统来说，总是将整个集群系统同时重新部署就不可取。

​	其次，如果应用程序是由多个组件构成的，而这些组件来源于不同的源代码库，那么二进制包就由这些源代码库中的一系列修正版本（x、y、z……）来定义。此时，如果你知道仅有一个组件发生了变更，而且将要部署到生产环境的所有组件的组合都已经测试通过了的话，那么只部署这个发生变更的组件就行了。这里的关键区别在于从上一个状态更新到新状态的过程已被测试过。这一原则也适用于面向服务架构的服务部署上。

​	最后，还有一种方法，那就是使用效果幂等的工具进行部署。比如，无论目标目录中的文件处于什么状态，Rsync都会使用一种强大的算法，仅通过网络传输目标目录与源目录中不同的部分，确保某系统上的目标目录与另一个系统中的源目录是完全一样的。版本控制的目录更新也能达到相似的结果。

### 部署系统的增量式演进

​	每个人都能看到一个完全自动化的部署过程的魅力，即“单击按钮即可发布软件”。当某个大型企业应用系统以这种方式部署时，看起来就像变魔术一般。但魔术有一个问题，即从外部看会显得极为复杂。事实上，当你查看我们的部署系统时会发现，它只是由一组非常简单的、增量的步骤组成的复杂系统，而这些步骤也是随着项目的进行不断完善的。

## 面向JVM的应用程序的项目结构

​	因为尽管有一些有用的惯例，但如果不使用Maven[插图]的话，这些就只是惯例，而不是规定。如果开发人员能够遵守这些标准结构的话，生活会更美好一些。另外，花一点儿精力也可以将下面的知识用到其他技术平台上。尤其是对于．NET项目来说，可以卓有成效地使用完全相同的结构，只是要把“/”换成“\”。

**源代码管理**

​	请坚持遵循标准的Java实践，将文件放在以包名为目录名的目录中，每个文件保存一个类。Java编译器和所有时新的开发环境都会使用这种惯例，但仍有人会违反它。如果不遵循它或语言的其他惯例，有可能引入很难被发现的缺陷。

**测试管理**

​	请将所有要测试的源代码都放在test/[language]目录中。单元测试应该放在与包名相对应的目录中。也就是说，某个类的测试应该与该类放在同一个包中。

**构建输出的管理**

​	当用Maven做构建时，它把所有的东西都放在项目根目录中一个叫做target的文件夹中，包括生成的代码、元数据文件（如Hibernate映射文件）等。将这些内容放在一个单独的目录中能让我们更容易清除前一次构建结果，因为只要把整个目录删除就行了。不要把这个目录中的东西提交到版本控制库中，而如果打算把二进制文件提交到版本控制库中，请将它们先复制到另一个存储库中再提交。源控制系统应该忽略target目录。

​	无论使用什么样的策略，你都要记住，创建多个JAR文件的目的有两个：一是令应用程序的部署更简单；二是令构建流程更加高效，并将构建依赖图的复杂性最小化。这些是应用程序打包的指导方针。

**库文件管理**

​	库文件的管理有几种不同的选择。一是完全交给工具来管理，比如Maven或Ivy工具。这时就不需要将库文件提交到版本控制库中，只需要声明一下项目中所依赖的库文件就可以了。另一个极端是把库文件（包括构建、测试和运行时必需的所有库文件）都提交到版本控制库中，最常见的做法是将它们放在项目根目录下的lib文件夹中。我们喜欢根据其用途，将这些库放在不同的目录中，比如构建时、测试时和运行时。

​	一种比较高级的做法是建立组织级的第三方依赖库，将所有项目需要的所有依赖库文件都放在其中。Ivy和Maven都支持仓库自定义。在强调纪律的组织中，通常用这种方式。

## 部署脚本化

​	环境管理的核心原则之一就是：对测试和生产环境的修改只能由自动化过程执行。也就是说，我们不应该手工远程登录到这些环境上执行部署工作，而应该将其完全脚本化。有三种方式执行脚本化的部署。首先，如果系统只运行在一台机器上，我们就可以写一个脚本，让它在那台机器上本地执行所有的部署活动。

​	有三种方法做远程部署。第一种方法就是写个脚本，让它登录到每台机器上，运行适当的命令集。第二种方法是写个本地运行的脚本，在每台远程机器上安装一个代理（agent），由代理在其宿主机上本地运行该脚本。第三种方法就是利用操作系统自身的包管理技术打包应用程序，然后利用一些基础设施管理或部署工具拿到新版本，运行必要的工具来初始化你的中间件。第三种方式最为强大，理由如下。

* 像ControlTier和BMC BladeLogic这类部署工具，以及像MarionetteCollective、CfEngine和Puppet这样的基础设施管理工具都是声明式的而且是等效的，即使在部署时某些机器停机了，或者新增机器或VM时，它们都能确保将正确版本的二进制包安装到所有机器上。关于这此工具的更多信息，参见第11章。
* 你还可以使用同一套工具管理应用程序的部署以及基础设施。由于同一组人（运维团队）同时负责这两件事情，而这两件事情关系紧密，所以使用相同的工具就更有必要了。



如果你无法使用这种方法的话，使用支持代理模式的持续集成服务器（现代的持续集成服务器几乎都支持这种模式）会让第二种方式变得更简单。这种方法有以下几种好处。

* 你的工作更少。只要写一些本地运行的脚本，把它们提交到版本控制库中，让持续集成服务器在指定的远程机器上运行这些脚本就可以了。
* 持续集成服务器提供了管理任务（job）的整套基础设施，比如失败后重新执行，显示控制台输出，提供信息辐射显示板，让你能看到部署状态，以及每个环境中部署的版本号。
* 如果有安全性需求，可以让自己机器上的持续集成代理从持续集成服务器上得到部署所需的所有内容，而不必用脚本远程登录到测试或生产环境中。



​	最后，假如由于某种原因，你无法用上述任何一种工具的话，也完全可以从头到尾自己定制一个部署脚本。如果远程机器是UNIX，你可以使用原始的Scp或Rsync复制二进制包和数据，然后通过Ssh执行相关命令来进行部署。如果你使用Windows操作系统，也有两种选择：PsExec和PowerShell。当然，还有高层次的工具（如Fabric、Func和Capistrano等）让你绕过底层操作，直接将部署脚本化。

### 多层的部署和测试

​	对于软件交付或某个复杂系统的构建和部署，假如说有一个基础的核心原则的话，那就是应该总是把根基扎在已知状态良好的基础之上。我们不去测试那些没有编译成功的代码，也不会对没有通过提交测试的代码进行验收测试等。

​	当把候选版本发布到类生产环境中时更应该如此。在将应交付的二进制包复制到文件系统的某个正确位置之前，我们就要确保环境已经准备好了。为了做到这一点，我们喜欢把部署看做是一个层级沉积序列。

![](https://pic.imgdb.cn/item/60d01732844ef46bb2f05b3e.jpg)

### 测试环境配置

​	任何一个层级的部署出错，都可能导致应用程序无法正常运行。所以，当准备每一层级时，都要对其进行测试（参见图6-3）。如果发现问题，就要让环境配置流程快速失败，而测试结果也应该给出清晰指示，指出错误出现在哪里。

![](https://pic.imgdb.cn/item/60d017a0844ef46bb2f40b7a.jpg)

​	这些测试不必非常详尽，它们只需要捕获常见错误或昂贵的潜在错误，应该只是一些非常简单的“冒烟测试”，断言某些关键资源是否存在。我们的测试目标是为“刚部署的层级是可以工作的”提供一定的信心指数。

​	你写的基础设施冒烟测试针对每个具体系统应该是各不相同的，但测试目标是一致的，即证明环境的配置与我们的期望相符。关于基础设施监控在11.9节有详细阐述。为了给读者一些感觉，下面列出了我们认为比较有用的测试示例：

*  确认能从数据库中拿到一条记录；
*  确认能连上网站；
*  断言消息代理中的已注册的消息集合是正确的；
*  透过防火墙发送几次“ping”命令，证明线路是通的，且各服务器之间提供了一个循环负荷分配。

## 小贴士

### **总是使用相对路径**

​	构建中最常见的错误就是默认使用绝对路径。这会让构建流程和某台特定机器的配置形成强依赖，从而很难被用于配置和维护其他服务器。

​	当然偶尔也会使用绝对路径，这是很难完全避免的。但是，一定要尝试用一些更有创造力的方法尽量避免。如果不得不使用绝对路径的话，应该确保这一定是构建中的特例，而不是常规用法。确保把这些绝对路径放在属性文件或其他配置机制中，使其与构建系统相互独立。当然，有时绝对路径是必要的。

### 消除手工步骤

​	我们什么时候应该考虑将流程自动化呢？最简单的回答就是：“当你需要做第二次的时候。”到第三次时就应该采取行动，通过自动化过程来完成这件工作了。

### 从二进制包到版本控制库的内建可追溯性

​	能够确定“某个二进制包是由版本控制库中的哪个具体版本生成的”是非常必要的。假如在生产环境中出了问题，能够轻松确定机器上每个组件的版本号，以及它们的来源，你的生活会轻松很多。Bob Aiello写的Configuration ManagementBest Practices一书中有个很好的例子。

### 不要把二进制包作为构建的一部分放到版本控制库中

​	有时候，把二进制包或结果报告当做构建的一部分放到版本控制库中看起来是一个不错的主意。可是，一般来说，我们应该避免这种做法，原因如下

​	首先，版本控制标识的最重要作用之一就是能够追踪到某次提交中修改了什么。通常我们会将一个版本控制ID与一个构建标签相关联，用于在各种环境中（直至生产环境）追踪每次变更。假如把构建生成的二进制包和结果报告也提交到版本控制库中，那么与版本标识对应的这些二进制包就会有属于它们自己的版本标识了，有时这会令人感到迷惑。

​	取而代之的是，我们可以把二进制包和结果报告放在一个共享的文件系统中存储。如果你把它们弄丢了或者需要重新生成它们的话，最好是从源代码中重新构建一份。假如你无法根据源代码重新构建出一份一模一样的副本，这说明你的配置管理没达到标准，需要加以改进。

​	一般的经验法则是不要将构建、测试和部署过程中生成的任何产物提交到版本控制库中，而要将这些产物作为元数据，与触发该次构建的版本的标识关联在一起。大多数时新的持续集成和发布管理服务器都有制品库和元数据管理功能，如果没有的话，你也可以使用像Maven、Ivy或Nexus这样的工具。

### “test”不应该让构建失败

​	在某些构建系统中，一旦某个任务失败，便默认令本次构建立即失败。也就是说，假如你有一个“test”任务，如果在其运行时，任何测试失败了，整个构建就将立即失败。通常来说，这种做法是不好的。相反，应该将当前失败的任务记录下来，然后继续构建流程的后续部分。最后，在过程结束时，如果发现有任意一个任务失败了，就退出并返回一个失败码。

### 用集成冒烟测试来限制应用程序

​	交互设计师常常通过界面约束来避免那些未预期的用户输入。你可以使用同样的方式来限制应用程序，使得当程序本身发现自己处于非正常状态时，它就会停止运行。比如，可以在部署之前令部署脚本先检查一下是否被部署在了正确的机器上。对于测试和生产环境配置来说，这尤其重要。

###  .NET小贴士

​	略

##  小结

略

# 提交阶段

## 引言

​	当更改项目状态（向版本控制库的一次提交）时，提交阶段就开始了。当它结束时，你要么得到失败报告，要么得到后续测试和发布阶段可用的二进制产物和可部署程序集，以及关于当前应用程序状态的报告。理想情况下，提交阶段的运行应该少于五分钟，一定不会超过十分钟。

![](https://pic.imgdb.cn/item/60d1cb9d844ef46bb22ba8e6.jpg)

* 编译（如果需要的话），并在集成后的源代码上运行提交测试；
* 创建能部署在所有环境中的二进制包（如果使用需要编译的语言，则包括编译和组装）；
* 执行必要的分析，检查代码库的健康状况；
* 创建部署流水线的后续阶段需要使用的其他产物（比如数据库迁移或测试数据）。

## 提交阶段的原则和实践

​	如果说部署流水线的目标之一是消除无法在生产环境运行的构建的话，那么提交阶段就是“门卫”。提交阶段的目标是在那些有问题的构建引起麻烦之前，就把它们拒之门外。提交阶段的首要目标是要么创建可部署的产物，要么快速失败并将失败原因通知给团队。

### 提供快速有用的反馈

​	提交测试的失败通常是由以下三个原因引起的：（1）由于语法错误导致编译失败；（2）由于语义错误导致一个或多个测试失败；（3）由于应用程序的配置或环境方面（包括操作系统本身）的问题引起。

​	因此，为了得到高效的部署流水线，我们要尽早捕获错误。在大多数项目中，我们实际上在提交阶段之前就开始做这些事儿了。比如充分利用新式开发环境，只要开发环境中发现编译警告（如果适用）或语法错误，就尽快修复它们。很多时新的持续集成服务器也提供称为预测试提交或试飞构建的功能，即在提交之前就运行一下提交测试。如果没有这样的环境或设备，在提交之前必须在本地运行一下编译和提交测试。

​	提交阶段是第一个将质量视角从个体开发人员扩大到更多人的正式步骤。提交阶段的第一件事儿就是把提交者的修改与主线合并，然后对集成后的应用程序执行某种自动化的“验证”。既然“尽早识别错误”是我们的目标，那么就要做到“有问题就尽早失败”，所以提交阶段要捕获开发人员引入到应用程序中的大多数错误。

​	在采纳持续集成实践的早期，常见的错误是对“有问题就尽早使之失败”只按字面理解，即一旦发现错误，就让构建立即失败。这基本上是正确的，但优化过了头儿。我们一般会把提交阶段分成一系列的任务（具体包括哪些任务就因项目而异了），比如编译、运行单元测试等。只有在某个错误让提交阶段的其他任务无法执行时，我们才会让提交阶段停下来，比如编译错误，否则就直至提交阶段全部运行完后，才汇总所有的错误和失败报告，以便可以一次性地修复它们。

### 何时令提交阶段失败

​	传统上讲，当出现下列任一情况时，提交阶段就应该失败，即出现编译错误、测试失败，或者环境问题，否则就应该让提交阶段成功通过并报告一切OK。但是，假如测试通过是由于仅执行了一小部分测试呢？如果代码质量不高呢？如果编译成功，但有很多编译警告，我们也能满足吗？一个绿色（表示成功通过）的提交阶段很容易变成一个假象，即看上去应用程序的质量是不错的，但事实却不是这样的。

​	有人认为，在提交阶段结束时，应该提供更丰富的信息，比如关于代码覆盖率和其他度量项的一些图表。实际上，这些信息可以使用一系列阈值聚合成一个“交通灯信号”（红色、黄色、绿色），或者浮动的衡量标度。比如，当单元测试覆盖率低于60%就令提交阶段失败，但是如果它高于60%，低于80%的话，就令提交阶段成功通过，但显示成黄色。

​	但要记住的是，我们的纪律是如果提交阶段失败，交付团队就要立即停下手上的工作，把它修复。如果全团队尚未就某个原因[插图]达成一致意见，就不要让提交测试失败，否则大家会不拿失败当回事儿，而持续集成就渐渐会失去其应有的作用。我们强烈建议在提交阶段持续检查应用程序的质量，并在恰当的时候考虑加强代码质量的度量。

### 精心对待提交阶段

​	提交阶段中有构建用的脚本和运行单元测试、静态分析等的脚本。这些脚本需要小心维护，就像对待应用程序的其他部分一样。和其他所有软件系统一样如果构建脚本设计得很差，还没得到很好维护的话，那么保持它能够正常工作所需投入的精力会呈指数级增长。这相当于双重打击。一个较差的构建系统不但会把昂贵的开发资源从创造业务功能的工作中拖走，而且会令那些仍在创建业务功能的开发人员的工作效率降低。

​	随着项目的进行，要不断努力地改进提交阶段脚本的质量、设计和性能。一个高效、快速、可靠的提交阶段是提高团队生产效率的关键，所以只要花点儿时间和精力在这上面，让它处于良好的工作状态，就会很快收回这些投入成本。要想令提交阶段在较短时间内完成，并尽早捕获任何问题的话，就要有一些创造性，比如仔细地选择和设计测试用例。与应用程序的代码相比，若不太看重脚本，很快就会令脚本变得很难理解和维护。

### 让开发人员也拥有所有权	

​	在某些组织中会有一支专家团队，团队成员都精通创建有效且模块化的构建流水线，并且擅长管理这些脚本的运行环境。本书的两位作者都曾经担当过这样的角色。但是，如果真的只有那些专家才有权维护持续集成系统的话，那就是一种失败的管理方式。

​	交付团队对提交阶段（也包括流水线基础设施的其他部分）拥有所有权是至关重要的，这与交付团队的工作和生产效率是紧密联系在一起的。如果你设置了人为障碍，使开发人员不能快速有效地作出修改，就会减缓他们的工作进程，并在其前进的道路上埋下地雷。

​	如果必要的话，即使是很普通的变更（比如增加新的库文件和配置文件等）也都应该由一起工作的开发人员和运维人员来执行。这类活动不应该由构建专家完成，除非是在项目初期团队刚开始建立构建脚本时。

###  在超大项目团队中指定一个构建负责人	

​	但在大团队中，这并不总是一件容易的事。此时，让某个（或多个）人扮演构建负责人的角色是必要的。他们不但要监督和指导对构建的维护，而且还要鼓励和加强构建纪律。如果构建失败，构建负责人要知会当事人并礼貌地（如果时间太长的话，不礼貌也没问题）提醒他们为团队修复失败的构建，否则就将他们的修改回滚。

​	这个角色能起作用的另一种情况是，当团队刚开始接触持续集成时。在这样的团队中，构建纪律还没有建立起来，有个人能不断提醒大家，会令事情走向正轨。

​	构建负责人不应该是由固定的人担任。团队成员应该轮流担当，比如每星期轮换一次。这个纪律不错，能让每个人都学到一些经验。无论怎么说，想一直做这项工作的人还是不多的。

## 提交阶段的结果

**制品库**

​	提交阶段的输出（结果报告和二进制包）需要保存在某个地方，以便部署流水线的后续阶段能重用它们，并使团队也能使用它们。最容易想到的地方就是版本控制库，但它却不是一个正确的选择，因为这会让你的硬盘空间很快被吃掉，而且有些版本控制系统对二进制文件支持不佳。除此之外，还有几个理由。

* 制品库算是一个不同寻常的版本控制系统，它仅保存某些版本，而不是全部。如果候选发布版本在部署流水线的某个阶段失败了，就不再需要保留它了。如果有必要的话，我们完全可以将这类二进制包和报告从制品库中彻底删除。
* 还有一点也至关重要，那就是能够追溯已发布的软件究竟是由版本控制库中的哪个版本产生的。为了能够做到这一点，部署流水线的一个实例应该与版本控制库中触发它的那个版本相关联。作为部署流水线的一部分，我们已经把所有东西都提交到版本控制库了，而将更多修订版本与相应的流水线实践关联在一起会让这个流程更加复杂。
* 对于良好的配置管理策略，其标准之一就是二进制文件的创建过程应该是可重复的。也就是说，如果我不小心删除了二进制包，只要在同一个版本上再次触发提交阶段，就能再次得到一模一样的二进制包。在配置管理的范畴内，二进制包不那么重要，但我们会永久保存二进制包的散列，来验证重新生成的二进制包是否与生产环境上使用的一模一样。

**创建自己的制品库**

​	显示了一个制品库在典型安装中的使用方式。它是为每个候选发布版本保存二进制包、结果报告和元数据的关键资源。

![](https://pic.imgdb.cn/item/60d1d228844ef46bb250d804.jpg)

​	下面是一个候选发布版本在理想情况下在部署流水线中成功走向生产环境的每一步，其序号与图7-2中各阶段相对应。

1. 交付团队的某个人提交了一次修改。
2. 持续集成服务器运行提交阶段。
3. 成功结束后，二进制包和所有报告和元数据都被保存到制品库中。
4. 持续集成服务器从制品库中获取提交阶段生成的二进制包，并将其部署到一个类生产测试环境中。
5. 持续集成服务器使用提交阶段生成的二进制包执行验收测试。
6. 成功完成后，该候选发布版本被标记为“已成功通过验收测试”。
7. 测试人员拿到已通过验收测试的所有构建的列表，并通过单击一个按钮将其部署到手工测试环境中。
8. 测试人员拿到已通过验收测试的所有构建的列表，并通过单击一个按钮将其部署到手工测试环境中。
9. 测试人员执行手工测试。
10. 一旦手工测试也通过了，测试人员会更新这个候选发布版本的状态，指示它已经通过手工测试了。
11. 持续集成服务器从制品库中拿到通过验收测试（根据部署流水线的配置，也可能是手工测试）的最新候选发布版本，将其部署到生产测试环境。
12. 对这个候选发布版本进行容量测试。
13. 如果成功了，将这个候选版本的状态更新为“已通过容量测试”。
14. 如果部署流水线中还有后续阶段的话，一直重复这种模式。
15. 一旦这个候选发布版本通过了所有相关阶段，把它标记为“可以发布”，并且任何被授权的人都能将其发布，通常是由质量保证人员和运维人员共同批准。
16. 一旦发布以后，将其标记为“已发布”。

## 提交测试套件的原则与实践

​	对于提交测试套件的管理，有一些重要的原则和实践。提交测试中，绝大部分应由单元测试组成，这也是本节中我们主要讲的内容。单元测试最重要的特点就是运行速度非常快。有时候，我们会因为测试套件运行不够快而令构建失败。第二个重要的特点是它们应覆盖代码库的大部分（经验表明一般为80%左右），让你有较大的信心，能够确定一旦它通过后，应用程序就能正常工作。

​	Mike Cohn找到了一种很好的可视化方法指出自动化测试套件应该如何组织。在他的自动化测试金字塔（图7-3）中，单元测试占了自动化测试中相当大的比例。但由于它们执行得非常快，所以单元测试套件应该能在几分钟内就结束。即便验收测试比较少（可进一步分成服务和用户界面测试），它们也会花较长时间，因为这些测试需要启动应用程序。所有层次的测试对于确保应用程序可以工作并交付预期的业务价值都是至关重要的。这个测试自动化金字塔覆盖了4.2节中的那个测试象限图的左半边（支持开发过程的）。

![](https://pic.imgdb.cn/item/60d1d90c844ef46bb2795e17.jpg)

​	设计能快速运行的提交测试并不总是那么简单的事情。下面我们会介绍几种策略，其中大部分都是为了达到一个共同的目标：将指定测试的范围最小化，并让它尽可能聚焦于系统的某个方面。尤其要注意的一点是，运行的单元测试不应该与文件系统、数据库、库文件、框架或外部系统等打交道。所有对这些方面的调用都应该用测试替身代替，比如模拟对象（mock）和桩等。

### 避免用户界面

​	根据定义，用户界面是用户最容易找到缺陷的地方。这让大家自然而然地想到，要把测试焦点放在用户界面上，这有时还会吃掉其他测试的成本。

​	然而，对于提交测试来说，我们建议根本不要通过用户界面进行测试。用户界面测试的困难来自两方面。首先，它会涉及很多组件或软件的多个层次。这样是容易出问题的，因为要花很多时间和精力去准备各种各样的组件或数据，才能让测试运行起来。其次，用户界面是提供给用户手工操作的，而手工操作的速度与计算机操作的运行速度相比，是相当慢的。

### 使用依赖注入

​	依赖注入（或控制反转）是一种设计模式，用于描述如何从对象外部建立对象间的关系。显然，只有在使用面向对象语言时才能用上它。

​	这种技术不但是构建灵活的模块化软件的很好的方法，而且它还能让测试变得很容易，只需要测试必要的类，那些依赖包就不再是包袱了。

### 避免使用数据库

​	刚接触自动化测试的人常常写出一些需要与代码中的某一层进行交互的测试，并将结果写入数据库，然后再验证该结果的确被写到了数据库中。尽管这种方法简单，容易理解，但从其他方面来说，它不是一个很有效的方法。

​	首先，这种测试运行得非常慢。当想重复测试，或者连续运行几次相似的测试时，这种有状态的测试就是个障碍。其次，基础设施准备工作的复杂性令这种测试方法的建立和管理更加复杂。最后，如果从测试中很难消除数据库依赖的话，这也暗示着，你的代码在通过分层进行复杂性隔离方面做得不好。这也使得可测试性和CI在团队身上施加了一种微妙的压力，迫使其开发出更好的代码。

​	提交测试套件的这些单元测试根本不应该依赖于数据库。为了达到这一点，你就要把被测试的代码与其存储分离开来。这就要求对代码实现良好的分层，也需要使用像依赖注入这样的技术。实在做不到的话，也至少要使用内存数据库。

​	然而，在提交测试中，也应该有一两个非常简单的冒烟测试。这些测试应该是端到端的测试，并选自那些高价值的、常用功能的验收测试套件，用来证明应用程序可以真正运行起来。

### 在单元测试中避免异步

​	在单个测试用例中的异步行为会令系统很难测试。最简单的办法就是通过测试的切分来避免异步，这样就能做到：一个测试运行到异步点时，切分出来的另一个测试再开始执行。

​	比如，当系统需要发出一条消息，再根据这个消息作出反应，那么可以自己实现一个接口封装原生的消息发送机制。然后你可以利用一个简单的实现了消息接口的桩或者下一节讲的模拟技术，先在一个测试用例中验证这种调用与你所期望的相同。然后，再增加第二个测试，只要通过消息接口调用一下原来的那个调用点，验证一下消息处理程序（message handler）的行为就可以了。当然这也依赖于你的架构，有时候可能需要很多工作才能做到这一点。

### 使用测试替身

​	理想的单元测试集中在很小且紧密相关的代码组件上，典型的就是单个类或一小组极其相关的类。

​	打桩是指利用模拟代码来代替原系统中的某个部分，并提供已封装好的响应。桩并不对外界作出响应。这是个极其有用且灵活的方法，可以用在任何软件层次上，从模拟被测试代码依赖的一个非常简单的类，到模拟一个完整的系统。

**使用桩代替消息系统**

​	模拟技术恰好做到了这一点。现在有几种模拟技术工具集，比如Mockito、Rhino、EasyMock、JMock、NMock和Mocha等。使用模拟技术，你就可以说：“给我构建一个对象，让它假装就是某某类型的一个类。”

### 最少化测试中的状态

​	理想情况下，单元测试应聚焦于断言系统的行为。然而，特别对于那些刚接触有效测试设计的新手来说，常见的问题是测试中状态的不断增加。实际上问题包括两个方面。首先，很容易想到的是，测试就是为系统中的某个组件提供一些输入信息，然后得到一定的返回结果。所以在写测试时，你就会组织一下相关的数据结构，以便以正确的形式提交输入信息，然后再把结果与你期望的进行比较。事实上，所有的测试或多或少都是这种形式。问题是，如果处理不当的话，这个系统及其相关的测试会变得越来越复杂。

​	这样就很容易落入一个陷阱，即为了支撑测试，精心地建立起一堆难以理解和维护的数据结构。理想的测试应该能很容易和快速地进行测试准备，而清理工作也应该更快、更容易。对于结构良好的代码来说，其测试代码往往也非常整洁有序。如果测试看起来繁琐复杂，那可能是系统设计有问题。

​	然而，这是个很难定性的问题。我们的建议是设法让测试中的这种对状态的依赖最小化。你可能无法从根本上消除它，但为了运行测试，持续关注“如何降低要构造的测试环境的复杂性”是合理的。如果测试变得越来越复杂，很可能是由于代码结构问题引起的。

### 时间的伪装

​	时间问题是自动化测试需要面对的问题，原因有以下几个。你的系统可能需要在每天晚上八点触发一个处理过程，也可能在启动下一步前要等上500毫秒，也可能要在每个闰年的二月二十九号做一些特殊的处理。

​	如果你将这些时间和真正的系统时间绑定的话，这些情况处理起来可能会有点儿棘手，且对单元测试策略说，很有可能是灾难性的。

​	对于所有基于时间的系统行为，我们的做法是将对时间的请求抽象到一个你能够控制的类中。通常，我们使用依赖注入把用到的系统时间行为注入到包装类中（wrapper）。

​	通过这种方法，我们就可以为Clock这个类的行为进行打桩或模拟，或做一些我们认为合理的抽象。在我们的测试中，如果我们能设定当前是闰年，或要延时500毫秒的话，那么它就完全在我们的控制之下了。

### 蛮力

​	开发人员总是为最快的提交周期争论不休。然而，事实上，这要与在提交阶段识别最常见错误的能力平衡考虑。这是个只能通过不断试错才能找到的优化过程。有时候，运行速度稍慢一点儿的提交测试可能优于通过优化测试或减少发现的缺陷数来追求运行速度的提交测试。

​	通常我们会让提交测试在十分钟内完成。这基本上是我们能够承受的上限。这个时间比我们期望的理想时间（少于五分钟）还要长一些。大型项目中的开发人员可能无法接受“十分钟以内”这个限制，认为他们就目前的情况来说无法达到这一标准。其他开发团队可能会把这看做是一种太离谱的妥协，认为最高效的提交测试要比十分钟短得多。根据对很多项目上的观察，我们建议把这个数字作为指导。当这一限制被打破时，开发人员会开始做两件事，而这两件事对于开发流程来说都是极其糟糕的：（1）提交频率变低；（2）如果提交阶段的用时远远超过十分钟，他们可能就不再关注提交阶段通过与否了。

​	有两招儿能加快测试套件的运行。首先，将它分成多个套件，在多台机器上并行执行这些套件。时新的持续集成服务器都有“构建网格”功能，直接支持这种做法。记住，计算能力是廉价的，而人力是昂贵的。及时得到反馈比准备几台服务器的成本要有价值得多。第二招儿就是，作为构建优化过程的一部分，将那些运行时间比较长且不经常失败的测试放到验收测试阶段运行。然而，需要注意的是，这会导致需要更长的时间才能知道这些测试是否失败了。

## 小结

略

# 自动化验收测试

## 引言

​	验收测试在部署流水线中是一个关键阶段：它让交付团队超越了基本的持续集成。一旦正确实施自动化验收测试，你就是在测试应用程序的业务验收条件，即验证应用程序是否为用户提供了有价值的功能。

![](https://pic.imgdb.cn/item/60d3e570844ef46bb29706f9.jpg)

​	把验收测试作为证明应用程序是否满足了每个需求验收条件的方法来重点考虑，这种做法还有一个附带的好处。它能让与交付流程有关的每个人（包括客户、测试人员、开发人员、分析人员、运维人员和项目经理）都参与其中，共同考虑“每个需求需要达到什么要求才算成功”，详见8.3.3节

## 为什么验收测试是至关重要的

​	关于自动化验收测试，总是有很多争议。项目经理和客户常常认为创建和维护它们的成本太高。的确，如果实现不好，成本确实相当高。很多开发人员相信，通过测试驱动开发（TDD）方式创建的单元测试套件足以防止回归问题的发生。而我们的经验是，通过合理地创建和维护自动验收测试套件，其成本就会远低于频繁执行手工验收和回归测试的成本，或者低于发布低质量软件带来的成本。

​	在敏捷社区的一些人主张几乎完全取消自动化验收测试，代之以全面的单元和组件测试套件。

​	（1）创建验收测试；（2）创建应用程序驱动层；（3）实现验收测试；（4）维护验收测试套件。在详细讲述这些内容之前，先简单介绍一下我们的方法。

### 如何创建可维护的验收测试套件

​	要写出可维护的验收测试套件，首先需要细心地关注分析过程。验收测试来源于验收条件，因此写应用程序的验收条件时必须想着如何使其自动化，并要遵循INVEST原则[插图]，尤其是“对最终用户有价值”和“可测试”这两点。这是另一个既微妙却又很重要的压力，即对自动化验收测试的关注会影响整个开发流程，因为它需要有高质量的需求来支撑。验收条件写得很差，就无法解释某功能应该完成什么才能对用户有价值。而对那些写得很差的验收条件进行自动化是造成质量差且很难维护的验收测试套件的主要原因。

​	一旦你拿到了一些验收条件来描述对用户的价值，下一步就是将它们自动化。自动化验收测试应该是分层的。

![](https://pic.imgdb.cn/item/60d3ea25844ef46bb2ae3052.jpg)

​	在验收测试中，第一层就是验收条件。像Cucumber、JBehave、Concordion、Twist和FitNesse这样的工具让你能够把验收条件直接写在测试中，并把它们与底层实现关联在一起。然而，正如本章后面会讲到的那样，当使用XUnit Test这类测试框架时，可以将验收条件写在测试的名字中，然后通过XUnit测试框架直接运行验收测试。

​	使用领域语言来实现测试是至关重要的，不要把与应用程序如何交互的细节也包含在其中。直接引用应用程序的内部API或UI来实现验收测试是很脆弱的，即使在UI上做很小的改动也立刻会导致引用该元素的所有测试失败。这种事情是很常见的。

​	不幸的是，这种反模式随处可见。大多数的测试都写在详细实现这个层次上：“先点一下这里，然后在那里输入个字符，那么，这里就会出现这样的结果。”这种测试通常是那些“记录回放式”测试自动化工具的结果。而这也是自动验收测试被认为成本昂贵的主要原因之一。用这种工具创建的验收测试套件与UI是紧耦合的，所以很脆弱。

​	我们真正想知道答案的那个问题实际上是“如果我下了订单，它是否被接受了？”或者“如果我的信息卡额度超出了限制，系统会正确地通知我吗？”

​	长期维护验收测试，就需要有很强的原则性。必须注意保持测试实现的高效性及结构良好性，特别是在状态管理、超时处理以及测试替身（Test Double）的使用方式等方面。当新增验收条件时，要对验收测试套件进行重构，确保它们的相关性。

### GUI上的测试

​	在写验收测试时，一个非常重要的考虑是：测试是否直接基于应用程序的GUI运行。由于验收测试试图模拟用户与系统的交互，因此如果有图形界面的话，理想情况下理应通过系统提供的这个用户界面与系统打交道。如果没有通过用户接口进行测试，那么就没有测试用户与系统进行真实交互所执行的代码路径。然而，直接通过GUI进行测试会遇到几个问题：界面变化速度很快、场景的设置复杂、拿到测试结果很难，以及不可测的GUI技术。

​	其次，如果UI是系统的唯一入口的话，那么场景准备也可能很复杂。准备测试用例可能需要与系统交互多次才能达到用例本身所要求的状态。在一个测试结束后，可能很难通过UI拿到测试结果，因为UI很可能无法提供你需要验证的测试结果。

​	还有另一种方式通过GUI进行测试。假如应用程序设计得比较好，GUI层仅是清晰定义用于数据展现的代码，不包括任何业务逻辑。在这种情况下，绕过界面，基于界面下的代码进行测试的风险会相对小一些。将可测试性铭记在心，写出来的应用程序就会有一个API，使GUI和测试用具（test harness）都能用它来驱动应用程序。如果应用程序能够做到这一点的话，我们建议直接基于业务层执行测试，这是一个合理的策略。唯一的要求就是开发团队在这方面的纪律性，即让表现层只负责展现，不要涉足业务领域或应用逻辑。

​	如果应用程序没有设计成这个样子的话，就只能通过UI来测试了。

## 创建验收测试

### 分析人员和测试人员的角色

​	开发流程应该经过裁剪，来满足个体项目的需求。但是，一般来说，我们建议大多数项目（无论大小）都应该有一个业务分析师作为核心团队的一部分，与团队一同工作。业务分析师这个角色主要代表客户和系统的用户。他们与客户一起工作，识别需求，并排定优先级。他们与开发人员一起工作，确保开发人员能从用户的角度很好地理解应用程序。他们对开发人员进行指导，确保那些用户故事真正交付了它们应有的业务价值。他们与测试人员一起工作，确保验收条件已被合理阐明，并且开发出来的功能满足这些验收条件，交付了期望的价值。

​	并不是每个项目都需要不同的人担任不同的角色，来完成这些工作。有时候，开发人员会做一些分析人员的工作，或者分析人员会做一些测试人员的工作。理想情况下，与团队在一起的客户可以担任分析师的角色。关键是这些角色在团队中不能缺失。

### 迭代开发项目中的分析工作

​	总的来说，本书一直试图避免限定你所使用的开发流程。我们相信，我们描述的这些模式对所有交付团队都有益处，无论这些团队使用什么样的开发流程。然而，我们仍旧认为，对于创建高质量的软件，迭代开发过程是至关重要的。

​	在迭代交付方法中，分析人员会花大量时间定义验收条件。团队用这些验收条件来评判某个具体需求是否被满足。最开始，分析人员会与测试人员和客户紧密合作，定义验收条件。在这个阶段，鼓励分析人员和测试人员协作不仅对双方都有利，并且能使流程更加有效。分析师会有所收获，因为测试人员会根据他们的经验提供一些信息，比如哪些事情可能或应该用于定义用户故事是否做完了。而测试人员在测试这些需求之前，就能获得对这些需求本质的理解。

​	当开发人员认为工作已经完成时，通常是指所有相关的单元测试和组件测试都已经通过了，验收测试也全部实现，并证明系统满足需求。

### 将验收条件变成可执行的规格说明书

​	那些使用迭代过程的项目来说，由于自动化测试变得更加重要，所以，很多实践者都认识到，自动化测试不仅仅是测试而已。相反，验收测试就是正在开发的应用程序行为的一个可执行规格说明书。这作为自动化测试的一种新方法，被称为行为驱动开发。行为驱动开发的核心理念之一就是验收测试应该以客户期望的应用程序行为的方式来书写。这样，就可以拿这些写好的验收条件直接在应用程序之上运行，来验证它是否满足规格说明了。

​	这种方法有一些相当显著的好处。随着应用程序的演进，大多数需求规格说明书会过时。而对于可执行的规格说明来说，这是不可能的：如果它们没有准确指定应用程序是如何运行的，在运行时就会抛出异常。如果某个版本的应用程序没有满足它的这个规格说明，部署流水线的验收测试阶段就会失败，而这个版本就无法部署或发布。

​	验收测试是面向业务的，所以它们应该验证应用程序的确向用户交付了价值。分析人员为用户故事定义验收条件，只有这些验收条件被满足了，这个用户故事才算完成。Chris Matts和Dan North总结出一种写验收测试的领域特定语言，其格式如下：

​	假如（Given）初始条件，当（When）某个事件发生时，那么（Then）就会有……结果。

![](https://pic.imgdb.cn/item/60d3edb1844ef46bb2bfc8ee.jpg)

​	像Cucumber、JBehave、Concordion、Twist和FitNesse这样的工具都能让你用纯文本方式写出这样的验收条件，并且让它们与实际的应用程序保持同步。比如，在Cucumber中，可以将上面描述的验收条件保存在一个文件中，文件名为“features/placing_an_order.feature”。这个文件代表图8-2中所述的验收条件。可以创建一个Ruby文件，在其中列出该场景所有的步骤，文件名为“features/step_definitions/placing_an_order_steps.rb”。这个文件代表了图8-2中的测试实现层。

​	这种创建可执行规格说明的方法是行为驱动设计的本质。让我们再回顾一下这个过程：

*  和客户一起讨论用户故事的验收条件；
*  以可执行的格式将得到的验收条件写下来；
*  为这些使用领域专属语言所描述的测试写出它的代码实现，与应用程序驱动层进行交互。
*  创建应用程序驱动层，使测试通过它来与系统交互。



​	相比于传统方式（比如使用Word文档或者跟踪工具来管理验收条件，或者使用录制回放方式创建验收测试），这种方式有很多优点。可执行的规格说明组成了对测试的记录系统，因为它们真的是可执行的规范。测试人员和分析人员不再需要写Word文档，然后把文档扔给开发人员，因为在整个开发过程中，分析人员、客户、测试人员和开发人员可在这些可执行规范上协作。

​	对于在那些有特殊规定限制的项目上工作的读者来说，值得注意的是，这些可执行的规格说明一般可以使用一个简单的自动化流程将它转化为一个文档，用于审计。我们曾工作过的好几个团队都使用这种方法，而且很成功，审计人员对结果非常满意。

## 应用程序驱动层

**什么是领域专属语言**

​	什么是领域专属语言?

​	DSL（Domain-Specific Language，领域专属语言）是一种计算机编程语言，用于解决某个具体问题域的某个问题。它与通用编程语言不同，因为它无法像通用编程语言那样可以解决很多类型的问题，它专门为解决某个专属问题域的问题而设计。

​	DSL可以分为两种类型：内部的和外部的。外部的领域专属语言在其指令被执行之前需要明确的解析。前面使用的Cucumber例子中，最顶层的那个验收测试脚本就是一种外部DSL。另外一些例子还包括Ant和Maven的XML构建脚本。外部的DSL不必是图灵完备的（Turing-complete）。

​	如果有一个设计良好的应用程序驱动层，就能够完全放弃验收条件层，在测试的实现中表达验收条件。对于前面我们用Cucumber写的一些验收测试，只用JUnit测试也可以表达。下面这个例子就是Dave目前的项目上的真实测试。

![](https://pic.imgdb.cn/item/60d3ee78844ef46bb2c3dbbb.jpg)

### 如何表述验收条件

​	将这两种方式（使用JUnit和Cucumber写验收测试）对比一下，对我们是很有启发性的。首先，这两种方法都能够很好地工作，而且各有其优缺点。另外，它们都要比传统的验收测试做得好。本书作者Jez在当前的项目中使用Cucumber形式的方法（尽管使用Twist的时间比Cucumber更多一些），而另一作者Dave则在其项目中直接使用JUnit（比如上面的例子）。

​	外部DSL方法的好处在于，可以在验收条件之间任意切换。无需用跟踪工具管理验收条件之后再用xUnit写一遍测试，这种方式下，验收条件和用户故事就是可执行的规范。然而，虽然这些现代工具能够减少撰写可执行的验收条件及使其与验收测试实现保持同步所需的开销，但还是有一定的开销的。

### 窗口驱动器模式：让测试与GUI解耦

​	验收测试分为三层：可执行的验收条件、测试实现和应用程序驱动器层。只有应用程序驱动器层知道如何与应用程序打交道，而其他两层只用业务的领域语言。如果应用程序有GUI，而且已经决定验收测试需要基于GUI来做的话，应用程序驱动器层就要了解如何与其进行交互。应用程序驱动器层中与GUI交互的这部分就叫做窗口驱动器。

​	口驱动器模式是通过提供一个抽象层，减少验收测试和被测试系统GUI之间的耦合，从而让基于GUI的测试运行时更加健壮。它有助于隔离系统GUI的修改对测试的影响。实际上是写了一个抽象层，作为测试的用户接口。所有测试都要通过这个抽象层与真正的UI进行交互。所以，如果对GUI做了一些修改，我们可以对窗口驱动器做相应的修改，这样就不用改测试了。

​	FitNesse（一个开源的测试工具）就是使用类似的方法，通过创建Fit夹具（fixture）作为你将要测试的部件的“驱动器”。在这方面，它是一个非常杰出的工具。

​	在实现这种窗口驱动器模式时，要为GUI上每个设备驱动器（device driver）写一个等价物。验收测试代码只能通过某个适当的窗口驱动器与GUI进行交互。作为应用程序驱动层的一部分，窗口驱动器提供了一个抽象层，用于将测试代码与UI的具体修改进行隔离。当UI变化时，只要修改窗口驱动器的代码，所有依赖于它的测试就都可以运行了。窗口驱动器模式如图8-3所示。

![](https://pic.imgdb.cn/item/60d3f07f844ef46bb2ceb00d.jpg)

**使用窗口驱动模式创建可维护的测试**

​	我们实现了窗口驱动模式，改变了创建和维护测试的流程，最显著一个变化就是让开发人员负责对测试的维护。到发布结束时，我们就有了一个可以工作的部署流水线，其中每次成功的提交之后都会立即运行自动化验收测试。

![](https://pic.imgdb.cn/item/60d3f0d0844ef46bb2d06265.jpg)

​	下面是重构成两层（测试实现层和窗口驱动层）以后的例子，其中AccountPanelDriver就是一个窗口驱动器。这是一个对测试进行解耦的很好的出发点。

![](https://pic.imgdb.cn/item/60d3f0f8844ef46bb2d12cf3.jpg)

​	我们可以更清晰地看到测试语义和在其之下与UI交互细节的分界线。如果只看该测试中的代码量，再加上窗口驱动器的代码量，其代码量当然比不分层的测试要多，但是，这种方式使抽象的层次更高一些。我们可以在需要与该页面交互的很多不同的测试中重用这个窗口驱动器，并不断对其增强。

## 实现验收测试

### 验收测试中的状态

​	这里的 “有状态”是指为了测试应用程序的某个行为，应用程序必须处于某种特定的起始状态（就是行为驱动开发中，“假如”那段所描述的内容）。

​	验收测试最有效的方法是：利用应用程序自身的功能特性来隔离测试的范围。

### 过程边界、封装和测试

​	最直截了当的测试是那些不需要权限就能验证需求的测试，所以，它们也是所有验收测试的榜样。刚接触自动化测试的人会发现，想让代码可测试，必须修改对它的设计，事实的确如此。但是他们常常希望在代码上开很多秘密的后门，用于验证结果。

​	然而，当一点儿灵感都没有的时候，你就不得不使用某种后门了。比如设计某些方法调用，让你能够修改系统某部分行为，可能还会返回一些关键结果，或者将系统的某个部分完全调整到某种特定的测试模式下。如果你没有其他选择了，这种方法也行。但是，我们的建议是：只对那些系统外部的组件这么做，用受控的桩或者其他测试替身来代替与外部组件交互的那部分代码。另外，还建议不要添加那些只为测试而写的与远程系统部件交互的接口，这些远程系统部件将会被部署到生产环境中。

### 管理异步与超时问题

​	我们发现，最有效的策略是构建一个夹具用于将测试本身与这个问题隔离开来。诀窍是，对于测试本身而言，让事件顺序发生，使测试看起来像是同步的。这可以通过把同步调用背后的异步性隔离开来实现。

​	这个新策略基于两个想法。一个是去轮询结果数据，另一个是将监控中间事件作为测试的一个门槛。我们用了“重试”（retry），而不是在超时之前等上足够长的时间。

​	就ConfirmEmailWasReceived的所有调用者而言，该确认步骤看上去好像就与上面写的各版本的代码都是同步的了。这样，写高层次的测试就容易多了，尤其是在这个检查之后还有后续活动的时候。这类代码应该是在应用驱动器层上，这样很多测试用例就都可以重用它了。这种相对复杂一点的实现是值得花上一点儿功夫的，因为它高效，而且完全可靠，这也让所有依赖于它的测试都变得可靠。

### 使用测试替身对象

​	这个策略有两个分支：我们通常创建测试替身对象，用于代表系统与所有外部系统交互的连接器，如图8-4所示。另外，还会围绕集成点创建一些测试，目的是在一个真正与外部系统连接的环境中运行。

​	在测试中，用替身对象取代外部系统还有一个好处，那就是能够控制行为、模拟通信失败、模拟错误响应事件或高负载下的响应等，所有这些都能在我们的掌握之中。

​	应用一些好的设计原则，可以让外部系统与你开发的系统的耦合最小。通常会设计一个系统组件专门与某个外部系统进行交互，也就是说，每个外部系统对应一个内部组件（一个网关或适配器）。该组件把这些交互及与其相关的问题集中到一点，并将这些交互的技术细节与系统的其他部分隔离开来。还可以使用各种模式改善应用程序的稳定性，比如Release It! : Design and Deploy Production-ReadySoftware一书中所描述的circuit breaker模式。

**测试外部集成点**

​	当为这类集成点的行为编写测试时，测试点应该是那些可能出现问题的点，而到底哪些点容易出问题则更多地是由该集成自身的特性以及外部系统在整个生命周期中所处的位置决定的。假如外部系统比较成熟并且已经上线，那么出现的问题可能与正在开发当中的系统遇到的问题有所不同。这些因素是我们决定在哪里、什么时候做哪种程度的测试的依据。

​	如果外部系统也在开发当中，那么两个系统间的接口很可能会修改。模式（schema）和契约（contract）等都可能改变。而更微妙的是，与外部系统交换信息内容的方式也会发生变化。

​	测试应该总是被限定在两个系统（系统与外部系统）之间的特定交互上，不应该对外部系统接口进行全面测试。

​	而缓解策略就是实现它自己的测试套件，这样就不用每次验收测试运行时就运行它，但最好还是一天或一周运行一次。你也可以将这些测试放在部署流水线的另一个阶段中，比如放在容量测试阶段。

## 验收测试阶段

​	令验收测试失败的构建版本不能被部署。在部署流水线模式中，只有已经通过这一阶段的候选发布版本才能走向后续阶段。而后续阶段常常被认为是需要人为评判的：在大多数项目中，如果某个候选发布版本无法通过容量测试，就会有人来决定这次失败是否足以严重到要取消这个候选版本的发布资格，还是让它继续走下去。可是，对于验收测试，不应该提供这种人为评定的机会。如果成功，就可以继续，如果失败，就不能向前。

​	这也符合我们的总原则：将流程中的痛点尽量提前。据我们所知，如果没有这种良好的自动化验收测试覆盖率，会有三种结果：（1）当你认为开发快要结束，马上就能完成的时候，可能会在找bug和修改bug上花相当长的时间；（2）花很多时间和金钱做手工验收和回归测试；（3）发布低质量的软件。

### 确保验收测试一直处于通过状态

​	当某个验收测试失败时，团队要停下来立即评估问题。它是一个脆弱的测试，还是由于环境配置问题，或者是由于应用程序的某个修改使原有的假设不成立了，还是一个真正的失败？然后，让某人立即采取行动，使测试通过。

**谁是验收测试的责任人**

​	在传统模式下，我们把验收测试的责任划分给测试团队。事实证明，这种策略很有问题，特别是在大项目中。测试团队总是在开发链的最末端，所以验收测试经常处在失败状态。

​	我们改变了自动化验收测试的拥有权。与其让测试团队负责验收测试的开发和维护，不如让整个交付团队（包括开发人员和测试人员）来负责。带来的好处是：它令开发人员关注并努力达到某个功能的验收条件。这能让他们立即意识到他们的修改对代码库的影响，因为他们也要对验收测试负责，也需要跟踪验收测试的运行。也就是说，开发人员也需要考虑验收测试了，而且更加了解他们的修改会影响哪些验收测试，这样会更好地完成他们的工作。

**识别可能的“罪魁祸首”**

​	然而，在两次验收测试之间，可能会有多次提交，所以验收测试失败的几率更大。仔细设计构建流水线，以便能追踪与每个验收测试相关联的修改，这是非常值得做的一件事。某些现代持续集成系统很容易就能让你在整个构建部署生命周期中追踪每个部署流水线的构建版本，解决这个问题会相对容易一些。

**验收测试与构建负责人**

​	在首个实现了复杂构建流水线的项目中，我们写了一些简单的脚本，并作为多阶段CruiseControl构建流程的一部分来运行。这些脚本会核对自上次成功运行验收测试之后的所有提交版本，识别所有的提交标签，因此也就知道到底是哪些开发人员提交了代码，可以向那些已经提交但还没有通过验收测试的代码的开发人员发送邮件。

###  部署测试

​	如前所述，好的验收测试关注于验明某个具体用户故事或需求的某个具体验收条件是否被满足了。最好的验收测试是具有原子性的，即它们创建自己的起始条件，并在结束时将环境清理干净。这些理想测试会将其对状态的依赖最少化，并且通过公共入口而不是预留后门来测试应用程序。然而，仍有某些类型的测试不满足这种要求。但无论如何，在验收测试阶段运行它们都是非常有价值的。

​	我们设计的测试环境会尽可能与期望的生产环境一致。如果成本不太高的话，它们就应该是一样的。

​	我们的目标是快速失败。如果验收测试有问题的话，我们希望验收测试构建尽快失败。由于这个原因，我们常常将部署测试套件作为一种特殊的套件。如果它失败了，我们会让整个验收测试阶段失败，并且不会等待需要长时间运行的验收测试套件执行完。

**土豚检录**

​	在一个项目中，我们曾使用JUnit写验收测试。我们所掌握的唯一方便控制运行测试套件的方式就是利用套件的名字，因为它们是按字母顺序排列的。我们组织了一组环境测试，并把它命名为“土豚”（Aardvarks），以确保它在所有其他测试之前执行。

​	请记住，在运行其他测试之前，一定做土豚检录测试。

## 验收测试的性能

​	我们认为，持续地关注维护验收测试套件，以保持它的良好结构和连贯性是非常重要的，但是自动化验收测试的全面性要比测试在10分钟内运行完成更重要。

### 重构通用任务

​	最显而易见且快速奏效的方法就是每次构建结束后都找到最慢的几个测试，再花上一点儿时间找些办法让它们更加高效。这种策略与我们管理单元测试的方法相同。

​	这之后就要寻找通用模式，尤其是在测试准备阶段。一般来说，根据验收测试的特点，它要比单元测试有更多的状态。由于我们建议你使用端到端的方法来做验收测试，尽可能减少共享状态，这也暗示着，每个验收测试应该准备自己的起始条件。然而，你常常会发现，在多个测试用例中，准备过程中的某些步骤是完全一样的，因此，值得多花些时间让这些步骤变得更高效一些。在比较理想的情况下，假如在准备工作中有一个公共API可以利用，就不要通过界面来操作。有时，事先为应用程序准备一些“种子数据”（seed data），或者用一些应用测试的后门为它准备测试数据都是有效的办法。

### 共享昂贵资源

​	根据被测系统的特质，有时候可对其他的耗时资源进行优化，使验收测试套件在整体上能更快地执行。

### 并行测试

​	当验收测试间的独立性比较好时，还有一种办法可加速测试的运行，那就是“并行执行测试”。对于那些基于服务器的多用户系统来说，这是显而易见的。如果你能将测试分开，并且保证它们之间没有互相影响的话，那么，在同一个系统实例上并行执行测试会大大减少验收测试阶段运行的总时长。

### 使用计算网格

​	实际上，有更多约束的分配策略通常更有意义。这个领域中一些供应商并没有忽略这种优势。大多数现代持续集成服务器都提供了管理测试服务器网格的功能。如果你使用Selenium的话，还有另外一个选择，那就是使用开源的Selenium Grid，它可以让使用Selenium Remoting写的验收测试不必修改就能并行运行于计算网格中。

**使用云计算进行验收测试**

​	然而，此时我们决定稍微改变一下战术，利用Amazon EC2云计算，以便获得更大的扩展性。图8-5显示了我们所用的测试虚拟机的逻辑组织结构。一部分虚拟机放在我们公司内部，而模拟多个客户端与被测试的系统交互的那些机器以分布式的方式运行于EC2云中。

![](https://pic.imgdb.cn/item/60d3fd37844ef46bb2153e55.jpg)

## 小结

* 为“软件是否满足业务目标”提供了更高的信心；
* 为系统进行大范围修改提供了一个保护网；
* 通过全面的自动回归测试极大地提高了质量；
* 无论什么时候出现缺陷，都能提供快速、可靠的反馈，以便可以立即修复；
* 让测试人员有更多的时间和精力去思考测试策略、开发可执行的规格说明，以及执行探索性测试和易用性测试；
* 缩短周期时间，使持续部署成为可能。

# 非功能需求的测试

## 引言

​	本章将讨论非功能需求的测试方法，这主要是关于容量（capacity）、吞吐量（throughput）和性能（performance）的测试。

​	“性能”是对处理单一事务所花时间的一种度量，既可以单独衡量，也可以在一定的负载下衡量。“吞吐量”是系统在一定时间内处理事务的数量，通常它受限于系统中的某个瓶颈。在一定的工作负载下，当每个单独请求的响应时间维持在可接受的范围内时，该系统所能承担的最大吞吐量被称为它的容量。客户通常对吞吐量和容量较感兴趣。在现实生活中，“性能”常被用来指这些术语的合集，本章会小心地使用它们。

​	非功能需求[比如有效性（availability）、容量、安全性和可维护性等]与功能测试同样重要，同样有价值，它也是系统功能中至关重要的组成部分。由于这个术语——非功能测试——让人产生错觉，所以有人建议使用“跨功能需求”（cross-functionalrequirement）或“系统特征”（system characteristics）来描述这些内容。根据我们的经验，通常用于处理这类需求的办法和效果都不是很好。

## 非功能需求的管理

**非功能需求的分析**

​	（1）创建一些具体任务来管理非功能需求；（2）如果有必要，向其他功能需求中加入非功能需求的验收条件。

​	在性能需求中，还有另一种常见的错误要求，即用一种非常懒散的方式来描述系统的可用性。当很多人说“在两秒种内作出响应”时，他们是想说：“我不想坐在计算机前等上很长时间还没有得到任何反馈”。如果这是他们真正的想法，其实这就并不一定是一个性能问题。

## 如何为容量编程

​	假如没有很好地分析非功能需求，它们就往往会限制我们的思维，从而导致过分设计和不恰当的优化。很容易花过量的时间写一些“高性能”的代码。在预测应用程序中哪里有性能瓶颈这一方面，开发人员的表现相当差。他们往往会在代码中引入不必要的复杂性，并且花很多成本来维护，以达到无法确定的性能。这值得让我们引用一下高德纳（Donald Knuth）最著名的格言。

> 在97%的时间里，我们都应该忘记那种小的效率提升：过早优化是所有罪恶之根。然而，我们也不能让另外非常关键3%的机会与我们擦肩而过。一个优秀程序员不会因为这个原则而对其置之不理，他们非常聪明，只会在识别出那段关键代码后，才会非常细心地去查看。

**过早优化**

​	这种对于容量近乎偏执的关注常常导致过于复杂（也因此变得很差）的代码。设计高容量的系统的确很难，但是开发过程中在不适当的时候去担心容量问题，则会让它变得更难。

​	那么就应该避免在开发期间进行更复杂的“优化”，除非是修复那些被清晰识别并可度量的问题，这就是经验的用武之地。为了能够获得项目成功，必须避免两个极端：一是假设自己能在项目后期解决所有容量问题；二是因害怕未来可能出现的容量问题而写一些具有防范性的、过分复杂的代码。

为了解决容量问题，可采取的策略如下。

* 为应用程序决定一种架构。通常要特别注意进程、网络边界和I/O。
* 了解并使用正确的模式，避免使用那些影响系统容量和稳定性的反模式。Michael Nygard撰写的优秀著作Release It！一书中详细描述了这些模式。
* 除了采用适当模式以外，还要确保团队在已经明确的应用架构下进行开发，不要为容量做无谓的优化。鼓励写清晰且简单的代码，而不是深奥难以理解的代码。在没有明确测试结果表明有容量问题时，坚决不能在代码可读性上作出让步。
* 注意在数据结构和算法方面的选择，确保它们的属性与应用程序相吻合。比如，只需要O（1）的性能，就不要用一个O（n）的算法。
* 处理线程时要特别小心。Dave现在的项目就是一个高性能系统（这个交易系统每秒可以处理数以万计的交易）。要能达到这一点，关键之一就是保持应用程序的核心是单线程的。正如Nygard所说，“线程阻塞反模式是大多数失败最直接的（proximate）……会导致连锁反应和级联失败。”
* 创建一些自动化测试来断言所期望的容量级别。当这些测试失败时，用它们作为向导来修复这些问题。
* 使用调测工具主要关注测试中发现的问题，并修复它，不要使用“让它越快越好”这类策略。
* 只要有可能，就使用真实的容量数据来做度量。生产环境是唯一真实度量的来源。使用这样的数据，并分析这些数据到底说明了什么。特别要注意系统的用户数，他们的行为模式以及生产环境中的数据量。

## 容量度量

* 扩展性测试。随着服务器数、服务或线程的增加，单个请求的响应时间和并发用户数的支持会如何变化。
* 持久性测试。这是要长时间运行应用程序，通过一段时间的操作，看是否有性能上的变化。这类测试能捕获内存泄漏或稳定性问题。
* 吞吐量测试。系统每秒能处理多少事务、消息或页面点击。
* 负载测试。当系统负载增加到类似生产环境大小或超过它时，系统的容量如何？这也许是最典型的容量测试。



​	目标明确的基准式（benchmark-style）容量测试对于代码中某个具体问题的防范或局部代码优化是非常有用的。

**如何定义容量测试的成功与失败**

​	当使用容量环境做测试和度量时，对每个运行在其上的测试都要定义它的成功条件。设定容量测试成功的条件是比较棘手的问题。一方面，如果把条件定得太高，那么只有当环境中的所有设施都有利于应用程序时，该测试才能成功，很可能要经常面临间歇性失败。比如，当网络被其他任务占用或者其他任务同时在该容量测试环境上执行时，测试就可能失败。

​	这里可以使用两种策略。首先，把目标设定为得到稳定、可重现的结果。只要有可能的话，为容量测试专门准备一个环境，用于度量容量。这会将那些与测试不相关任务对结果的影响降到最低，从而使结果保持一致性。容量测试是少有的几个虚拟技术不太适用的地方之一，除非生产环境也是虚拟环境，因为虚拟环境在性能方面有额外的开销。然后，一旦某个测试通过了最低验收标准，就把验收标准提高一点儿，调整该测试的成功门槛。这能避免“假阳性”（false-positive）场景。如果提交后测试失败了，而验收门槛刚好高于需求中所定义的要求，那么只要降低容量是能被接受的，直接降低一点儿门槛就行了。当然，该测试仍旧是有价值的，因为它对那些不小心威胁到容量需求的修改起到了保护作用。

**设置初始的容量标准**

​	为了使测试更好用，而不只是性能度量，每个测试都必须体现一个具体的场景，并且只有达到某个标准门槛时，才能认为该测试通过了。

## 容量测试环境

​	理想情况下，系统容量的绝对度量应该在一个尽可能与生产环境相似的环境上执行。

​	假如对某应用程序来说，容量或性能是一个非常关键的问题，那么就一定要有所投入，为该系统的核心部分准备一个生产环境的副本。使用相同的软硬件规格要求，遵循我们关于如何管理配置信息的建议，以确保每个环境中都使用相同的配置文件，包括网络配置、中间件及操作系统的配置。在大多数情况下，如果你在构建一个高容量系统，除了生产环境是连接真正的外部系统并有真正的负载和生产数据进行测试之外，其他任何策略都是一种带有风险的妥协，因为应用程序很可能无法满足容量要求。

​	在另一种极端情况下，复制生产环境也是不可能的，比如那些较大的软件即服务（SaaS）提供商。它们的生产环境中常常有数十万台服务器在运行，复制生产环境的话，维护开销就已经很大了，更不用说硬件成本了。即使它们真的复制了生产环境，为这样的系统作负载压力和设计有代表性的数据所具有的复杂性也是一个巨大的工程。在这种情况下，可以把容量测试作为金丝雀发布策略（canary releasestrategy，详见10.4.4节）的一部分来执行。更频繁的发布可以减小影响应用程序容量的修改所带来的风险。

​	另外，也不要依据硬件的某种特定参数对应用程序的扩展性作出线性推论，这是在蒙蔽你自己。

​	假如真的别无选择，那么，如果可能的话，你还可以尝试缩放范围进行测试，从而找到测试环境和生产环境之间的差异基准。

**缩放因素的缺点**

​	在我们的一个项目中，客户不想花两套标准生产环境硬件的钱，所以，提供了一些低配置的机器来运行容量测试。幸运的是，我们说服客户，只要他们能将对生产环境中的服务器进行升级的时间向后推迟一周的话，我们就能更好地缓解可能遇到的容量风险。在那个星期里，我们疯狂地在这些设备上运行容量测试，并收集了很多数据。然后又在低配置环境中运行了同样的测试，并建立了在这两个环境下的一系列对比基准，以便作为今后做容量测试的参考。

​	对于那些需要部署到服务器集群中的应用程序来说，一个既可以降低环境成本又能提供适当准确度量的策略就是，仅复制一小部分的服务器（如图9-2所示），而不是整个集群。

![](https://pic.imgdb.cn/item/60d403c1844ef46bb2377e4d.jpg)

![](https://pic.imgdb.cn/item/60d403cd844ef46bb237b884.jpg)

​	事实上，对于不同的项目，容量的推演方式也各不相同，包括如何做推演，以及如何判定它是成功的。所以我们只能建议你，要带一定的怀疑眼光来对待推演出来的容量结果。

## 自动化容量测试

​	对于一个项目来说，当容量非常重要时，那么就请暂且忽视这些成本吧，因为更重要的是，要记住：代码的修改对系统容量的影响与其对功能的影响一样重要。当做了修改之后，要尽早掌握容量会下降多少，这样就能快速且有效地修复它。这就要在部署流水线中加入一个阶段，即容量测试阶段。

容量测试应该达到以下几点目标。

* 测试具体的现实场景，这样就不会因为测试太抽象而错过真实应用场景中那些重要的bug。
* 预先设定成功的门槛，这样就能判定容量测试是否通过了。
* 尽可能让测试运行时间短一些，从而保证容量测试在适当时间内完成。
* 在变更面前要更健壮一些，从而避免因对应用程序的频繁修改而不断返工。
* 组合成大规模的复杂场景，这样就可以模拟现实世界中的用户使用模式。
* 是可重复的，并且既能串行执行，也能并行执行，以便这些测试既可以做负载测试，也可以做持久性测试。



​	一个比较好的方法是能用已有的验收测试，做一定的调整，使它们变成容量测试。

​	我们的目标有两个：一是创建比较现实的类生产环境的负载，二是选择并实现那些具有实际代表性且现实生产中非正常负载状态的场景。

​	根据系统行为的多数变量及其基础架构，记录重放的切入点（如图9-3所示）可归结为以下三个。

![](https://pic.imgdb.cn/item/60d439a5844ef46bb2a8ddb3.jpg)

* 通过UI（用户界面）。
* 通过某个服务或者公共API。比如，直接向Web服务器做HTTP请求。
* 通过底层API。比如，直接调用某个服务层，或者数据库。

### 通过UI的容量测试

​	最明显的切入点就是通过UI对系统的交互操作进行记录和回放。

​	这种方法还有一个很大的缺点：在分布式架构中，服务器负责主要的业务逻辑（容量问题可能更集中），很可能无法加载足够的负载进行适当的测试。当客户端有很多业务逻辑，比较复杂，或者只有很薄的UI做集中服务时，情况也是这样的。在这些情况下，比较实际的做法就是衡量客户端与服务器之间的比例。

​	对于某些系统来说，基于UI进行测试是正确的事。然而，这种方法实际上只对那些处理交互量适中的系统有效。即便如此，对于管理和维护以UI为中心的测试来说，其成本可能也会非常高。

### 基于服务或公共API来录制交互操作

​	可以避开那些扩大客户端数量、管理成百上千客户端进程以及通过用户界面进行交互的脆弱性等引发的问题。面向服务架构特别适合用这种方法。

![](https://pic.imgdb.cn/item/60d43a49844ef46bb2ad80e3.jpg)

### 使用录制的交互模板

​	对于交互操作的录制来说，我们的第一个目标是获得验收测试与系统进行交互操作时所用的一些模板。这些交互模板可以用来为后续的容量测试生成容量测试数据。

![](https://pic.imgdb.cn/item/60d43ae1844ef46bb2b19a67.jpg)

​	一旦录制好这些交互操作的模板后，就要为它们创建测试数据。这些数据用于补全这些交互操作模板。每个测试数据集与适当的模板相结合后，就形成了与被测试系统交互的有效实例。

![](https://pic.imgdb.cn/item/60d43b0a844ef46bb2b2be4e.jpg)

​	交互模板和测试数据也可以作为开源性能测试工具的输入，比如Apache的JMeter、Marathon或者Bench。还可以用这种方式写一个简单的测试用具来管理和运行测试。

​	有一点需要注意。对于特别高容量和高性能的系统来说，对性能要求最高的部分是测试代码，而不是生产代码。测试一定要运行得足够快，以便能够加载负荷并验证结果。

### 使用容量测试桩开发测试

​	对高性能系统来说，写容量测试的复杂度往往超过为了通过这些测试而写出足够快的产品代码的复杂度。所以，要解决一个至关重要的问题，那就是确保每个测试的自身运行速度非常快，足以用来判定被测试的产品代码的性能是否达到要求。无论你在什么时候写容量测试，一定要先实现一个被测试应用程序、接口或技术的桩，而且这个桩一定要非常简单且无操作，这样你才能展示出该测试满足所需的运行速度，并且当另一端无操作时可以正确地断言，该测试可以通过。

## 将容量测试加入到部署流水线中

​	一般来说，应用程序都要满足某个最低容量标准。而大多数现代商业应用程序都会有多个并发用户，所以在交付可接受的性能要求的同时，还会要求可扩展性，以便满足其峰值需求。在开发过程中，我们要能够断言，应用程序足以达到用户在容量方面的需求。

​	我们要一直遵守这样的格言，即做最少的工作达到我们的目标，这也是YAGNI（“You Ain't GonnaNeed It”）原则所暗示的。YAGNI提醒我们，增加防御性行为都有可能成为浪费。如果遵循高德纳的格言，应该直到明确需要优化而且到了最后时刻才做优化。另外，还要基于应用程序运行时分析结果，直接解决最重要的瓶颈问题。

​	假如你很幸运，容量测试在几秒之内就能证明应用程序满足了性能目标，就请将它放在提交测试阶段，这样你就能得到即时反馈了。然而，在这种情况下，要当心那些依赖运行时优化编译器的技术。在．NET和Java中，这种运行时优化要花几个迭代才能稳定下来，而且只有花上几分钟 “热身”后，才能收集到合理的结果。

​	为了防止已知的性能关键点随代码的开发而逐渐变差，可以使用另一种类似策略，即当识别出这种关键点后，创建一个运行得非常快的“防卫测试”（guardtest），并把它放在提交测试阶段。这种测试扮演着性能冒烟测试的角色，它的目的并不是为了验证应用程序满足所有的性能要求，而是起到错误趋势上的警示作用，以便在性能出现问题之前就可以处理。然而，需要当心的是，使用这种策略时，一定不要引入那些常有间断性失败、无法信赖的测试。

​	但是，大多数容量测试不适合放在部署流水线的提交测试阶段，因为它们通常需要的时间太长，资源占用太多。如果容量测试相当简单，并且花的时间不长，可以将其增加到验收测试阶段，尽管我们并不建议这么做。原因如下。

* 为了得到真正有效的结果，容量测试需要在它自己的环境上运行。如果其他自动化测试与容量测试同时运行在同一个环境上，那么要找到某版本不符合性能要求的原因，所需成本就太高了。有些持续集成系统让你能够为测试指定环境。你可以使用这种功能对容量测试进行分组，让它们与验收测试一起并行执行。
* 某些类型的容量测试可能要运行很长时间，这样可能会耽误验收测试结果的反馈时间。
* 在验收测试之后的很多质量保障活动可以和容量测试并行执行，比如演示最新版本的可工作软件、手工测试、集成测试，等等。对于很多项目来说，没有必要等到容量测试成功之后才做这些事情，那样的话，效率很低。
* 对于一些项目来说，也没有必要像验收测试那样，频繁运行容量测试。



​	通常，除了前面提到的性能冒烟测试以外，我们建议把自动化容量测试作为部署流水线中的一个完全独立的阶段。

![](https://pic.imgdb.cn/item/60d4439e844ef46bb2e81f18.jpg)



## 容量测试系统的附加价值

​	容量测试系统通常是与你所期望的生产系统最接近的。因此，它也是一个非常有价值的资源。并且，如果你遵循我们的建议，把容量测试设计成为一系列组合式的、基于场景的测试，那么实际上这已经是生产系统的一个精密模拟系统了。

我们曾用这种方法执行了各种各样的任务，如下所述。

*  重现生产环境中发现的复杂缺陷。
*  探测并调试内存泄漏。
*  持久性（longevity）测试。
*  评估垃圾回收（garbage collection）的影响。
*  垃圾回收的调优。
*  应用程序参数的调优。
*  第三方应用程序配置的调优，比如操作系统、应用程序服务器和数据库配置。
*  模拟非正常的、最糟糕情况的场景。
*  评估一些复杂问题的不同解决方案。
*  模拟集成失败的情况。
*  度量应用程序在不同硬件配置下的可扩展性。
*  与外部系统进行交互的负载测试，即使容量测试的初衷是与桩替身接口（stubbed interface）打交道。
*  复杂部署的回滚演练。
*  有选择地使系统的部分或全部瘫痪，从而评估服务的优雅降级（gracefuldegradation）。
*  在短期可用的生产硬件上执行真实世界的容量基准，以便能计算出长期且低配的容量测试环境中更准确的扩展因素。

# 应用程序的部署与发布

## 引言

​	将软件发布到生产环境和部署到测试环境是有差异的——绝对不仅仅是执行发布者的血液中肾上腺素水平高而已[插图]。从理论上讲，这些差异应该被封装在一组配置文件中。当在生产环境部署时，应遵循与其他任何环境部署同样的过程。启动自动部署系统，将要部署的软件版本和目标环境的名称告诉它，并点击“开始”就行了。所有后续部署和发布都要使用同样的流程。

​	本章将讲述如何为软件的发布（包括将其部署到测试环境上）创建并遵循一个策略。部署与发布之间的主要区别在于回滚的能力。

​	我们应该能有一个列表，其中包含能够部署到每个环境的所有构建，并且只要通过点击按钮或鼠标就可以选择一个软件版本向某个环境进行自动部署。这样，就能看到每个环境中究竟运行的是哪个版本的应用程序，谁授权部署了这个版本，从上次部署之后应用程序到底有哪些修改。

## 创建发布策略

​	当在项目一开始创建发布策略的第一个版本时，应该考虑下列内容。

* 每个环境的部署和发布都是由谁负责的。
* 创建一个资产和配置管理策略。
* 部署时所用技术的描述。运维团队和开发团队应该对其达成共识。
* 实现部署流水线的计划。
* 枚举所有的环境，包括用于验收测试、容量测试、集成测试、用户验收测试的环境，以及每个构建在这些环境中的移动过程。
* 描述在测试和生产环境中部署时应该遵循的流程，比如提交一个变更申请，以及申请授权等。
* 对应用程序的监控需求，包括用于通知运维团队关于应用程序相关状态的API或服务。
* 讨论部署时和运行时的配置方法如何管理，以及它们与自动化部署流程是如何关联在一起的。
* 描述应用程序如何与所有外部系统集成。比如，在哪个阶段进行集成？作为发布过程里的一份子，如何对这种外部集成进行测试？一旦出现问题，运维人员如何与供应商进行沟通？
* 如何记录日志详情，以便运维人员能够确定应用程序的状态，识别出错原因。
* 制定灾难恢复计划，以便在灾难发生之后，可以恢复应用程序的状态。
* 对软件的服务级别达成一致，比如，应用程序是否有像故障转移以及其他高可用性策略等方面的需求。
* 生产环境的数量大小及容量计划：应用程序会创建多少数据？需要多少个日志文件或数据库？需要多少带宽或磁盘空间？客户对响应延迟的容忍度是什么？
* 制订一个归档策略，以便不必为了审计或技术支持而保留生产数据。
* 如何对生产环境进行首次部署。
* 如何修复生产环境中出现的缺陷，并为其打补丁。
* 如何升级生产环境中的应用程序以及迁移数据。
* 如何做应用程序的生产服务和技术支持。

### 发布计划

​	第一次发布风险最高，需要细致地做个计划。而这种计划活动的结果可能是产出一些文档、自动化脚本或其他形式的流程步骤（procedure），用来保证应用程序在生产环境上的部署过程具有可靠性和可重复性。除了在发布策略中的这些材料以外，还要包括以下内容。

* 第一次部署应用程序时所需的步骤。
* 作为部署过程的一部分，如何对应用程序以及它所使用的服务进行冒烟测试。
* 如果部署出现问题，需要哪些步骤来撤销部署。
* 对应用程序的状态进行备份和恢复的步骤是什么。
* 在不破坏应用程序状态的前提下，升级应用程序所需要的步骤是什么。
* 如果发布失败，重新启动或重新部署应用程序的步骤是什么。
* 日志文件放在哪里，以及它包括什么样的信息描述。
* 如何对应用程序进行监控。
* 作为发布的一部分，对必要的数据进行迁移的步骤有哪些。
* 前一次部署中存在问题的记录以及它们的解决方案是什么。

### 发布产品

​	另外，对于商业产品软件来说，还有如下一些事情需要考虑。

* 收费模式。
* 使用许可策略。
* 所用第三方技术的版权问题。
* 打包。
* 市场活动所需要的材料（印刷材料、网站、播客、博客、新闻发布会等）。
* 产品文档。
* 安装包。
* 销售和售后支持团队的准备。

## 应用程序的部署和晋级

### 首次部署

​	对于任何一个应用程序，首次部署发生在第一个迭代结束时，即当向客户演示第一个开发完的用户故事或需求的时候。把这个演示活动作为一个借口或理由，以便在类生产环境（UAT）部署应用程序。我们认为，项目首个迭代的主要目标之一就是在迭代结束时，让部署流水线的前几个阶段可以运行，且能够部署并展示一些成果，即使可展示的东西非常少。尽管我们不建议让技术价值的优先级高于业务价值的优先级，但此时是个例外。你可以把这一策略看做实现部署流水线的“抽水泵”。

​	当这个启动迭代结束时，你应该已经有了以下内容。

* 部署流水线的提交阶段。
* 一个用于部署的类生产环境。
* 通过一个自动化过程获取在提交阶段中生成的二进制包，并将其部署到这个类生产环境中。
* 一个简单的冒烟测试，用于验证本次部署是正确的，并且应用程序正在运行。



​	如果生产环境是一个集群环境，那么应该搭建一个有限的小集群作为试运行环境。如果生产环境是一个分布式且多节点的环境，那么就要确保类生产环境至少用一个独立的进程来代表每类进程边界。

​	虚拟化和chicken-counting（0, 1, many）是你的好朋友。利用虚拟化技术在一个物理机器上创建一个环境来模拟生产环境的某些重要特征，还是非常容易的。chicken-counting意味着，假如生产环境里有250个Web服务器的话，用两个服务器就足以代表进程边界了。随着开发工作的进行，可在以后适当的时间再根据需要不断完善它。

一般来说，类生产环境具有如下特点。

* 它的操作系统应该与生产环境一致。
* 其中安装的软件应该与生产环境一致，尤其不能在其上安装开发工具（比如编译器或IDE）。
* 使用第11章所描述的技术，用与管理生产环境相同的方式对这种环境进行管理。
* 对于客户自行安装的软件，UAT环境应该基于客户硬件环境的统计结果，具有一定的代表性，至少要基于别人做过的真实统计[插图]。

### 对发布过程进行建模并让构建晋级

* 为了达到发布质量，一个构建版本要通过哪些测试阶段（例如，集成测试、QA验收测试、用户验收测试、试运行以及生产环境）。
* 每个阶段需要设置什么样的晋级门槛或需要什么样的签字许可。
* 对于每个晋级门槛来说，谁有权批准让某个构建通过该阶段。



​	分析完这些以后，你可能会得到一张图，类似于图10-1。当然，流程可能比这复杂，也可能比这简单。实际上，创建这样一张图，第一步就是为发布流程创建一个价值流图。在第5章中，曾讨论过价值流映射是对发布流程进行优化的一种方法。

![](https://pic.imgdb.cn/item/60d58576844ef46bb224eb83.jpg)

​	一旦创建了这个价值流图，就可以在所用的管理部署工具上为发布流程中的每个部分创建占位符。Go和AntHill Pro都具有这样的功能。另外，大多数持续集成工具可能需要通过一定的定制工作，也可以对这种发布部署过程进行建模和管理。做完这些以后，负责对某个阶段进行审核的人就可以使用这个工具对某个构建版本进行审批了。

​	部署流水线的管理工具还必须提供另一个关键功能，即在每个阶段都能够看到流水线里的哪些构建已经成功通过前面的所有阶段，并已准备好进入下一阶段了。然后，还应该可以选择这些构建版本中的某个版本，并通过单击一下按钮来部署它。这个过程就是“晋级（promoting）”。通过点击按钮的方式让构建版本晋级使部署流水线变成了一个“拉动”系统，让所有参与到交付过程的人都能够安排他们自己的工作。分析人员和测试人员可以通过自服务方式部署某个构建版本，做探索性测试、演示或可用性测试。运维人员可以自己选择某个构建版本，点一下按钮就将其部署到试运行环境或生产环境上。

**为了产品开发，请做持续演示**

​	应用程序很快就超过了那个原型系统，所以，我们就开始从多套手工测试环境中拿出来一套，专门用来做演示。另外，经常是得到通知后不久就要演示。我们的部署流水线做得很好，所以我们可以很有信心地说，任何一个成功通过验收测试的构建版本都可以用于演示。而且我们可以很轻松且快速地部署任何一个候选版本。

​	无论是哪种情况，对于流程中的每个测试阶段来说，其工作过程都是相似的。

* 做测试的人（或团队）通过某种方式在一个列表中选出他们要部署到测试环境中的应用程序版本，该列表中包括所有已通过部署流水线前面各阶段的构建版本。选择某个特定版本之后就会自动执行后续的步骤，直至真正的测试活动。
* 准备环境和相关的基础设施（包括中间件），以便能在一个干净的状态下进行应用程序的部署。这应该是以完全自动化的方式进行的，如第11章所描述的那样。
* 部署应用程序的二进制包。这些二进制包应该是从制品库中拿到的，而不是每次部署时重新构建出来的。
* 对应用程序进行配置。在应用程序中，配置信息应该以某种统一的方式来管理，并在部署和运行时使用，比如使用像Escape这样的工具[apvrEr]。更多的信息请参见第2章。
* 准备或迁移该应用程序所管理的数据，如第12章所述。
* 对部署进行冒烟测试。
* 执行测试（可能是手工的，也可能是自动化的）。
* 如果应用程序的这个构建版本通过了这些测试，允许其晋级到下一个环境中。
* 如果应用程序的这个构建版本没能通过这些测试，记录一下是什么原因。

### 配置的晋级

​	需要晋级的并不仅仅是二进制包。与其同时得到晋级的还包括环境及应用程序的配置信息。然而，你并不想让所有的配置信息都晋级，这让事情变得复杂了一些。

​	对于这个问题，一种解决办法是用冒烟测试来验证配置信息的指向是正确的。比如，可以用一个字符串返回值来代表测试替身对象的服务所在的环境。然后，让冒烟测试检查应用程序从外部服务得到的返回值与其想要部署环境的预期返回字符串是否一致。

​	在面向服务架构和组件化应用程序中，所有的服务或者构成应用程序的组件都需要一同晋级。

### 联合环境

​	几个应用程序常常会共享同一个环境。此时，有两种途径引入复杂性。首先，当为某个应用程序的新版本准备部署环境时，需要花额外的精力，来保证不会破坏同一环境中正在运行的其他应用程序。

### 部署到试运行环境

​	在用户使用应用程序之前，应该在试运行环境（与生产环境非常相似）上执行一些最终测试。如果能想办法得到一个容量测试环境（它几乎是生产环境的复制品），有时也可以跳过试运行这一步骤：可以用这个容量测试环境同时做容量测试和试运行。

​	在项目一开始，就应该准备好试运行环境。如果你已经为生产环境准备好了硬件，而这些硬件尚没有其他用途的话，那么在第一次发布之前就可以把它作为试运行环境。以下是项目开始时就需要计划的一些事。

* 确保生产环境、容量测试环境和试运行环境已准备好。尤其是在一个全新的项目上，在发布前的一段时间就准备好生产环境，并把它作为部署流水线的一部分向其进行部署。
* 准备好一个自动化过程，对环境进行配置，包括网络配置、外部服务和基础设施。
* 确保部署流程是经过充分冒烟测试的。
* 度量应用程序的“预热”时长。如果应用程序使用了缓存，这一点就尤其重要了。将这也纳入到部署计划中。
* 与外部系统进行测试集成。你肯定不想在第一次发布时才让应用程序与真实的外部系统集成。
* 如果可能的话，在发布之前就把应用程序放在生产环境上部署好。如果“发布”能像重新配置一下路由器那样简单，让它直接指向生产环境，那就更好了。这种被称作蓝绿部署（blue-green deployment）的技术会在本章后面详细描述。
* 如果可能的话，在把应用程序发布给所有人之前，先试着把它发布给一小撮用户群。这种技术叫做金丝雀发布，也会在本章后续部分描述。
* 将每次已通过验收测试的变更版本部署在试运行环境中（尽管不必部署到生产环境）。

## 部署回滚和零停机发布

​	万一部署失败，回滚部署是至关重要的。在运行的生产环境中通过调试直接查找问题的这种做法几乎总会导致晚上加班、具有严重后果的错误和用户的不满。当出现问题时，你应该有某种方法恢复服务，以便自己能在正常的工作时间内调试所发现的错误。接下来将讨论执行回滚的几种方法。更先进的技术（蓝绿部署和金丝雀发布）也可以用于零停机发布和回滚。

​	在开始讨论之前，先要声明两个重要的约束。首先是数据。如果发布流程会修改数据，回滚操作就比较困难。另一个是需要与其他系统集成。如果发布中涉及两个以上的系统（也称联合环境的发布，orchestrated releases），回滚流程也会变得比较复杂。

​	当制定发布回滚计划时，需要遵循两个通用原则。首先，在发布之前，确保生产系统的状态（包括数据库和保存在文件系统中的状态）已备份。其次，在每次发布之前都练习一下回滚计划，包括从备份中恢复或把数据库备份迁移回来，确保这个回滚计划可以正常工作。

### 通过重新部署原有的正常版本来进行回滚

​	这通常是最简单的回滚方法。如果你有自动化部署应用程序的流程，让应用程序恢复到良好状态的最简单方法就是从头开始把前一个没有问题的版本重新部署一遍。这包括重新配置运行环境，让它能够完全和从前一样。这也是能够从头开始重建环境如此重要的原因之一。

​	为什么创建环境和部署要从头开始呢？有以下几个理由。

* 如果还没有自动回滚流程，但是已有自动部署流程了，那么重新部署前一版本是一种可预知时长的操作，而且风险较低（因为重新部署相对更不容易出错）。
* 在此之前，已经对这个操作做过数百次测试（希望如此）。另外，执行回滚的频率相对比较低，所以包含bug的可能性要大一些。



​	我们没有想到在哪些什么情况下，这种方式会不适用。然而，它也有如下一些缺点。

* 尽管重新部署旧版本所需的时间固定，但并不是不需要时间。所以一定会有一段停机时间。
* 更难做调试，找到问题原因。重新部署旧版本通常是覆盖那个新版本，所以也失去了找到问题原因的最佳机会。如果生产环境使用的是虚拟化技术，那么还有办法来弥补这个缺点，后续部分会讲到。对于那些相对简单的应用程序来说，把新版本安装到一个新目录中，改一下符号链接（Unix系统中的目录链接方式），让它指向这个新目录，就可以把旧版本保留下来，非常容易。
* 如果你在部署新版本前已经备份了数据库，那么在重新安装旧版本时把数据库备份文件恢复回来的话，那些在新版本运行时产生的数据就丢失了。如果问题发现及时且回滚速度足够快的话，这也没什么大不了的，但有些时候这却可能是个严重问题。

### 零停机发布

​	零停机发布（也称为热部署），是一种将用户从一个版本几乎瞬间转移到另一个版本上的方法。更重要的是，如果出了什么问题，它还要能在瞬间把用户从这个版本转回到原先的版本上。

​	零停机发布的关键在于将发布流程中的不同部分解耦，尽量使它们能独立发生。尤其是，在升级应用程序之前，就应该能将应用程序所依赖的共享资源（比如数据库、服务和一些静态资源）的新版本放在适当的位置。

​	对于静态资源和基于Web的服务来说，这相对容易一些。你只要在URI中包含这些资源或服务的版本就可以了，而且它们的很多版本可以同时并存。比如，Amazon的Web服务有一个基于日期的版本标识系统，其中EC2的API最新版本（在撰写本书时）在这里http://ec2.amazonaws.com/doc/2009-11-30/AmazonEC2.wsdl。当然，他们还会保持旧版本的API按原有的URI工作。对于资源来说，当发布网站的一个新版本时，你可以将这些静态资源（比如图片、JavaScript、HTML和CSS）放在一个新目录中，比如，可以将应用程序版本2.6.5的图片放在目录/static/2.6.5/images之下。

### 蓝绿部署

​	对于发布管理来说，蓝绿部署是我们所知道的最强大的技术之一。做法是有两个相同的生产环境版本，一个叫做“蓝环境”，一个叫做“绿环境”。

​	系统的用户被引导到当前正在作为生产环境的绿环境中。现在我们要发布一个新版本，所以先把这个新版本发布到蓝环境中，然后让应用程序先热身一下（你想多长时间都行），这根本不会影响绿环境。我们可以在蓝环境上运行冒烟测试，来检查它是否可以正常工作。当一切准备就绪以后，向新版本迁移就非常简单了，只要修改一下路由配置，将用户从绿环境导向蓝环境即可。这样，蓝环境就成了生产环境。这种切换通常在一秒钟之内就能搞定。

![](https://pic.imgdb.cn/item/60d58eee844ef46bb2550d3d.jpg)

​	如果出了问题，把路由器切回到绿环境上即可。然后在蓝环境中调试，找到问题的原因。

​	这种方式比重新部署要有一些改进。然而，在做这种蓝绿部署时，要小心管理数据库。通常来说，直接从绿数据库切换到蓝数据库是不可能的，因为如果数据库结构有变化的话，数据迁移要花一定的时间。

​	解决这个问题的一种方法是在切换之前暂时将应用程序变成只读状态一小段时间。然后把绿数据库复制一份，并恢复到蓝数据库中，执行迁移操作，再把用户切换到蓝系统。如果一切正常，再把应用程序切换到读写方式。如果出了什么问题，只要把它再切回绿数据库就可以了。如果这发生在切成读写方式之前，那么什么额外工作也不需要做。如果应用程序中已经写入了一些你想保留的数据，那么，当再次切换回去之前，你就要找到一种方法可以拿到新记录并把它们迁回到绿数据库中。另外，你还可以找个办法让应用程序的新版本把数据库事务同时发向新旧两个数据库。

​	如果只有一个生产环境，也可以使用蓝绿部署。只要让应用程序的两份副本一起运行在同一个环境中，每个副本都有自己的资源（自己的端口、在文件系统中有自己的根目录，等等）。这样它们就可以同时运行且互不干扰了。你也可以分别对每个环境进行部署。还有一种方法就是使用虚拟化技术，但是要先测试一下这种虚拟化对应用程序在容量方面的影响有多大。

​	如果有足够预算的话，蓝绿环境应该是相互完全分离的环境副本。这需要的配置较少，但需要的成本较高。该方法的一种变形[也叫做影子域发布（shadow domainreleasing）、影子环境发布（shadow environment releasing）或者双热发布（live-live releasing）]是使用试运行环境和生产环境作为蓝绿环境。将应用程序的新版本部署到试运行环境上，然后把用户从生产环境引导至试运行环境中，让用户开始使用这个新版本。此时，试运行环境就变成了生产环境，生产环境就变成了试运行环境。

​	我们曾为一个大型组织工作，该组织有五个并行的生产环境。他们利用这种技术保持生产系统的多个版本并行运行，这种方式使他们能够以不同的速度来迁移业务中的不同领域。这种方法也具有金丝雀发布的某些特征。

### 金丝雀发布

​	在任意时刻，生产环境中只有应用程序的一个版本正在运行”这个假设都是正确的。这会让缺陷补丁以及基础设施的管理更容易一些。然而，这同时也是对软件测试的一种阻碍。即便有稳固且全面的测试策略，还是会在生产环境上发现缺陷。而且即便周期时间（cycle time）很长，开发团队仍可以从新特性或其他工作的快速反馈中得到收益，作出适当调整，让软件更有价值。

​	如果生产环境极其庞大的话，创建出一个有意义的容量测试环境也是不可能的——除非应用程序的架构是那种端到端共享（end-to-end sharing）方式。

​	金丝雀发布就是用来应对这些问题的。如图10-3所示，金丝雀发布就是把应用程序的某个新版本部署到生产环境中的部分服务器中，从而快速得到反馈。就像发现一只煤矿坑道里的金丝雀那样，很快就会发现新版本中存在的问题，而不会影响大多数用户。这是一个能大大减少新版本发布风险的方法。

![](https://pic.imgdb.cn/item/60d5904a844ef46bb25ae56e.jpg)

​	金丝雀发布有以下几个好处。

* 非常容易回滚。只要不把用户引到这个有问题的版本上就行了。此时就可以来分析日志，查找问题。
* 还可以将同一批用户引至新版本和旧版本上，从而作A/B测试。某些公司会度量新特性的使用率，如果用的人不多，就会废弃它。另外一些公司会度量该版本产生的收入，如果收入较低，就把该版本回滚[插图]。如果软件产生了研究结果，那么可以对新旧版本之间从真正用户那儿得到的结果质量进行对比。你不必使用大量用户对新版本做A/B测试，只要有代表性的样本就足够了。
* 可以通过逐渐增加负载，慢慢地把更多的用户引到新版本，记录并衡量应用程序的响应时间、CPU使用率、I/O、内存使用率以及日志中是否有异常报告这种方式，来检查一下应用程序是否满足容量需求。如果生产环境太大，无法创建一个与实际情况相差不多的容量测试环境，那么这对于容量测试来说，是一个风险相对比较低的办法。



​	可是，金丝雀发布也并不适用于所有情况。对于那些需要用户安装到其自己环境中的软件来说，这么做就比较困难了。对于这个问题，有另一个解决方案（使用网格计算），那就是让客户软件或桌面应用程序自动从设置的服务器上拿到新版本并自动升级。

​	金丝雀发布在对数据库升级以及其他共享资源方面引入了更进一步的约束，即任何共享资源（如共享的会话缓存或外部服务等）要能在生产环境中的所有版本中相兼容。另一种方法是使用非共享架构（shared-nothing architecture），即每个结点与其他结点绝对独立，不共享数据库或外部服务[插图]，也可以将两种方法结合使用。

## 紧急修复

​	让每个紧急修复都走完标准的部署流水线。这是另一个应该保持更短周期的原因。

下面是处理生产环境中的缺陷时应该考虑的一些因素。

* 别自己加班到深夜来做这事儿，应该与别人一起结对做这事儿。
* 确保有一个已经测试过的紧急修复流程。
* 对于应用程序的变更，避免绕过标准的流程，除非在极端情况下。
* 确保在试运行环境上对紧急修复版本做过测试。
* 有时候回滚比部署新的修复版本更划算。做一些分析工作，找到最好的解决方案。想一想，假如数据丢失了，或者面对集成或联合环境时，会发生什么事？

## 持续部署

​	指导思想非常简单：使用部署流水线，并让最后一步（部署到生产环境）也自动化。这样，如果某次提交的代码通过了所有的自动化测试，就直接部署到生产环境中。如果想让这种做法不引发问题，自动化测试（应该包括自动化的单元测试、组件测试、功能性和非功能性验收测试）就必须异乎寻常的强大，覆盖整个应用程序。必须先写所有的测试（包括验收测试），然后再写代码。这样你才能做到，只有用户故事完成的最后那次代码提交才能使验收测试通过。

**持续发布用户自行安装的软件**

* 管理升级的历程。
* 迁移二进制包、数据和配置信息。
* 测试升级流程。
* 从用户那里收集问题报告。



​	对于客户自行安装的软件来说，一个重要问题是：随着时间的推移，如何管理已经发布的众多版本。它很可能引发技术支持的恶梦：为了调试某个问题，你要将版本回滚到相应的版本上，努力回忆当时开发与这个问题相关的某个特性的情形。理想情况下，希望大家都用同一个版本，即最新的稳定版本。为了达到这一点，就要尽可能做到无痛苦的版本升级。



​	客户端处理升级有如下几种方式。

* 让软件自己检查是否有新版本，并提示用户下载并升级到最新版本。这是最容易实现的，但用起来也是最痛苦的。没人想看着一个下载进度条一点一点地向前走。
* 在后台下载，并提醒用户安装。在这种模式中，软件需要周期性地检查更新，在运行的同时悄悄地下载。当下载成功后，不断地提醒用户升级到最新版本。
* 在后台下载并在应用程序下次启动时悄悄升级。应用程序可能也会提示你立即重新启动（Firefox就是这么做的）。



​	正确的解决方案是升级过程已通过“防弹测试”（bullet proof）了，而且静默升级。特别是，当升级过程失败时，应用程序应该能够自动回滚到原来的版本并把失败报告给开发团队。开发团队就能修复这个问题，然后再次发布一个新版本（并希望）正确升级。所有这些都应该悄悄发生，无须用户知道。需要提示用户的唯一理由就是需要用户采取一些纠正措施。



## 小贴士和窍门

### 真正执行部署操作的人应该参与部署过程的创建

​	当开发人员和运维人员成为朋友时，事情会变得更美好。

### 记录部署活动

​	如果部署过程没有完全自动化（包括环境的准备工作），记录哪些文件在自动化部署过程中复制和创建，这是非常重要的。这样做之后，很容易对发生的问题进行跟踪调试，因为很清楚在哪里能找到配置信息、日志和二进制包。

​	同样重要的是，在每个环境的部署过程中，记录每个改动过的硬件清单和实际部署的日志。

### 不要删除旧文件，而是移动到别的位置

​	当做部署操作时，确保已保留了旧版本的一份副本。然后，在部署新版本之前清除旧版本的所有文件。如果旧版本的某个文件被遗忘在了最新部署版本的环境当中，出现问题后就很难追查了。更糟糕的是，如果旧版本的管理接口页面还留在那儿，那么很可能引起错误数据。

​	在UNIX环境中，一个最佳实践是：把应用程序的每个版本部署在一个单独目录中，用一个符号链接指向当前版本。版本的部署和回滚就只是改一下符号链接这么简单。

### 部署是整个团队的责任

​	“构建和部署专家”的存在是一种反模式。团队中的每个成员都应该知道如何部署，如何维护部署脚本。通过每次部署软件（即使是在开发机器上）都使用真正的部署脚本，就可以达到这一点。

​	如果部署脚本有问题，构建就应该失败。

### 服务器应用程序不应该有GUI

​	略。

### 为新部署留预热期

​	不要在预热时激活eBay-killer网站。当这样的网站在官方发布时，它应该已经运行了一段时间，足以让应用服务器和数据库建立好它们的缓存，准备好所有的连接，并完成了“预热”。

​	对于网站来说，可以通过金丝雀发布达到这个目标。新服务器和新的发布在开始时可以服务于一小部分请求。然后，当环境无异常并被证明行之有效后，你就可以将更多的负载切换到这个新系统上。

​	许多应用程序在部署时都会急于建立内部缓存。在缓存完成之前，应用程序的响应时间往往较长，甚至可能会失败。如果应用程序行为的确如此的话，请确保在部署计划中考虑到了这件事，包括重建缓存所需的时间（当然是在一个类生产环境中测试）。

### 快速失败

​	部署脚本也应该被纳入测试之中，以确保部署成功。这些测试也应该作为部署的一部分来运行。然而它们不应该是全面的单元测试，而是简单的冒烟测试，确保被部署的内容可以工作。

​	理想情况下，系统在启动初始化时也应该执行这些检查，一旦遇到了问题，就应该让系统无法启动。

### 不要直接对生产环境进行修改

​	大多数生产环境的停机是由于那些未受控的修改。生产环境应该是被完全锁定的，这样只有部署流水线可以对其进行改变，包括从环境配置信息到部署在其中的应用程序和相关数据。很多组织有严格的访问管理流程。我们曾看到过，某个组织管理生产环境访问方式是使用由审批流程和两阶段验证系统生成的有限有效期的密码，在使用这个验证系统时需要输入一个由RSA fob产生的代码。在某个组织中，对生产系统的变更可能只能在一个带有闭路电视监控摄像机的房间的某个终端进行操作。

​	这类授权过程也应该放在部署流水线中。这样做会得到相当大的好处，它意味着有一个系统来记录对生产环境的每一次变更。没有比确切记录谁、什么时候对生产环境做了哪些修改更好的审计跟踪方式了。而部署流水线正好提供了这种便利。

# 基础设施和环境管理

## 引言

部署软件有如下3个步骤。

* 创建并管理应用程序运行所需的基础设施（硬件、网络、中间件和外部服务）。
* 在其上安装应用程序的正确版本。
* 配置应用程序，包括它所需要的任何数据和状态。



​	环境是指应用程序运行所需的所有资源和它们的配置信息。用如下这些属性来描述环境。

* 组成运行环境的服务器的硬件配置信息——比如CPU的类型与数量、内存大小、硬盘和网络接口卡等，以及这些服务器互联所需的网络基础设施。
* 应用程序运行所需要的操作系统和中间件（如消息系统、应用服务器和Web服务器，以及数据库服务器等）的配置信息。



​	准备部署环境的过程以及部署之后对环境的管理是本章的主要内容。然而为了能够做到这一点，就要基于下面这些原则，用一个整体方法来管理所有基础设施。

* 使用保存于版本控制库中的配置信息来指定基础设施所处的状态。
* 基础设施应该具有自治特性，即它应该自动地将自己设定为所需状态。
* 通过测试设备和监控手段，应该每时每刻都能掌握基础设施的实时状况。



​	基础设施不但应该具有自治特性，而且应该是非常容易重新搭建的。这样的话，当有硬件问题时，就能迅速重建一个全新的已知状态的环境配置。

* 操作系统及其配置信息，包括测试环境和生产环境。
* 中间件软件栈及其配置信息，包括应用服务器、消息系统和数据库。
* 基础设施软件，比如版本控制代码库、目录服务以及监控系统。
* 外部集成点，比如外部系统和服务。
* 网络基础设施，包括路由器、防火墙、交换机、DNS和DHCP等。
* 应用程序开发团队与基础设施管理团队之间的关系。



​	强调合作是DevOps运动的核心原则之一。DevOps运动的目标是将敏捷方法引入到系统管理和IT运营世界中。

​	请记住指导原则：测试环境应该是与生产环境相似。也就是说，对于上面列出的所有条目，绝大多数应该是相似的（尽管不必完全相同）。其目的是为了尽早发现环境方面的问题，以及在向生产环境部署之前对关键活动（比如部署和配置）进行操作演练，从而减少发布风险。测试环境需要与生产环境足够相似，以达到这一目标。

## 理解运维团队的需要

​	无需证明，大多数项目的失败原因在于人，而不是技术本身。对于“将代码部署到测试和生产环境中”这事来说，更是如此。几乎所有大中型公司都会将开发活动和基础设施管理活动（也就是常说的运维活动）分交给两个独立的部门完成[插图]。常常能看到这两拨人的关系并不是很好。这是因为往往鼓励开发团队尽可能快地交付软件，而运维团队的目标则是稳定。

​	需要谨记的最重要的事情是：所有的项目干系人都能达成一个共识，即让发布有价值的软件成为一件低风险的事情。根据我们的经验，做这件事的最佳方法就是尽可能频繁地发布（即持续交付）。这就能保证在两次发布之间的变更很小。如果你所在的组织中，发布总是需要花上几天的时间，还要熬夜加班的话，你肯定会强烈反对这种想法。

​	运维团队依据一些关键的服务质量指标来衡量他们的效率，比如MTBF（MeanTime Between Failure，平均无故障时间）和MTTR（Mean Time To RepairFailure，平均修复时间）。运维团队常常还必须满足某些SLA（Service-LevelAgreement，服务级别的条款）。对运维团队来说，任何变更都可能是风险（包括那些可能影响到运维团队达成这些目标或其他要求的流程的变更）。既然这样，运维团队就有几个最为重要的关注点。

### 文档与审计

​	运维主管希望确保其所管任意环境中的任意变更都要被记录在案并被审计。这样一旦出了问题，他们可以查到是由哪些修改引起的。

​	变更管理流程肯定是任何组织中最重要的流程之一，它用于管理受控环境的每一次变更。通常，运维团队会掌管生产环境，以及与生产环境近似的测试环境。这就意味着，任何人在任何时候想修改一下测试环境或生产环境，都必须提出申请，并被审批。很多低风险的配置变更可以由运维团队来执行。在ITIL中，这些变更叫做“标准”变更（standard change）。

### 异常事件的告警

​	运维团队会有自己的系统来监控基础设施和正在运行的应用程序，并希望当系统出现异常状况时收到警报，以便将停机时间最小化。

​	缺乏经验的开发人员最常犯的一个编码错误就是吞噬错误信息（swallowerror）。与运维团队聊一下，你就会发现，应该把每个错误状态都记录下来，并放到某个已知的位置上，同时记录相应的严重程度，以便他们能确切知道发生了什么问题。这么做以后，若应用程序由于某种原因出问题了，运维人员能够很容易地重启或重新部署它。

​	再强调一次，了解并满足运维团队的监控需求，并把这些需求放到发布计划中是开发团队的责任。

​	首次发布仅仅是所有应用程序生命周期的一个开始。应用程序的每个新版本都会有所不同，比如错误的类型以及其生成的日志信息，可能被监控的方法也不同。它还可能以某种新的形式发生错误。因此，当要开发新版本时，让运维人员也参与其中是非常重要的，这样他们就可以为这些变更做一些准备工作。

### 保障IT服务持续性的计划

​	运维经理要参与组织的IT服务连续性计划的创建、实现、测试和维护。运维团队掌管的每个服务都会设定一个RPO（Recovery Point Objective，恢复点目标，即灾难之前丢失多长时间内的数据是可接受的）以及一个RTO（Recovery TimeObjective，恢复时间目标，即服务恢复之前允许的最长停机时间）。

​	RPO控制了数据备份和恢复策略，因为数据备份必须足够频繁，才能达到这个RPO。

​	为了满足业务方面所需的RTO，可能要额外建立一个生产环境和基础设施的副本，以便当主系统出错时，可以启用这个后备系统。

### 使用运维团队熟悉的技术

​	运维主管希望用运维团队自身熟悉的技术对其管理的环境进行变更操作，这样他们就能真正掌控和维护这些环境了。

​	在每个项目开始时，开发团队和运维团队就应该坐下来，讨论并决定应用程序的部署应该如何执行。一旦所用技术达成一致，双方可能都需要学习一下这些技术（可能是某种脚本语言，比如Perl、Ruby或Python，或者某种打包技术，比如Debian打包系统或者WiX。

## 基础设施的建模和管理

​	除了项目干系人管理之外，从广义上讲，本章的其他内容都可以算做是配置管理的一个分支。

​	每种环境中都有很多种配置信息，所有这些配置信息都应该以自动化方式进行准备和管理。

![](https://pic.imgdb.cn/item/60d9be375132923bf86058fd.jpg)

​	如果你对将要开发的系统所用技术有最终决定权的话，那么在项目的启动阶段（inception），你应该回答一个问题：用这种技术做自动化部署和配置软硬件基础设置容易吗？对于系统的集成、测试和部署的自动化来说，使用能够以自动化方式进行配置和部署的技术是一个必要条件。

​	假如你无权控制基础设施的选择，但还想全面自动化构建、集成、测试和部署的话，你必须解决下述问题。

* 如何准备基础设施？
* 如何部署和配置应用程序所依赖的各种软件，并作为基础设施的一部分？
* 一旦准备并配置好基础设施后，如何来管理它？



​	与交付流程的其他方面一样，你应该把创建和维护基础设施需要的所有内容都进行版本控制。至少对下述内容应该这么做。

* 操作系统的安装定义项（比如使用的Debian Preseed、RedHat Kickstart和Solaris Jumpstart）。
* 数据中心自动化工具的配置信息，比如Puppet或CfEngine。
* 通用基础设施配置信息，比如DNS 区域文件（zone file）、DHCP和SMTP服务器配置文件、防火墙配置文件等。
* 用于管理基础设施的所有脚本。



​	对于基础设施的变更来讲，部署流水线的工作包括三部分。首先，在对任何基础设施的变更部署到生产环境之前，它应该验证所有的应用程序在这些变更之后也能正常工作，并确保在该新版本的基础设施之上，所有受到影响的应用程序的功能和非功能测试都能成功通过。其次，它应该将这些变更放到运维团队管理的测试和生产环境上。最后，流水线还应该执行部署测试，确保新的基础设施配置已成功部署。

### 基础设施的访问控制

控制包括以下三方面。

* 在没有批准的情况下，不允许他人修改基础设施。
* 制定一个对基础设施进行变更的自动化过程。
* 对基础设施进行监控，一旦发生问题，能尽早发现。



​	当出问题时，直接登录到出问题的环境上去尝试解决问题的做法是非常有诱惑力的[这种做法有时候被礼貌地称为“试探式的问题解决方法”（problem-solvingheuristic）]。这是个可怕的想法，原因有二。首先，它通常导致服务中断（人们倾向于尝试重启或临时打服务补丁）。其次，如果在事后出现某些问题，那么根本没有记录表示谁在什么时间做了这件事，也就是说，无法找到当前遇到问题的原因。在这种情况下，你可能就需要从无到有重新创建一个环境，以便确信它处于一个已知良好的状态上。

​	如果无法通过一个自动化过程从头重新创建基础设施的话，首先要做的事情就是实现访问控制。这样，如果没有通过审批，就无法对任何基础设施作出修改。The Visible Ops Handbook把这叫做“稳住病人”（stabilizing the patient）。这无疑会带来很多不必要的麻烦，但它是下一步的前提条件，而下一步就是指在不关闭访问控制的情况下，创建自动化过程来管理基础设施。

​	对生产环境和测试环境的变更请求应该执行一个变更管理流程。这并不意味着需要官僚作风：正如The Visible Ops Handbook所指出的，在MTBF（平均无故障时间）和MTTR（平均修复时间）这两方面做得好的公司能够做到“每星期变更1000到1500次，变更成功率超过99%。”

### 对基础设施进行修改

​	当然，有时还是需要对基础设施进行修改的。高效的变更管理流程有如下几个关键特征。

* 无论是更新防火墙规则，还是部署flagship服务的新版本，每个变更都应该走同样的变更管理流程。
* 这个流程应该使用一个所有人都需要登录的ticketing系统来管理。这样就可以得到有用的度量数据，比如每个变化的平均周期时间。
* 做过的变更应该详细清楚地记录到日志中，这样便于以后做审计。
* 能够看到对每个环境进行变更的历史，包括部署活动。
* 想做修改的话，首先必须在一个类生产环境中测试通过，而且自动化测试也已经运行完成，以确保这次变更不会破坏该环境中的所有应用程序。
* 对每次修改都应该进行版本控制，并通过自动化
* 流程对基础设施进行变更。
* 需要有一个测试来验证这次变更已经起作用了。



​	如果你还没有的话，可以使用数据中心自动化工具（比如Puppet、CfEngine、BladeLogic、Tivoli或HP OperationsCenter）。

## 服务器的准备及其配置的管理

​	从较高的抽象层次上来说，服务器的准备工作（不管是为测试环境还是生产环境）最开始都要把一台机器放到数据中心，把它连接好。完成之后，后续的所有活动（包括首次加电）都可以用完全自动化的方式通过远程控制来完成。可以使用带外（out-of-band）远程管理系统（比如IPMI或LOM）启动那台机器，通过网络启动并使用PXE（描述如下）安装一个基本的操作系统，该基本操作系统中应安装数据中心管理工具（如图11-2中的Puppet）的一个代理器。然后，这个数据中心管理工具（图11-2中的Puppet）就会管理这台机器的配置。整个自动化过程如图11-2所示。

![](https://pic.imgdb.cn/item/60d9ca5f5132923bf8a265bc.jpg)

### 服务器的准备

创建操作系统基线有如下几种方法。

* 完全手工过程。
* 自动化的远程安装。
*  虚拟化。



​	对于拿到一台物理机，把它安装好并启动起来这个工作来说，自动化远程安装是一个不错的选择，即使打算以后把它作为虚拟机的宿主机来使用也是一样。最佳入手点就是PXE（Preboot eXecutionEnvironment）或Windows DeploymentServices。

​	几乎每个常见的UNIX 风格的系统都提供与PXE相适应的映像。当然，也可以自己定制映像——RedHat和Debian的包管理系统允许你将一个已安装系统的当前状态保存到一个文件中，这样就可以用它来做其他系统的初始化工作。

​	进行配置了。做这件事的一种方式是用操作系统中的无人参与安装过程：RedHat的Kickstart、Debian的Preseed，或者Solaris的Jumpstart。这些都可以用来执行系统安装之后的一些活动，比如安装操作系统补丁，并决定运行哪个守护进程。下一步就是把基础设施管理系统的代理客户端安装到这台机器上，然后就让那些基础设施管理工具来管理操作系统的配置。

### 服务器的持续管理

​	配置管理过程的目标是，保证配置管理是声明式且幂等的（idempotent），即无论基础设施的初始状态是什么样，一旦执行了配置操作后，基础设施或系统所处的状态就一定是你所期望的状态，即使某个配置项进行了重复设置对配置结果也没有影响。这在Window平台和UNIX平台都是可行的。

​	之后，就可得到如下收益。

* 确保所有环境的一致性。
* 很容易准备一个与当前环境配置相同的新环境，比如创建一个与生产环境相同的试运行环境。
* 如果某个机器出现硬件故障，可以用一个全自动化过程配置一个与旧机器完全相同的新机器。



​	而在UNIX世界里，LDAP通常是UNIX进行访问控制的工具，用于控制谁在哪台机器上能做什么。对于当前操作系统配置（比如安装了哪种软件和版本更新）的管理来说，有很多解决方案。也许最流行的工具就是CfEngine、Puppet和Chef，但还有几个类似的工具，比如Bcfg2和LCFG [9bhX9H]。在撰写本书时，唯一支持Windows平台的这类工具只有WPKG，但它不支持UNIX平台。然而，Puppet和Chef正在开发对Windows平台的支持。另外，值得一提的是难以置信的MarionetteCollective（简称mcollective），它是使用某种消息总线（message bus）来查找和管理大量服务器的一种工具。它有一些插件可以远程控制其他服务，并能与Puppet和Facter通信。

​	我们将重点介绍一下Puppet，因为它是目前最流行的开源工具（当然CfEngine和Chef也很流行）。对于其他工具来说，基本原则是相同的。Puppet通过一种声明式的外部配置信息领域专属语言来管理配置。对于那些复杂的企业级配置信息来说，可以通过常见模式将它们抽取成可以共享的模块。这样就可以避免大量的重复配置信息了。

​	Puppet的配置由一个集中式主服务器（centralmaster server）来管理。这个服务器运行Puppet后台主服务进程（puppetmasterd），它有一个列表，所有需要管理的机器都在该列表当中。每台受控机器都运行了一个Puppet代理客户端（puppetd）。它与主服务器通信，确保Puppet管理的那些机器与最新的配置信息保持同步。

​	当一个配置发生变化时，后台主服务器进程将会通知所有的客户端有新的变更了，客户端就会更新、安装并配置新软件。如果需要的话，它还会重启某些服务器。配置信息是声明式的，描述了每台服务器最终需要达到的状态。也就是说，这些服务器的初始状态可以是不同的，比如它可能是一个虚拟机的新副本，或是一个刚刚准备好的机器。

## 中间件的配置管理

### 管理配置项

​	如果中间件不是操作系统标准安装的一部分，最好是用操作系统的包管理系统将其打包，并把它放在组织级的包管理服务器上。然后就能通过所用的服务器管理系统以相同的方式对其进行管理了。

​	然而，有些中间件无法使用这种方式，通常来说，是那些设计时就没有考虑脚本化或后台安装方式的产品。下节讨论这种中间件的管理。

​	我们把应用服务器的安装目录放到了版本控制之下。然后写了一个脚本把它从版本控制库中签出，远程将其复制到所选的某个环境的正确位置上。

​	我们还记录了它的配置信息存放的位置。然后，我们在另一个版本控制库中为每个需要部署的环境都创建了一个目录。在每个环境的目录中，把与该环境有关的应用服务器的配置文件也放在里面。

​	很多现代中间件支持配置脚本化方式：XML的配置方式比较常见，并且还提供一些简单的命令行工具来做脚本化。学习并使用这些工具，像管理系统中的其他代码一样，将这些文件进行版本管理。

​	如果你有选择权的话，选择那些支持这类特性的中间件。根据我们的经验，这些工具的重要性要比华丽的管理工具高得多，甚至高于对最新标准的兼容性。

​	我们认为，除非能以自动化方式进行部署和配置，否则它就不适合企业级应用。如果不能把重要的配置信息保存在版本控制中，并以可控的方式来管理变更的话，那么这种技术会成为高质量交付的障碍。过去我们被这样的事情折磨过很多次。

### 产品研究

​	但对我们当前正要用的这个项目来说，即使现在的版本中有这个组件，或者是一两年之后才有没有什么区别，因为与一个粗糙的专有版本控制系统进行集成不利于我们对配置信息集管理的一致性。

### 考查中间件是如何处理状态的

​	如果已经确定所用中间件的确不支持任何形式的自动化配置，接下来就要看看是否能够通过对该产品后台的存储方式做版本控制了。现在，很多产品使用XML文件来存储它们的配置信息。这种文件很适合使用现代版本控制系统，很少会出现问题。如果第三方系统把它的状态保存在二进制文件中，那么可以考虑对这些二进制文件进行版本控制。随着项目开发的进展，它们通常也会频繁变更。

​	此时，对安装过程进行反向工程是必要的，而且关键是要写一个你自己的安装程序。你要找到该产品把它的二进制包和库文件安装到了哪里。

​	在这之后，你有两种选择。最简单的选择就是将相关的二进制文件与安装它们到相关环境的自动化脚本一起放到版本控制库中。第二种选择是再向前一步，自己写一个安装器（或者某种安装包，比如当你用衍生自RedHat系统的Linux系统时，就是RPM）。创建RPM安装包（或其他安装程序）并不是那么难，对问题的解决有多大帮助，就取决于你的环境了。这样，你就能使用自己的安装包将这个产品部署到一个新环境中，并从版本控制库到获取配置信息，应用到其上。

### 查找用于配置的API

​	很多产品会提供某种可编程接口。有些产品会提供一些API足以让你对系统进行配置，满足你的需求。一种策略是自己为系统定义一个简单的配置文件。创建自定义的构建任务来解释这些脚本，并使用API对系统进行配置。这种“创造自己的”配置文件的方式让配置管理权回到了你的手中（你可以对配置文件进行版本控制，并以自动化的方式来使用它们）。根据以往的经验，对于微软的IIS，我们就是用这种方法通过它自己的XML元数据库（metabase）进行自动化配置管理的。现在，IIS的新版本已经可以通过PowerShell进行脚本化了。

### 使用更好的技术

​	理论上，你可以尝试一些其他方法。例如，自行创建有利于版本控制的配置信息，然后写一些代码，通过产品自身的使用方式把它们映射到你所选产品的配置上——如通过管理控制台的用户交互回放方式或对数据库结构进行反向工程。现实中，我们还没有真正这么做过。虽然曾经遇到过这种情况，但通常都找到了一些API，让我们能完成我们想做的事。

## 基础设施服务的管理

​	经常看到一些已经成功通过部署流水线并正在生产环境中运行的软件因为基础设施服务的问题（比如路由、DNS和目录服务）而不能正常工作的情况。

​	最后证明，问题出在某个防火墙每运行一个小时后会扔掉不活跃的TCP连接。由于系统在夜间处于空闲状态，当早上开始有活动时，数据库连接的TCP包就会被悄悄扔掉。

我们有如下几个建议。

* 网络基础设施配置的每个部分[从DNS 区域文件（zone file）到DHCP、防火墙、路由配置，到SMTP以及应用程序所依赖的其他服务]都应该进行版本控制。使用像Puppet这样的工具把配置文件从版本控制库中取出放到运行系统上，以便能将它们自动化。这种方式还确保了只有通过修改版本控制库中的配置文件才能对环境进行修改。
* 安装一个好用的网络监控系统，比如Nagios、OpenNMS、HPOperations Manager或者它们的同类产品。保证当网络连接被破坏时你就会得到通知，而且监控应用程序所使用的每个路由的每个端口。关于这个问题的细节会在11.9节讨论。
* 日志是你的好伙伴。每次网络连接超或者连接异常关闭时，应用程序都应该在“警告”（warning）这一级别进行记录；每次关闭连接时，应该使用INFO级别进行记录，如果日志显得太冗长，也可以使用DEBUG级别。每次打开连接时，应该使用DEBUG级别记录，并且尽可能多地包含所连终端的相关信息。
* 确保冒烟测试在部署时检查所有的连接，找出潜在的路由或连接问题。
* 确保集成测试环境的网络拓扑结构尽可能与生产环境相似，包括使用同样的硬件和物理连接（甚至使用相同的socket和同样的缆线）。以这种方式构建出来的环境甚至可以作为硬件故障时的一个备用环境。事实上，很多企业都有这种双重身份的试运行环境，既承担生产环境部署的测试目的，也作为故障备份。10.4.3节中提到的蓝——绿部署模式让你能够做到这一点，即使你只有一个物理环境。



​	最后，当出现问题时，使用一些辅助工具。Wireshark和Tcpdump都是相当有用的工具，用它很容易查看和过滤包，从而完全隔离你想要找的包。



**多宿主系统**

​	生产系统的一个重要的增强部分是为不同类型的流量使用多个隔离网络，并与多宿主服务器结合使用。多宿主服务器有多个网络接口，每个接口对应一个不同的网络。至少，有一个网络用来监控和管理生产服务器，一个用于运行备份，一个用于在服务器间做生产数据的传输。

​	安全起见，管理网络与生产环境网络是物理隔离的。通常，要求控制监管生产服务器的任何服务（如ssh或SNMP）都会被配置成只绑定nic2，这样就不可能从生产环境网络中访问到这些服务。备份网络与生产环境网络也是物理隔离的，以便当备份时大量数据的移动不会影响性能或管理网络。高可用性高性能系统有时会为了生产数据而使用多个NIC，也许是为了故障转移，也许是为了专属服务，比如可能有一个隔离的专属网络作为组织的消息总线或数据库。

​	对一个多宿主网络配置的所有配置信息（包括路由）都应该进行集中管理和监控。在需要访问数据中心时很容易出错，比如Jez在其职业生涯早期，就曾经在生产环境上降低了管理用NIC，并且忘记了他是通过SSH登录到机器上而不是物理的TTY。正如Nygard指出的[插图]，很可能还会引起更严重的路由错误，比如在一个多宿主机器（multihomed box）将流量从一个NIC导向另一个，潜在地创建了安全漏洞，比如导出客户数据。



## 虚拟化

​	一般来说，我们可以认为虚拟化是一种在一个或多个计算机资源上增加了一个抽象层的技术。

​	虚拟化有助于减少部署软件所花费的时间，并用一种不同的方式来降低与部署相关的风险。就在系统的宽度与深度两方面达到高效配置管理来说，部署领域中虚拟机的使用帮了很大的忙。

* 对需求的变化作出快速响应。需要一个新的测试环境？准备一个新的虚拟机在几秒钟内就能完成，而无需几天甚至几星期内申请一个新的物理环境。当然，无法在一台机器上运行无限多个虚拟机。但在某些情况下，用虚拟化技术可以把买硬件的需求与它们运行所需要的环境的生命周期这两者之间进行解耦。
* 固化。当组织相对不成熟时，每个团队常常有其自己的持续集成服务器和位于他们的物理机上的测试环境。虚拟化让持续集成和测试基础设施的固化变得非常容易，因此可以将它作为一种服务提供给交付团队。对于硬件的使用而言，它也更加高效。
* 硬件标准化。组件和应用程序的子系统之间的功能差异不再迫使你来维护不同的硬件配置，它们都有自己的规范。虚拟化让你能够为物理环境进行单一的硬件配置标准化，却可以虚拟运行多种混合环境和平台。
* 基线维护更容易。你能维护一簇基线映像（包括操作系统和应用程序栈）甚至环境，并且通过一键式方式将其放到一个集群中。



​	当将其应用到部署流水线中时，它可以算是简化环境维护和准备工作最有用的技术了。

* 虚拟化技术提供了一个简单的机制来创建系统所需的环境基线。可以创建并调整那些虚拟服务器，与应用程序相匹配。一旦调整好以后，就可以保存这些映像及配置，然后就能随时创建任意多个所需要的环境。要知道，拿到的是和原始环境一模一样的副本。
* 因为所保存的服务器映像在一个库中，并且能够与应用程序的某个特定版本进行关联，所以这就很容易将任何环境恢复到原有状态，不仅仅是恢复应用程序本身，还包括该软件版本的其他方面。
* 通过使用虚拟服务器来做主机环境的基线使创建生产环境的副本变得更容易，即使一个生产环境中包含多台服务器也无所谓。当创建测试环境时，也很容易重现生产环境的配置。现代虚拟软件都提供了一定程度的灵活性，对于系统某些方面（比如网络拓扑）可以通过编程的方式进行控制。



虚拟化也有助于提高对功能需求和非功能需求的测试能力。

* VMM提供了对系统某些方面的编程控制方式，比如网络连接。这让非功能需求的测试（比如可用性）更容易，且可以自动化。例如，可以直接通过编程方式从一个服务器集群中分离出一台或多台服务器，从而测试集群的行为，观察对系统的影响。
* 虚拟化还提供了显著加快运行时间较长的那些测试。因为可以将这些测试放在多台虚拟机上并行运行，而不是放在一台机器上串行运行。我们在自己的项目上经常这么做。在我们的一个大项目中，通过这种方式，测试运行时间从13小时降到45分钟。

### 虚拟环境的管理

​	虚拟机技术的最重要特性之一就是一个虚拟机映像只是一个文件。这个文件叫做“磁盘映像”。磁盘映像的好处在于可以复制它们，并对它们进行版本控制（当然不一定要在文件版本控制系统中，除非版本控制系统可以处理大量的大二进制文件）。这样，就能把它们作为模板或者基线（这是配置管理术语）。有些VMM认为“模板”和“磁盘映像”是不同的，但实质上它们是一回事儿。很多VMM甚至允许用正在运行的虚拟机创建模板。这样，就可以用这个模板随时创建任意多个运行实例了。

​	有些VMM供应商提供了另一个有用的工具，用于抓取物理机的快照，然后将其转成磁盘映像。这是极其有用的，因为这意味着，可以拿到生产环境的一份副本，将它们保存为模板，然后用其创建生产环境的多个副本来做持续集成，并在其上进行各类测试。

​	在本章开头，我们讨论过如何使用完全自动化的过程来准备新环境。如果有虚拟化基础设施，那么就可以创建一个已准备好的服务器的磁盘映像，把它作为所有具有相同配置的服务器的模板。或者，可以使用类似rPath的rBuilder这样的工具来创建并管理基线。一旦准备好了运行环境中所有机器的模板以后，就可以根据需要，使用VMM软件在这些模板中启动新环境了。

![](https://pic.imgdb.cn/item/60da6e5f5132923bf830ea1e.jpg)

​	现在，我们就能以增量的方式来实现一个自动化的环境准备过程了。为了避免每次都要从头做起，可以用一个已处于良好状态的基线映像（仅包含一个安装好的全新操作系统也行）作为基线来开始实现这个自动化的准备过程。要在每个模板中都安装有数据中心自动化工具的一个代理器（如图11-5中的Puppet），以便实现虚拟机的全自动管理，一旦做到这种全自动化，向整个系统推送变更信息时就可以保持一致性了。

![](https://pic.imgdb.cn/item/60da6ff05132923bf837e603.jpg)

​	虚拟化还能让另外两种（本章前面提到过的）不可追踪的场景更容易管理：（1）已经用非受控方式修改过的环境，（2）无法以自动化方式来管理栈中的软件。

​	虚拟化技术为我们提供了一种方式来缓解这种风险。使用虚拟化软件为生产环境中所有正在运行的机器抓个快照，并把它们转换成虚拟机。然后，就可以很容易地为测试活动创建该运行环境的副本了。

​	最后，虚拟化技术还提供了另一种方式来处理那些无法用自动化方式安装或配置但却被应用程序所依赖的软件，包括COTS。只要在虚拟机上手工安装并配置好这种软件，然后创建模板就行了。可以把这个模板当做一个基线，当需要时，直接使用它就行了。

### 虚拟环境和部署流水线

​	部署流水线的目的是，对应用程序做的每个修改都能通过自动化构建、部署和测试过程来验证它是否满足发布要求。一个简单的流水线如图11-6所示。

![](https://pic.imgdb.cn/item/60da70515132923bf839a27b.jpg)

​	部署流水线的一些特性值得我们再重新想一想，看一看如何在虚拟化技术中使用这些特性。

*  流水线的每个实例都与版本控制库中触发它的那个修改相关联。
* 流水线中提交阶段之后的每个阶段都应该运行在类生产环境上。
* 使用相同二进制包的同一个部署流程应该可以运行在每个环境上，而这些环境之间的不同之处应该作为配置信息来对待。



​	从中可以看出，在部署流水线中所测试的内容不仅仅是应用程序本身。的确，当在流水线中发生测试失败时，第一件事就是确定失败的原因。如下是五个最可能的原因。

* 应用程序代码中的bug。
* 某个测试中的bug或不正确的期望值。
* 应用程序的配置问题。
* 部署流程中的问题。
* 环境问题。



​	当将软件发布到生产环境时，你所用的生产环境应该与运行测试所用的环境一致。所有这一切的必然结果是：与其他内容（源代码、测试、脚本等）的变更一样，对环境配置的任何更改都应触发一个新的流水线实例，参见图11-7。构建和发布管理系统应该能记住用来运行部署流水线的虚拟机模板，当部署到生产环境时，也应该能够启动同一套虚拟机模板。

![](https://pic.imgdb.cn/item/60da718e5132923bf83f2418.jpg)

​	然而，每次都利用虚拟机基线的副本重新创建一个新的基线，让每个变更都部署到试运行环境和生产环境，这绝对不是一个好主意。如果这么做，不但磁盘空间很快就被占满，而且还会失去通过被版本控制的声明式配置信息进行基础设施自动化式管理所带来的好处。最好还是保持相对稳定的VM映像，即一个最基本的操作系统，并安装有最新的服务包，必需的中间件或者它所依赖的其他软件，还有负责和数据中心管理服务器进行交互的客户端软件。然后，用这个工具来完成准备过程，并将基线配置成所需的正确配置。

### 用虚拟环境做高度的并行测试

​	对于需要用户自行安装的软件来说，事情就有些不同了，尤其是在企业环境以外时。在这种情况下，你对生产环境的控制能力并不强，因为那是用户自己的计算机。此时，在各种可能的“类生产环境”中对软件进行测试就非常重要了。例如，桌面应用通常都是支持多平台的，可以运行在Linux、Mac OS和Windows之上，而且每个平台通常包含不同的版本和配置。

​	虚拟化提供了一种绝好的方法来处理多平台测试。只要为应用程序可能运行的每种平台创建虚拟机，并在其上创建VM模板。然后在所有这些平台上并行运行部署流水线中的所有阶段（验收、容量和UAT）就行了。现代持续集成工具对这种方法都提供直接支持。

​	可以使用同样的技术让测试并行化，从而缩短代价高昂的验收测试及容量测试的反馈周期。假设所有的测试都是独立的（参见8.7节中我们的建议），那么就可以在多台虚拟机上并行执行它们。当然，也可以通过不同的线程来并行运行它们，但线程方式有一定的局限性。为构建版本创建一个专门的计算网格能大大地加速运行自动化测试。最终，测试的性能仅仅受限于那个运行得最慢的测试用例的时间和硬件预算问题了。现代持续集成工具和像Selenium Grid这样的软件都让这件事变得非常简单。

![](https://pic.imgdb.cn/item/60da72b45132923bf844657f.jpg)

​	环境中的每台机器都通过虚拟LAN与外界联通。我们完全可以使用虚拟化API来做自动化非功能测试，通过编程方式来模拟应用程序与数据库服务之间的连接中断。毫无疑问，如果没有虚拟化技术，这件事情是非常困难的。

## 云计算

​	云计算的概念很久之前就出现了，但直到最近几年它才变得无所不在。在云计算中，信息存储在因特网中，并通过因特网上的软件服务进行读取和使用。云计算的特征是：通过扩展所使用的计算资源（比如CPU、内存、存储等）来满足需求，而只需要为自己所使用的这些资源付费就行了。云计算既包括它所提供的软件服务本身，也包括这些软件所用到的软硬件环境。

​	云计算的大体上分为三类[9i7RMz]：云中的应用、云中的平台和云中的基础设施。云中的应用指像WordPress、SalesForce、Gmail和Wikipedia这样的软件服务，即将传统的基于Web的服务放到云基础设施上。SETI@Home可能是云中应用的最早的主流例子。

### 云中基础设施

​	云中基础设施是最高层次的可配置性，比如AWS。AWS提供了很多基础设施服务，除了著名的名为EC2的虚拟机托管服务以外，还包括消息队列、静态内容托管，流媒体托管，负载均衡和存储。利用这些服务，几乎可以对系统进行完全控制，但也要做一些工作把这些东西绑定在一起[插图]。

​	打算迁移到云基础设施的人会提出两个非常重要的问题：安全问题和服务级别问题。

​	安全常常是大中型企业提到的第一个障碍。当生产环境基础设施放在别人手上时，如何防止别人危害你的服务，偷取你的数据呢？云计算的供应商已经意识到这个问题，并建立了多种机制来解决它，比如高度可配置的防火墙，以及连接用户公司VPN的私有网络。最终，尽管使用基于云的基础设施的风险有所不同，而且还需要筹备推入基于云的计划，但“基于云的服务的安全性比部署到公司自己的基础设施上的对外开发服务低”这种说法是缺少基本理由支持的。

​	整个基础设施都外包以后，服务级别就特别重要了。比如，在安全性方面，需要做一些调研以确保供应商能满足你的需求。当遇到性能问题时，这尤其重要。根据你的需求，Amazon提供了不同层次的性能参考，但即使它们提供的最高级的性能也无法与真实的高性能服务器相比。如果你的关系型数据库上有大量数据集且高负载的话，也许就不适合放在虚拟环境上。

### 云中平台

​	云中平台的例子包括一些服务，比如Google App Engine和Force.com，服务供应商给你提供了一个标准的应用栈来使用。作为你使用它们提供的应用栈的回报，它们会帮你解决应用程序和基础设施的扩展问题。关键是，你牺牲了灵活性，所以供应商可以很容易地应对非功能需求，比如容量和可用性。云中平台的优点如下。

* 就成本结构和准备工作的灵活性而言，它与云中基础设施的收益是一样的。
* 服务供应商会处理非功能需求，比如可扩展性、可用性和某种程度的安全性。
* 将应用部署到完全标准化的应用栈上，就意味着不需要担心测试环境、试运行环境和生产环境的配置和维护，也不需要担心虚拟机映像的管理。



​	最后一点尤其是革命性的。在本书中，用了大量的篇幅来讨论如何自动化你的部署、测试和发布流程，以及如何搭建和管理测试和部署环境。使用云中平台几乎完全不需要考虑这些方面。通常，可能只运行一条命令就可以将应用程序部署到因特网上。甚至能够在几分钟内就能从什么都没有的状态到完成一个应用程序发布。从自身的角度来说，一键部署也可以说是零投资的。

​	和云中基础设施一样，云中平台也面临着同样的不适用性。特别值得指出的是，在便携性和供应商绑定方面要比云中平台更严重。

### 没有普适存在

​	当然，可以混合和匹配使用不同的服务来实现系统。例如，可以把静态内容和流媒体放在AWS上，把应用程序放在Google App Engine，把专有服务放在自己的基础设施上。

​	为了实现这种方式，应用程序就要被设计成可以在这种混合环境中工作。这种部署方式也要求实现一种松耦合的架构。就成本和满足非功能需求而言，松耦合架构让这种混合解决方案带来引人瞩目的业务价值。当然，如何设计出这种架构是比较难的问题，也超出了本书的范围。

### 对云计算的批评

​	首先，“云”当然不是因特网——为互操作性和弹性而设计的一个开放的架构体系。每个供应商都提供一种不同的服务，而你在某种程度上被绑定在所选择的平台上。在一段时间里，对等服务（peer-to-peer service）似乎是最有可能构建大型分布式可扩展系统的技术。然而，对等服务的愿景并不清晰，因为对于供应商来说，很难从对等服务中赚到钱，而云计算仍旧遵循那个很容易理解的如何赚钱的效用计算模型。本质上来说，这也意味着你的应用和你的数据最终都在供应商的掌握中。这可能比你当前的基础设施要好，也可能不好。

## 基础设施和应用程序的监控

​	确切了解生产环境中正在发生什么事情是非常关键的，原因有三。首先，如果有实时的商业智能（BI），业务人员可以更快地从他们的策略得到反馈，比如产生了多少收入，这些收入来自哪里。其次，当出了问题时，需要立即通知运维团队有事情发生，并利用必要的工具追溯事件的根因并修复它。最后，出于计划目的，历史数据也非常重要。假如当未预见的事情发生时或者新增服务器时，你却拿不出来与系统如何运行相关的详细数据，就无法制订计划对基础设施进行改造，以满足业务需求。

​	当创建监控策略时，需要考虑以下四个方面。

* 对应用程序和基础设施进行监测，以便可以收集必要的数据。
* 存储数据，以便可以很容易拿来分析。
* 创建一个信息展示板（dashboard），将数据聚合在一起，并以一种适合运维团队和业务团队使用的形式展现出来。
* 建立通知机制，以便大家能找出他们关心的事件。

### 收集数据

​	首先，最重要的是决定你想收集什么样的数据。监控数据的来源可能有以下几个。

* 硬件，通过带外管理[out-of-band management，也被称为LOM（Lights-Out Management，远端控制管理）。几乎所有的现代服务器硬件都实现了IPMI（Intelligent Platform Management Interface，智能平台管理接口），让你可以监控电压、温度、系统风扇速度、peripheral health，等等，还要执行一些活动，比如反复开关电源或点亮前面板的指示灯，即使机器已经关机了。
* 构成基础设施的那些服务器上的操作系统。所有操作系统都提供接口以得到性能信息，比如内存使用、交换空间（SWAP）的使用、磁盘空间、I/O 带宽（每个磁盘和NIC）、CPU使用情况，等等。通过监控进程表来了解每个进程所用的资源也是非常有用的。在UNIX上，“收集”（Collectd）是一个标准方法来收集这些数据。在Windows平台上，利用一个叫做性能计数器的系统来做这件事，它也可以被其他供应商的性能数据所使用。
* 中间件。它可以提供资源的使用信息，如内存、数据库连接池、线程池，以及连接数、响应时间等信息。
* 应用程序。应用程序应该设计实现一些数据监控的钩子（hook）功能，这些数据应该是运维人员和业务人员比较关心的，比如业务交易数量、它们的价值、转换率，等等。应用程序应该使对用户分布以及行为的分析变得很容易。它应该记录其所依赖的外部系统的连接状态。最后，如果适当的话，它还应该能报告它自己的版本号及其内部组件的版本。



​	领先的开源工具包括Nagios、OpenNMS、Flapjack和Zenoss，当然还有很多[dcgsxa]。领先的商业产品是IBM的Tivoli，惠普的Operations Manager、BMC和CA。这一领域中，还有一个新进入的商业产品，那就是Splunk。

**Splunk**

​	近几年里IT运维领域的杀手级工具之一就是Splunk。Splunk会对整个数据中心中的日志文件和其他包含时间戳的文本文件（前面提及的那些数据源都可以通过配置提供时间戳）进行索引。这样，就可以进行实时搜索，精确找到非正常事件，进行根因分析。Splunk甚至可以作为运维信息展示板来使用，并可以通过配置来发送通知。

​	实际上，为了监控，这些产品使用了各种开放技术。最主要的是SNMP，以及它的后继者CIM和JMX（用于Java系统）。

​	SNMP是监控领域最可敬且最常见的标准。SNMP有三个主要组成部分：受管理的物理设备（如服务器、交换机、防火墙等物理系统），代理器（通过SNMP与那些你想监控和管理的应用或设备进行联系的代理），以及监控被管理设备的网络管理系统。网络管理系统和代理通过SNMP协议进行通信，它是标准TCP/IP栈最顶层的一个应用层协议（application-layer protocol）。SNMP的架构如图11-9所示。

![](https://pic.imgdb.cn/item/60da80115132923bf885fed5.jpg)

​	在SNMP中，所有的都是变量。通过查看这些变量来监控系统，通过修改变量来控制它们。而某种类型的SNMP代理使用哪些变量，以及这些变量的描述、类型、是否可写还是只读等这些信息都在一个MIB（Management Information Base，一种可扩展的数据库格式）中描述。每个供应商都为其所提供的SNMP代理器系统定义了MIB，并且IANA维护了一个中央注册表 [aMiYLA]。与很多设备一样，几乎每个操作系统和大多数常见的中间件（比如Apache、WebLogic和Oracle）及很多设备都自带SNMP。当然，尽管这是一个很平常的事儿，但通过开发和运维团队之间的密切合作，也能为自己的应用程序创建SNMP代理和MIB。

### 记录日志

​	应用程序也应该产生高质量的日志。尤其重要的是注重日志级别。大多数日志系统有几个级别，比如DEBUG、INFO、WARNING、ERROR和FATAL。默认情况下，应用程序应该只显示WARNING、ERROR和FATAL级别的消息，但当需要做跟踪调试时，可以在运行时或部署时配置成其他级别。由于日志只对运维团队可见，所以在日志信息中打印潜在的异常是可接受的。这对调试工作非常有帮助。

​	需要记住的是，运维团队是日志文件的主要用户群。对于开发人员来说，当与技术支持团队一起解决用户报告的问题，以及与运维团队解决生产环境的问题时，日志文件是很有启发作用的。开发人员会很快意识到，那些可恢复的应用程序错误（比如某个用户不能登录）不应该属于DEBUG之上的级别，而应用程序所依赖的外部系统的超时就应该是ERROR和FATAL级别（取决于应用程序没有这个外部服务是否还可以处理事务）。

​	像其他的非功能需求一样，应该把属于审计中的部分日志也作为第一级的需求来对待。与运维团队沟通，找出他们需要什么，并从一开始就把这些需求纳入计划。尤其要考虑日志的全面性与可读性之间的权衡。对于人来说，能够一页一页地翻看日志文件或从中很容易地找出他们想要的数据是非常关键的。也就是说，每一项都应以表格或使用基于列的格式在一行中给出，使时间戳、日志级别，以及错误来自应用程序的什么地方、错误代码和描述能够一目了然。

### 建立信息展示板

​	就像使用持续集成的开发团队那样，运维团队也应该有一个大且易见的显示器。如果有任何突发事件，都可以在上面高亮显示。当出问题时，就可以查看细节找到问题原因。所有开源工具和商业工具都提供这种功能，包括能够看到历史趋势，并生成某种报告。Nagios的一个截屏如图11-10所示。能够知道每个应用程序的哪个版本运行在哪个环境中也是极其有用的，而这也需要一些工具和集成工作。

![](https://pic.imgdb.cn/item/60da81195132923bf88b2b3c.jpg)

​	你能监控的信息有数千种之多，所以提前规划一下，让运维信息展示板不要太杂乱还是非常必要的。整理出一个风险列表，并依据可能性和影响进行分类。这个列表可能包括一般风险，比如没有磁盘空间或非法访问环境，还包括一些业务的特别风险，比如事务无法完成等。这样，你就要找出什么东西是真正需要监控的，如何显示这些信息。

​	绿灯代表以下条目都已成事实。

* 所有预期的事件都发生了。
* 没有异常事件发生。
* 所有度量项都是正常的（在这段时间中都在两个标准差之内）。
* 所有状态都充分运作。

黄灯代表以下条目至少有一项是事实。

* 某个预期的事件没有发生。
* 至少有一个中等严重程度的异常事件发生了。
* 一个或多个参数高于或低于阈值。
* 一个非关键状态没有充分运作（比如，一个断路器关闭了一个非关键功能）。

红灯代表以下条目中至少有一项是事实。

* 一个必定发生的事件没有发生。
* 至少有一个严重程度为高的异常事件发生了。
* 一个或多个参数远远高于或低于阈值。
* 一个关键状态没有充分运作（比如，“接受请求”在应该为“成功”时，结果却是“失败”）。

### 行为驱动的监控

​	就像开发人员通过写自动化测试做行为驱动开发来验证应用程序的行为那样，运维人员也能写自动化测试来验证基础设施的行为。可以先写个测试，验证它是失败的，然后定义一个Puppet manifest（或者任何所用的配置管理工具）让基础设施达到所期望的状态。接下来运行这个测试来验证这种配置可以正确工作，且基础设置的行为与期望的行为一致。

​	![](https://pic.imgdb.cn/item/60da82325132923bf890d281.jpg)

## 小结

略

# 数据管理

## 引言

​	对于测试和部署过程来说，数据及其管理与组织会带来一些特定的问题，原因有两个。首先，一般来说，测试中会涉及庞大的数据量。分配给应用程序本身（它的源代码和配置信息）的空间通常远远比不上记录其状态的数据量。其次，应用程序数据的生命周期与系统其他组成部分的生命周期是不同的。应用程序数据需要保存起来。事实上，数据通常要比创建和访问这些数据的应用程序的寿命长。而重点则在于，当系统升级或回滚时，需要保存并迁移数据。

​	大多数情况下，当部署新代码时，可以删除前一个版本，并用新版本完全代替旧版本。这样可以确定部署的初始状态。尽管在某些情况（很少）下，对数据这么做也是可行的，但在现实中，大多数系统无法使用这种方式。一旦将某个系统发布到了生产环境中，与其相关联的数据就会不断增加，并以其自己特定的形式提供着巨大的价值。甚至可以说，它是系统中最有价值的一部分。当我们需要修改结构或内容时，问题就来了。

​	随着系统的发展和演进，这类修改是不可避免的。因此，我们必须找到某种机制，既允许变更，同时又能使损失最小化，让应用和部署流程的可靠性更高。



## 数据库脚本化

​	与系统中其他变更一样，作为构建、部署、测试和发布过程的一部分，任何对数据库的修改都应该通过自动化过程来管理。也就是说，数据库的初始化和所有的迁移都需要脚本化，并提交到版本控制库中。无论是为开发人员创建一个新的本地数据库，还是为测试人员升级系统集成测试（Systems Integration Testing, SIT）环境，或者作为发布过程的一部分迁移生产环境中的数据库，都应该能够使用这些脚本来管理交付流程中的每个数据库。

**初始化数据库**

​	在这种交付方式中，一个极其重要的方面就是：能够以自动化方式重新建立一个应用程序的运行环境。如果做不到这一点，就无法断定系统的确是以期望的方式运行的。

简而言之，部署一份新数据库的过程如下。

* 清除原有的数据库。
* 创建数据库结构、数据库实例以及模式等。
* 向数据库加载数据。

## 增量式修改

​	持续集成要求在每次修改应用程序后，它都能够正常运行。这也包括对数据结构和数据内容的修改。持续交付要求我们必须能够部署应用程序的任意一个已通过验证的版本（包括对数据库变更的版本）到生产环境（对于用户自行安装且包含数据库的软件也是一样的）。除了那种最简单的系统，对数据库进行更新的同时，还要保留它们的数据。最后，由于在部署时需要保留数据库中的已有数据，所以需要有回滚策略，以便当部署失败时使用。

###  对数据库进行版本控制

​	以自动化方式迁移数据最有效的机制是对数据库进行版本控制。首先，要在数据库中创建一个，用来保存含它的版本号。然后每次对数据库进行修改时，你需要创建两个脚本：一个是将数据库从版本x升级到版本x+1（升级脚本），一个是将数据库版本x+1降级到版本x（回滚脚本）。还需要有一个配置项来设置应用程序使用数据库的哪个具体版本（它也可以作为一个常量放在版本控制库中，每次有数据修改时更新一下）。

​	在部署时，可以用某种工具来查看当前部署的数据库版本以及将要部署的应用程序所需要的版本。然后再找到需要运行哪个脚本将数据库从当前版本迁移到目标版本，并依据顺序在数据库上执行它。对于升级来说，它会按正确的顺序执行所有的升级脚本，从最老的到最新的；对于降级来说，它会以相反的顺序执行对应的降级脚本。Ruby On Rails本身就以ActiveRecord迁移这种方式提供了这种技术。如果用Java或．NET，我们的同事开发了一个简单的开源应用叫做DbDeploy（对应的．NET版本是DbDeploy.NET）可以为你管理这一过程。还有其他几种解决方案也能做类似的事情，包括Tarantino、微软的DbDiff和IBatis的Dbmigrate。

### 联合环境中的变更管理

​	在很多组织中，所有应用程序常常通过一个数据库互相集成。我们并不推荐这么做，最好是让这些应用程序直接交互，并找出在什么地方需要公共的服务（就像面向服务架构里的做法那样）。然而，有些情况下直接通过数据库集成也是合理的，或者因为架构改造工作太多，所以无法修改应用程序的架构。

​	最后，要确保在修改时已与维护其他应用程序的团队达成一致。管理增量修改的一种方法是让应用程序与数据库的多个版本兼容，以便数据库的迁移与依赖于它的那些应用程序相对独立。对于那种无停机发布（Zero-Downtime Release）来说，这种技术也很有用。下面将讨论一下无停机的发布。

## 数据库回滚和无停机发布

### 保留数据的回滚

​	在回滚时，回滚脚本（如前所述）通常被设计成可以保留执行升级后产生的数据。如果回滚脚本满足下面的条件，就应该没有什么问题。

* 它应该包括模式修改，即不丢失任何数据（比如范式化或非范式化，或者在表间移动列）。在这种情况下，只要运行回滚脚本就行了。
* 它只删除新版本使用的那些数据，假如这些数据丢失了也没什么大问题。在这种情况下，只要运行回滚脚本就行了。



​	然而，有些时候简单地运行这些回滚脚本是不行的，如下所述。

* 回滚涉及从临时表中将数据导回来。此时，由升级而新增的数据记录会破坏集成约束。
* 回滚要删除那些旧版本系统无法接受的数据。



​	一种方法是将那些不想丢失的数据库事务（transaction）缓存一下，并提供某种方法重新执行它们。当升级数据库和应用程序到新版本时，确保记录了每次在新版本上发生的事务。可以通过记录来自UI的所有事件，比如通过拦截系统各组件间传递的较粗粒度的消息（如果应用程序使用了事件驱动范式，就会相对容易一些），或真实地复制事务日志中发生的每个数据库事务。一旦应用程序被成功地重新部署，这些事件就可以被重新播放一遍。当然，这种方法需要细心地设计和测试以确保它可以发挥作用。如果真的需要确保回滚时不丢失数据，这也是一种可接受的做法（tradeoff）。

​	如果使用蓝——绿部署（参见第10章）的话，可以考虑第二种方法。提示一下，在蓝—绿部署环境中，应用程序会同时有新旧两个版本同时运行，一个运行于蓝环境，另一个在绿环境。“发布”只是将用户请求从旧版本转到新版本上，而“回滚”只是再把用户请求转到旧版本上而已。

​	如果使用蓝——绿部署方法，在发布时就要为生产数据库（假设它是蓝数据库）做一个备份。如果数据库不允许热备份，或者有其他原因无法这么做的话，就要将应用程序切换到只读状态，以便能够执行备份。然后，这个备份被放在绿环境中，并在其上执行迁移操作。然后，再把用户切换到绿环境上。

​	有些系统的数据太多，如果没有较长的停机时间，几乎无法执行这类备份和恢复操作。此时就不能用这种方法了。尽管使用蓝——绿环境仍旧可行，但需要在发布时切换到共同的数据库上，而不是使用自己的独立数据库。

### 将应用程序部署与数据库迁移解耦

​	还有第三种方法可用于管理热部署，那就是将应用程序部署过程与数据库迁移过程解耦，分别执行它们，如图12-1所示。

​	![](https://pic.imgdb.cn/item/60dc05b55132923bf84833d1.jpg)

​	如果能频繁发布，那么就不需要每次发布应用程序时都迁移数据库。当真的需要迁移数据库时，不能只让应用程序仅仅和新版本的数据库相匹配，还要确保应用程序既适应于该新版本，也适合于当前运行的版本。在图12-1中，版本241既能与当前部署的数据库版本14相匹配，也能在数据库的新版本15上运行。

​	当很难将数据库回滚到某个较早的版本时，这种方法也很管用。有一次，我们在对数据库的新版本做很大修改时（包括修改了数据库模式，丢失了一些数据），就曾用过这种方法。当发生问题后，通过再次升级操作回滚到早期版本的效果。我们部署了应用程序的一个新版本（它是向后兼容的，可以运行在早期版本的数据库上），但没有部署新的数据库变更。我们观察了一下这个新版本，确认它没有引入问题。最后，当我们有把握后，就部署了数据库的变更。

​	虽然对于那些常见的修改来说，向前兼容性是一个可采纳的有效策略，但它不是一种通用的解决方案。在这里，“向前兼容性”是指应用程序的早期版本仍旧可以工作在后续版本的数据库上的一种能力。当然，如果在新的模式中有新增的字段或表，那么它们会被该版本的应用程序忽略。只不过，两个数据库版本的共同部分还是一样的。

​	于大多数变更来说，最好将下面这种方法作为默认方法，即大多数修改应该是增加操作（比如向数据库中增加新表或字段），尽可能不修改已存在的结构。

## 测试数据的管理

​	首先是测试性能。我们想确保测试尽可能快地完成。就单元测试而言，要么根本不要依赖数据库来运行，要么运行在一个内存数据库上。对于其他类型的测试，就要细心管理测试数据了，一定不要使用生产数据库的一个dump，除非有特殊情况。

​	其次就是测试的独立性。理想的测试应运行在已定义好的环境中，其输入应该是受控的，这样我们才能很容易地评估它的输出。另一方面，数据库是信息的持久存储，每次测试可能会修改其持久化内容，除非采取某些措施，阻止这样的事情发生。否则的话，这会导致起始条件不清晰，尤其是当无法直接控制测试的执行顺序时。可遗憾的是，事实往往就是这样的。

### 为单元测试进行数据库模拟

​	单元测试不使用真正的数据库是非常重要的。通常单元测试会使用测试替身对象来取代与数据库打交道的服务。如果做不到这一点（比如你想测试这些服务）的话，你可以用另外两种策略。

​	一是用测试替身对象来替代那些访问数据库的代码。在应用程序中将这些代码封装起来是个最佳实践。为了做到这一点，常用的模式是repository模式[blIgdc]。在这种模式中，你在数据访问代码之上创建一个抽象层，将所用的数据库与应用程序解耦（这实际上就是13.2.3节“通过抽象来模拟分支”的一种实际应用场景）。这么做之后，就可以用测试替身对象替换出数据访问代码，如图12-2所示。

​	![](https://pic.imgdb.cn/item/60dc063d5132923bf84cd7c2.jpg)

​	如果不使用这种方法，也可以使用假的数据库。有几个开源项目提供了内存关系型数据库（比如H2、SQLite或JavaDB）。当能够通过配置来指定程序使用哪个数据库实例时，就能让单元测试运行在一个内存数据库之上，而让验收测试运行在平时使用的那个基于磁盘的数据库之上。另外，这种方法还有一些额外收益：它促使开发人员以更加解耦的方式来编写代码，至少代码可以运行在两种数据库实现上。反过来，这也确保未来的数据库修改（数据库新版本或甚至可能是不同的关系型数据库供应商）更容易。

### 管理测试与数据之间的耦合

​	总的来讲，有以下三种方法可以用来做测试设计，以便管理好数据的状态。

* 测试的独立性（test isolation）：合理地组织测试，以便每个测试的数据只对该测试可见。
* 适应性测试（adaptive tests）：按如下方式进行测试设计——每次运行时先对数据环境进行检查，然后使用这些检查中得到的数据作为数据基础，对系统行为进行测试。
* 测试的顺序性（test sequencing）：按如下方式进行测试设计——按某种已知的序列运行，每个测试的输入依赖于前一个的输出。

###  测试独立性

​	测试独立性是指确保每个测试都具有原子性。也就是说，每个测试不应该用其他测试的结果建立它的初始状态，并且其他测试也不应该以任何形式影响该测试的成功或失败。对于提交测试（甚至那些将测试数据持久化到数据库中的测试）来说，达到这种独立性是相对容易的。

​	最简单的方法是确保在测试结束时，总是将数据库中的数据状态恢复到该测试运行之前的状态。可以用手工方法来做，但最简单的方法是依靠大多数RDMS提供的事务特性。

​	对于那些与数据库相关的测试来说，在测试刚开始时先创建一个事务，在这个事务内，执行所有我们所需的数据库操作与交互，然后在测试结束（无论该测试是成功还是失败）时，将该事务回滚。这是利用数据库系统的事务独立特性确保其他测试或数据库的其他用户看不到该测试对数据库做的这些修改。

### 建立和销毁

​	无论选择的策略是什么，在测试运行之前建立一个已知的状态良好的起始点，并且在其运行结束时再重建这个起始点是至关重要的，可以避免测试间依赖（cross-test dependency）。

### 连贯的测试场景

​	常常有这样一种倾向，即创建一个连贯的“故事”（将多个测试场景串在一起），让一些测试顺序执行。这种方法的出发点是已创建的数据是有连续性的，这样可以将测试用例的建立和销毁工作最小化。而且，每个测试本身也会简单一点儿，因为它不再负责管理自己的测试数据了。另外，作为一个整体，测试套件运行得更快，因为它不用花太多时间创建和销毁测试数据了。

## 数据管理和部署流水线

### 提交阶段的测试数据

​	在项目中，我们常常将用于创建通用数据结构测试实例的代码隔离开，并在不同的测试用例之间共享它们。可以用名为CustomerHelper或CustomerFixture的辅助类用于在测试用例中简化Customer对象的创建，这样就可以使用统一的方式为每个Customer创建一套标准的默认数据。然后每个测试再根据它自己的需要对数据进行裁剪。这样，测试起始点，就是一致且已知的状态了。

### 验收测试中的数据

​	与提交测试不同，验收测试是系统测试。这意味着，它们的测试数据必然会更复杂，如果你想避免测试变得非常笨重，需要更细心地管理这些测试数据。也就是说，其目标是尽可能减少测试对大型复杂数据结构的依赖。方法基本上与提交阶段的测试一样：我们希望在测试用例的创建方面做到一些重用，并将每个测试对测试数据的依赖最小化。我们应该创建恰好够用的数据，用来验证我们对系统的期望行为。

​	当考虑如何为某个验收测试准备应用程序的某个状态时，区分以下三类数据是非常有用的。

* 测试的专属数据（test-specific data）：那些在测试中用于驱动应用程序行为的数据。它代表了测试中这个用例的细节。
* 测试的引用数据（test reference data）：这类数据通常是附加的，与某个测试相关，但是并不真正与被测试的行为相关。测试中需要它，但只是对该测试的一个支持，而不是主角。
* 应用程序的引用数据（application reference data）：经常有一类数据，它们与被测试的行为无关，但是是应用程序运行所必需的。



​	然而，与提交测试不同，在测试准备时，我们不推荐使用应用程序代码或数据库转储（dump）的方式来为应用程序准备正确的初始状态。取而代之的是，为了保持测试的系统级本质，我们推荐利用应用程序提供的API将应用程序设定到一个正确的状态。

​	利用应用程序API有如下几个优点。

* 使用应用程序代码或其他绕过应用程序业务逻辑的机制可能会让系统处于某种不一致的状态。在验收测试阶段，利用应用程序的API可以确保应用程序绝对不会处于某种不一致状态。
* 根据重构的定义，对数据库或应用程序的重构不会影响验收测试，因为重构不改变应用程序公共API的行为。这使验收测试的脆弱性显著下降。
* 验收测试也相当于对应用程序的API做了测试。



### 容量测试的数据

​	容量测试用来指出应用程序所需的数据规模问题。该问题在两方面体现：（1）为测试提供足够的输入数据；（2）准备适当的引用数据来支撑测试中的多个用例。

​	我们倾向于使用像交互模板（详见9.6.3节）这样的机制自动生成大量数据，包括输入数据和引用数据。

### 其他测试阶段的数据

​	抛开具体的实现技术，至少从设计理念上来讲，在验收测试阶段之后的所有自动化测试阶段中，我们都可以使用同样的方法。我们的目标是重用那些自动化验收测试所用的“行为规范”作为其他测试（不仅限于功能性测试）的起点。

​	对于手工测试阶段（比如探索性测试或者用户验收测试），也有两种方法来测试数据。一种方法是拿一个测试和应用程序引用数据的最小集让应用程序从最原始的初始状态启动。然后，测试人员就可以体验用户刚开始使用应用程序时的场景了。另一种方法是加载大量的数据，以便让测试人员可以测试该应用程序已运行一段时间后的情景。对于集成测试来说，使用这种大数据集是非常有用的。

## 小结

本章中提到了如下一些重要原则与实践。

* 对数据库进行版本管理，使用DbDeploy这样的工具管理数据迁移过程的自动化。
* 努力保持数据库模式修改的向前和向后兼容性，以便把数据的部署和迁移问题与应用程序的部署问题分开。
* 确保在准备过程中，测试可以创建它们所依赖的数据，并确保数据是分开的，以保证不会影响那些同时运行的其他测试。
* 只保存不同测试之前应用程序启动所需要的测试数据，以及一些非常通用的引用数据。
* 尽可能使用应用程序的公共API为测试创建正确的初始状态。
* 在大多数据情况下，不要在测试中使用生产数据集的副本。创建自定义数据集既可以通过细心选择生产数据集的最小子集来实现，也可以通过运行验收测试和容量测试来实现。

# 组件和依赖管理

## 引言

​	在大型重构或添加复杂新功能时又怎么办呢？从版本控制库上拉一个新的分支看上去好像是解决这个问题的一个方案。但我们强烈感觉到这是错误的做法[插图]。本章将描述如何在不断变化的同时保持应用程序随时可发布。要解决这个问题，一项关键的技术就是大型应用程序的组件化。所以，本章会详细讨论组件化，包括使用多组件来创建并管理大型项目。

​	组件是什么？在软件领域，这个术语的使用呈现一种泛滥状态，所以在使用这个术语前，我们试着对其进行一个清晰的定义。当我们说起组件时，是指应用程序中的一个规模相当大的代码结构，它具有一套定义良好的API，而且可以被另一种实现方式代替。对于一个基于组件的软件系统来说，通常其代码库被分成多个相互分离的部分，每个部分通过个数有限的定义良好的接口提供一些服务行为，与其他组件进行有限的交互。

​	基于组件的设计通常被认为是一种良好的架构，具有松耦合性，是一种鼓励重用的设计。事实也确实如此。但它还有另外一个重要的好处：对于大型软件开发团队的协作来说，它是最有效的方法之一。本章也会描述如何为这种基于组件的应用程序创建和管理构建系统（build system）。

​	尽管很多项目其实只用一个版本控制代码库和一个简单的部署流水线就足够了，但最终还是陷入了无法维护的代码泥潭，因为在很容易做组件分离的阶段，却没人打算创建分离式的组件。然而小项目会逐渐变成大项目。项目一旦大到某种程度，以原来那种开发小项目的方式来修改代码，其成本就相当高了。但很少有项目经理会有胆量要求他的团队长时间停下来，将一个大应用程序重新架构成组件方式。而“如何创建并管理组件”就是本章所要讨论的主题。

## 保持应用程序可发布

​	在开发过程中，团队会不断地增加新特性，有时候还要做较大的架构改变。在这些活动期间，应用程序是不能发布的，尽管它能够成功通过持续集成的提交测试阶段。通常，在发布之前，团队会停止开发新功能，并进入一个只做缺陷修复的稳定期。当应用程序发布后，就会在版本控制中拉出一个发布分支，而新功能的开发仍会在主干上进行。可是，这个流程常常会导致两次发布的时间间隔是几个星期或几个月。而持续交付的目标是让应用程序总是保持在可发布状态。那么如何做到这一点呢？

​	一种方法是在版本控制库中创建分支，当工作完成后再合并，以便主干一直是可发布的（下一章将会详细讨论这种方法）。然而，我们认为这种方法只是一种次优选择，因为如果工作成果是在分支上，那么应用程序就不是持续集成的。相反，我们提倡每个人都应该提交到主干。可是，怎么既能让每个人都在主干上开发，又让应用程序一直保持在可发布状态呢？

​	为了在变更的同时还能保持应用程序的可发布，有如下四种应对策略。

* 将新功能隐蔽起来，直到它完成为止。
* 将所有的变更都变成一系列的增量式小修改，而且每次小的修改都是可发布的。
* 使用通过抽象来模拟分支（branch by abstraction）的方式对代码库进行大范围的变更。
* 使用组件，根据不同部分修改的频率对应用程序进行解耦。

### 将新功能隐蔽起来，直到它完成为止

​	有一种解决方案，就是把新功能直接放进主干，但对用户不可见。例如，某网站提供了旅行服务。运维这个网站的公司想提供一种新的服务：酒店预订。为了做到这一点，先把它作为一个单独的组件来开发，通过一个单独的URI“/hotel”来访问。如果愿意的话，这个组件就可以与系统的其他部分一起部署，但不允许访问其入口就行了（在Web服务器软件中，可以通过一个配置项来控制）。

### 所有修改都是增量式的

​	这个通用策略就是：让所有修改都是增量式完成的。当需要做较大改动时，拉分支并在分支上做修改的方式非常有诱惑力。其理论是：如果变动较大，则会破坏应用程序，那么，拉分支并完成修改后再把代码合并回去能够提高效率。然而事实上，最后阶段才将所有东西合并在一起往往是最困难的部分。假如其他团队同时也在主干上开发，最后的合并可能会更困难。而且，改动越大，合并的难度就越大。分支的理由越明显，就越不应该分支。

​	虽然将大的改动变成一系列小步增量修改是一个很困难的工作，但你坚持这么做的话，就意味着你正在解决一个问题：保持应用程序一直可工作，避免后期的痛苦。这也意味着，如果必要的话，可以随时停下当前的工作，从而避免“大修改刚做到一半，就不得不放弃它”而产生的巨大成本浪费。

### 通过抽象来模拟分支

​	对应用程序做大修改时，可以采用另一种替代分支方法的策略，即在要修改的那部分代码上创建一个抽象层。然后在当前实现方法存在的同时，开发一种新的实现方式。当完成时，再把原始的实现和抽象层（它是可选的）删除。

​	当应用程序的某个部分需要做改进，但却无法使用一系列小步增量开发时，就要按如下步骤这么做。

* 在需要修改的那部分系统代码上创建一个抽象层。
* 重构系统的其他部分，让它使用这个抽象层。
* 创建一种新的实现代码，在它完成之前不要将其作为产品代码的一部分。
* 更新抽象层，让它使用这个新的实现代码。
* 移除原来的实现代码。
* 如果不再需要抽象层了，就移除它。



​	“通过抽象来模拟分支”是一次性实现复杂修改或分支开发的替代方法。它让团队在持续集成的支撑下持续开发应用程序的同时替换其中的一大块代码，而且这一切都是在主干上完成的。如果代码库的某一部分需要修改，首先要找到这部分代码的入口（一个缝隙），然后放入一个抽象层，让这个抽象层代理对当前实现方式的调用。然后，开发新的实现方式。到底使用哪种实现方式由一个配置选项来决定，可以在部署时或者运行时对这个选项进行修改。

​	这种方法中最困难的两部分是：（1）将涉及修改的这部分代码库的入口点隔离；（2）管理这部分还在开发当中的功能的所有修改，比如需要修改这部分代码中的缺陷。然而，与使用分支方法相比，这些问题更容易管理。可是，有时候很难在应用程序代码库中找到一个合适的缝隙，那么就只能拉分支了，之后再在这个分支上使用“通过抽象来模拟分支”。

## 依赖

​	我们是这样来区分组件和库的。库是指团队除了选择权以外，没有控制权的那些软件包，它们通常很少更新。相反，组件是指应用程序所依赖的部分软件块，但它通常是由你自己的团队或你公司中的其他团队开发的。组件通常更新频繁。这种区别非常重要，因为当设计构建流程时，处理组件要比处理库所需考虑的事情多一些。

​	比如，你要一次性编译整个应用程序吗？还是当某个组件被修改时，只独立编译它就可以了？如何管理组件之间的依赖，才能避免循环依赖呢？

### 依赖地狱

​	赖管理最常见的问题可能就是所谓的“依赖地狱”（dependency hell），有时被称为“DLL地狱”（DLL hell）。当一个应用程序依赖于某个库的特定版本，但实际部署的是另一个版本，或者根本没有部署时，依赖地狱就产生了。

​	在微软的Windows早期版本中，DLL地狱是很常见的问题。所有以DLL方式存在的共享库（shared library）都保存在系统目录中（windows\system32），但是没有版本标识，新版本只是把旧版本覆盖掉了。除此之外，在XP之前的Windows版本中，COM类表（class table）是一个单体，所以那些需要某个特定COM对象的应用程序只能找到该COM对象被最先加载的那个版本。[插图]所有这些都意味着，在这种情况下，即使你明知不同的应用程序使用某个DLL的不同版本，甚至知道在运行时需要该DLL的哪个版本，你也无法办到。

​	Linux通过使用简单的命名规则来避免依赖地狱：在全局库目录（/usr/lib）中，每个．so文件的文件名后都会有一个整数，并用一个软链接来决定在系统范围内所使用的标准版本。对于管理员来说，他们很容易对应用程序所使用的版本进行修改。如果某个应用程序依赖于某个特定的版本，它就会请求对应的那个具体版本的文件。当然，如果某个库文件在整个系统范围内只有一个标准的指定版本，就可以确保安装的每个应用程序都能使用它。这个问题有两种答案：像Gentoo那样，从源文件开始编译每个应用程序，或者对每个应用程序的二进制包进行全面的回归测试——大多数Linux发布包的创建者喜欢这种方法。这就意味着，如果没有非常好的依赖管理工具的支持，你就无法随意安装一个依赖于系统库新版本的应用程序的二进制发布包。幸运的是，Debian包管理系统就是这类包管理工具（可能是现存最好的依赖管理工具）。这也是为什么Debian平台如此平稳，而Ubuntu每年能发布两个稳定版本的原因。

​	对于整个系统范围内的依赖问题，一个简单的解决方案就是审慎地使用静态编译。也就是说，应用程序中的那些关键依赖在编译时就放到一个程序集中，以便减少运行时依赖。然而，尽管这使部署更简单了，但它也有一些缺点。除了会创建较大的二进制包以外，它还和那些与操作系统特定版本中的特定二进制包耦合在一起，这样就不可能通过升级操作系统的方式来修复相关的缺陷或安全漏洞了。因此，通常不推荐使用静态编译。

​	由于类加载器的设计原因，Java的运行时依赖面临的问题尤其严重。最初的设计使得在同一个JVM上每个类只能有一个版本生效。OSGi框架解决了这种严格限制，它提供了多版本的类加载，以及热部署和自动升级。如果不使用OSGi的话，这种约束就会一直存在，也就是说，在构建时就要小心地管理依赖。一个常见却令人不爽的场景是：一个应用程序依赖于两个库文件（比如两个JAR包），而这两个库文件又都依赖于另外一个库（比如一个日志包），但它们所依赖的版本各不相同。此时，虽然这个应用程序可能在编译时没出问题，但运行时肯定会出问题，比如可能会抛出一个ClassNotFound异常（如果所需的方法或类不存在的话），或者出现一点儿小缺陷。这个问题被称作“菱形依赖问题”。

### 库管理

​	一种是将它们提交到版本控制库中，另一种是显式地声明它们，并使用像Maven或Ivy这样的工具从因特网上或者（最好）从你所在组织的公共库中下载。你所要强化的关键约束就是让构建具有可重复性，即每个人从版本库中签出项目代码，然后运行自动化构建，得到的二进制包一定是完全相同的；而且三个月后，当某个用户发现了旧版本中的一个缺陷时，为了修复它，我能够从版本库中签出那个版本，并重新创建一个与之完全相同的二进制包。

​	Maven和Ivy都提供了自动管理依赖的方法。这两个工具让你显式声明项目中所需库文件的确切版本，然后自动为你下载适当版本的库文件，解决对其他项目的依赖（如果可用的话），并确保在项目依赖图中没有不一致现象，比如两个组件依赖同一公共库的不兼容版本。这些工具会将项目所需的库文件缓存在本地，尽管在一台机器上第一次进行项目构建时，可能会花较长的时间，但如果所需的库文件已经在本地的话，以后的构建就不比将库文件放在版本库中的情况慢了。Maven存在的一个问题是：为了能够做到可重复构建，必须使用其插件的具体版本对其进行配置，并指定项目所依赖库文件的每个具体版本。本章后面还会更详细地介绍Maven的依赖管理。

​	如果Maven和Ivy不适用，还可以利用一个属性文件来记录项目中所依赖的库文件以及这些库文件的版本，从而打造一个属于自己的声明式依赖管理系统。然后再写一个脚本，从组织级的制品库中下载这些库文件的正确版本，只要在备份的文件系统前端架一个简单的Web服务就行了。

## 组件

### 如何将代码库分成多个组件

​	只有当其具有一定的复杂度之后才应该考虑将这部分代码作为应用程序的独立部分。那么，它的上界是什么？我们将一个系统分成多个组件的目标是提高整个团队的效率。那么，为什么说组件开发方式让软件开发流程更高效呢？原因如下。

* 它将问题分成更小且更达意的代码块。
* 组件常常表示出系统不同部分代码的变化率不同，并且有不同的生命周期。
* 它鼓励我们使用清晰的职责描述来设计并维护软件，反过来也限制了因修改产生的影响，并使理解和修改代码库变得更容易。
* 它给我们提供了额外的自由度来优化构建和部署过程。



​	大多数组件的一个显著特征是，它们会以某种方式公开其API。这些API的技术形式可能不同：动态链接、静态链接、Web服务、文件交换（file exchange）和消息交换（message exchange），等等。这些API可能有不同的特征，但重要的是，它与外部合作者交换信息，所以这些组件与外部合作者的耦合度是至关重要的。即使当组件的接口是一种文件格式或一个消息模式时，它仍旧代表了某种信息上的耦合，这种耦合也需要作为组件之间的依赖来考虑。

​	当在构建与部署流程中将这些组件分离并作为独立单位对待时，正是组件之间的接口和行为的耦合度增加了复杂性。

​	将组件从代码库中分离出来的理由如下。

* 代码库的一部分需要独立部署（比如一个服务器或富客户端）。
* 你打算将一个“铁板”系统分成一个内核和一系列的组件，以便用另一种实现代替当前系统中的某个部分，或者支持用户自扩展。
* 组件为其他系统提供一个接口（比如提供某个API的框架或服务）。
* 在开发环境中打开项目的时间太长。
* 对于一个团队来说，代码库太大了。



​	当团队人数在十个人左右，并且都能够从内到外了解代码库的某个特定部分（无论是功能组件还是其他某种边界）时，团队是处于最佳状态的。如果你希望一个超过十人的团队以你期望的速度开发的话，一个最有效的方法是将系统分成多个松耦合的组件，而且也把团队分开。

​	我们并不建议让每个团队各自负责一个独立的组件。因为在大多数情况下，需求不会按组件的边界来分。根据我们的经验，那些有能力开发端到端功能的跨功能团队更加高效。尽管一个团队负责一个组件看上去好像更高效，但事实并非如此。

​	最好划分多个团队，以便每个团队都可以拿到一系列的用户故事（这些故事可能属于同一主题）。为了完成这些需求，每个团队都可以修改任何组件的代码。一个团队为了实现某个业务特性可以自由修改任何组件是一种更高效的工作方式。依据功能领域而不是组件来组建团队确保了每个人都有权力修改代码库的任何部分，同时在团队之间定期交换人员，确保团队之间有良好的沟通。

​	这种方法还有一个好处，即确保所有的组件能组合在一起正常工作是所有人的责任，而不只是最后负责集成的那个团队的责任。“每个团队负责一个组件”这种工作方式有一个非常严重的风险，那就是整个应用程序到项目后期才能工作，因为没人愿意去集成这些组件。

​	最后，值得注意的是康威法则（Conway's Law），即“设计系统的组织不可避免地要产生与其组织的沟通结构一样的设计”。[插图]例如，开源项目的开发人员只通过电子邮件来交流，所以，项目代码更趋向于较少接口的模块化特点。由都坐在一起的小团队开发出来的产品更趋向于紧耦合、非模块化特点[插图]。请细心地考虑如何组建开发团队，因为它会影响应用程序的架构。

### 将组件流水线化

​	即使应用程序是由多个组件构成的，也并不是说一定要为每个组件实现各自的构建。最简单而且是很令人吃惊的方法就是整个应用程序只有一个构建流水线。每次提交修改时，就应该构建并测试整个应用。在大多数情况下，我们建议将整个软件系统作为一个整体来构建，除非是反馈过程太长。如前所述，假如你遵守了我们在本书中的建议，你会发现自己完全有能力用这种方法构建一个超大且复杂的系统。这种方法的优点是，很容易追踪到底是哪一行代码破坏了构建。

​	然而，在很多现实场景下，系统会受益于将其分成多个不同的构建流水线。下面是几个使用多构建流水线的情况。

* 应用程序的某些组成部分有不同的生命周期（比如作为应用程序的一部分，你构建自己的操作系统内核版本，但是几个星期才需要做一次）。
* 应用程序的几个功能领域由不同的（很可能是分布式的）团队负责，那么这些团队可能都会有自己的组件。
* 某些组件使用不同的技术或构建流程。
* 某些共享组件被不同的几个项目所用。
* 组件相对稳定，不需要频繁修改。
* 全面构建整个应用程序所花时间太长，但为每个组件创建一个构建会更快（需要注意的是，有必要这么做的时间点要比大多数人想的晚得多）。



​	从构建和部署流程的角度来说，一件重要的事情是，管理基于组件的构建总要有一些额外的开销。为了将单一构建分成几个构建，你要为每个组件创建一个构建系统。也就是说，每个部署流水线都可能需要新的目录结构和构建文件，而且它们要遵循整个系统的同一模式。这意味着，每个构建的目录结构都应该包含单元测试、验收测试及它们所依赖的库文件、构建脚本、配置信息和其他需要放到版本库中的东西。每个组件或者组件集合的构建都应该有自己的构建流水线来证明它满足发布条件。这个流水线会执行下列步骤。

* 编译代码（如果需要的话）。
* 组装一个或多个二进制文件，它们能够部署到任意环境[插图]中。
*  运行单元测试。
* 运行验收测试。
* 如果合适的话，还要进行手工测试。



​	一旦二进制包成功通过它自身的迷你发布流程，就可以晋升到集成构建了（详细内容请参见下一节）。你需要将这个二进制包（以及能标识该二进制包来源的版本信息元数据）放在到一个制品库中。尽管你可以自己做这事儿，只需将产生该二进制包的流水线标识作为目录名就能办到了，但是，现在的CI服务器可以替你完成这种事。另一种方式是使用Artifactory、Nexus或其他的制品库管理工具。

### 集成流水线

​	集成流水线的起点是：从所有组件流水线中得到组成该应用系统的二进制包。集成流水线的第一个阶段应该是将这些二进制文件组装在一起，创建一个（也许是多个[插图]）部署安装包。第二个阶段应该将其部署到一个类生产环境中，并在其上运行冒烟测试，快速验证是否有最基本的集成问题。如果这个阶段成功了，那么流水线就应该进入到常规的验收测试阶段，以通常的方式来运行整个应用的验收测试，图13-1是一个常见的流水线阶段图。

![](https://pic.imgdb.cn/item/60e41f305132923bf85043f0.jpg)

​	当创建集成流水线时，需要牢记部署流水线的两个通用原则：快速反馈和为所有相关角色提供构建状态可视化。如果流水线或流水线链太长的话，会让反馈时间变长。如果你恰好遇到了这种情况，并且你有足够多的硬件环境的话，一种解决方案是，在生成了二进制文件并通过单元测试之后就立即触发下游的流水线。

​	对于可视化而言，如果集成流水线的任何一个阶段失败了，都应该能够明确地看到它为什么失败。也就是说，能够从集成构建反向追踪到组成该构建的每个组件的具体版本，这是非常关键的。对于“能否发现到底是哪些源代码变更令某次构建失败”而言，维护这种关系就显得非常重要了。现代CI工具应该为你提供这种功能。如果它不能的话，你就应该再找一个可以做到这一点的CI工具。它应该在几分钟内就能追溯到集成流水线失败的原因。

​	当然，在与其他组件组成一个完整的应用程序时，并不是每个“独自构建成功的个体组件”都是好的。因此，开发某个组件的团队应该可以看到他们的组件到底哪个版本成功通过了集成流水线（此时才可以说这个集成是好的）。事实上，也只有这样版本的组件才能被认为是“好的”。集成流水线是每个个体组件流水线的扩展。所以双方向的可视化是非常重要的。

​	如果在集成流水线的两次构建之间，有多个组件发生了变化，该构建很可能就会失败。此时，问题就复杂了，因为很难发现到底是哪个组件的变更让构建失败了，自上次成功的构建后很可能做过很多修改。

​	有几种不同的技术来解决这个问题，接下来我们就来讨论一下。最简单的方法是每当任何一个组件构建成功后就触发集成流水线的构建。如果组件变化的频率不高，或者在构建集群中你有足够多的计算能力，你就可以这么做。这也是最佳方法，因为它不需要人工干预或聪明的算法，而且相对于人力资源的话，计算资源更便宜。所以，如果条件允许，你就这么做好了。

​	第二个最佳方法是对尽可能多的应用程序版本进行构建。你可以使用相对简单的算法，比如拿到每个组件的最近一个版本，尽可能频繁地将它们组装在一起。如果这种做法足够快的话，在每次组装后都能运行并完成一个较短的冒烟测试套件。假如冒烟测试套件的时间稍长，有可能就会跳过一个版本，在第三个版本上运行测试了。

## 管理依赖关系图

​	组件本身对其他组件也可能有依赖，比如第三方库文件。如果在组件之间画一个依赖关系图的话，它应该是一个DAG（Directed Acyclic Graph，有向无环图）。如果不是的话（尤其是图中有循环的），你就遇上了病态依赖关系了，后面会简单讨论一下。

### 构建依赖图

![](https://pic.imgdb.cn/item/60e422385132923bf85f1b3b.jpg)

​	每个组件都应该有自己的构建流水线，当其源代码被修改时或者上游依赖有变化时都应该触发它的构建流水线。当该组件成功通过它自己的所有自动化测试时，就应该触发下游依赖。在为这种状况下的组件构建依赖图时，需要考虑以下几种可能的场景。

（1）对投资组合管理系统做修改。此时，只有投资组合管理系统需要重新构建。

（2）对报告引擎做修改。此时，报告引擎必须重新构建，并通过它自己的所有自动化测试。然后，需要使用报告引擎的最新版本与报价及处理引擎的当前版本重新构建投资组合管理系统。

（3）对报价库做修改。CDS报价库是第三方的二进制依赖。所以，如果使用的CDS更新后，报价引擎需要使用这个新版本和框架的当前版本重新构建。然后再触发投资组合管理系统的重新构建。

（4）对框架做修改。如果对框架进行了成功修改，即框架流水线通过了所有测试，那么它的下游依赖应该立即进行重新构建：报告引擎、报价引擎和处理引擎。如果这三个依赖的构建都成功了，那么就要用这三个上游依赖的新版本对投资组合管理系统进行重新构建。如果这三个组件中的任何一个构建失败，投资组合管理系统就不应该重新构建，而且应该把这次框架的构建结果视为失败，并对框架进行修复，以便这三个下游组件能够通过它们各自的测试，最终让投资组合管理系统的流水线成功通过。

​	在这个例子中，有一个非常重要的点，那就是场景（4）。看上去，在投资组合管理系统的三个上游组件之间好像需要一种“与”的关系。然而，事实并不是这样的。如果报告引擎的源代码被修改了，就应该触发对投资组合管理系统的重新构建，无论报价引擎或处理引擎是否重新构建了。另外，考虑下面的场景。

​	在这个例子中，有一个非常重要的点，那就是场景（4）。看上去，在投资组合管理系统的三个上游组件之间好像需要一种“与”的关系。然而，事实并不是这样的。如果报告引擎的源代码被修改了，就应该触发对投资组合管理系统的重新构建，无论报价引擎或处理引擎是否重新构建了。另外，考虑下面的场景。

### 为依赖图建立流水线

​	![](https://pic.imgdb.cn/item/60e4230e5132923bf862feba.jpg)

​	有几个非常重要的特性。首先，为了增加反馈的速度，一旦任何一个项目部署流水线的提交阶段完成了，就要触发下游的项目，并不需要等待验收测试全部通过，只要下游项目所需的二进制文件已经产生就行了。这些二进制文件产生之后就被放到了制品库中。当然，后续的验收测试和各种部署阶段会重用这些二进制文件。

​	除了向手工测试环境和生产环境部署以外，其他的触发都是自动的，因为这两个环境通常需要手工授权操作。这些自动化触发使任何变更（比如对框架）都会触发报价引擎、处理引擎和报告引擎的构建。如果这三个组件都使用新版本的框架构建成功了，投资组合管理系统就会用上游所有组件的新版本进行重新构建。

​	队要能够追踪在应用程序的某个具体版本中每个组件的源是什么，这一点是非常关键的。一个好的持续集成工具不仅可以做到这一点，还应该能够展示它是由哪些组件的哪个版本集成在一起的。比如在图13-4中，可以看到投资组合管理系统的V2.0.63是由报价引擎的V1.0.217和处理引擎的V2.0.11，以及报告引擎的V1.5.5和框架的V1.3.2396组成的。

![](https://pic.imgdb.cn/item/60e423975132923bf865808b.jpg)

![](https://pic.imgdb.cn/item/60e423a25132923bf865b275.jpg)

​	持续集成工具还要确保在每个流水线实例中，从前到后每个组件所有的版本都是一致的。它应该防止“依赖地狱”这样的事，并确保当版本控制库中的某次变更影响到多个组件时，它只能通过流水线传播（propatage）一次。

​	在本章开始时给出的关于增量式开发的建议对组件也同样适用。要以增量方式进行修改，且不要破坏依赖。当增加新功能时，在被修改的组件中为它提供一个新的API入口。如果不想支持旧的功能，就把静态分析放在部署流水线中，用来检查哪个组件还在使用旧的API。流水线应该很快就会告诉你，某次修改是否不小心破坏了某个依赖。

​	如果你要对某个组件做一个深远的变更，那么可以为它创建一个新的发布版本。在图13-6中，我们假设开发报告引擎的团队需要创建一个新版本，会破坏一些API。为了做到这一点，他们为其创建了一个分支V1.0，并在主干上开发V1.1。

![](https://pic.imgdb.cn/item/60e423f95132923bf8673cfe.jpg)

​	报价引擎团队会在主干上不断增加新功能。与此同时，报价引擎的下游用户仍旧可以用1.0分支上创建的二进制文件。如果需要修复缺陷，可以提交到1.0分支上，再合并回主干。一旦下游用户准备好使用新版本了，就可以切换过来。需要澄清的是，“按发布创建分支”模式仍旧需要承担推迟集成的后果。因此，就持续集成而言，它是次优选择。然而，如果组件（至少应该）是松耦合的，那么延迟集成的痛苦会更加可控一些。所以，当管理组件出现更复杂的变化时，这是一个非常有用的策略。

### 什么时候要触发构建

​	上面的所有讨论都有一个假设，即只要上游依赖发生变更就触发一次新的构建。这是正确的做法，但在很多团队中这并不是标准做法。相反，他们更倾向于在代码库稳定后（比如集成阶段或开发到达某个里程碑时），才更新它们的依赖。这种行为强调稳定，但潜在风险成本是在集成时花更多的时间。

​	由此可以看出，在开发过程中涉及依赖的地方都会存在一种张力。一方面，最好是保持与上游依赖最新的版本一致，以确保得到最新的功能和已修复的缺陷。而另一方面，集成每个依赖的最新版本会有一定的成本，因为要花时间来修复这些新版本带来的破坏。大多数团队会妥协，当更新的风险比较低时，在每次发布之后才更新所有依赖。

​	当决定更新依赖的频率时，一个关键的考虑是：对这些依赖的新版本的信任度有多高。如果你所依赖的组件也是你的团队自己开发的，通常你能快速且简单地修复由于API变更引起的问题，这样，频繁集成是最好的。如果组件足够小，最好就让整个应用只有一个构建——这样才反馈最快。

​	假如上游依赖组件是由你所在公司的其他团队开发的，那么最好这些组件有它们各自的流水线。然后，你可以再判断并决定是使用上游依赖组件每次变更后的最新版本，还是仍旧使用某个具体版本。这个决定既依赖于它们变化的频率，又依赖于上游团队解决问题的速度。

​	你对组件变更的掌控性、可视性和影响力越少，你对它的信任就越少，你接受新版本时就越保守。如果没有明显需求，就不要更新第三方库。如果那些变更并没解决你遇到的问题，就不要更新，除非供应商不再为你所用版本提供技术支持了。

​	在大多数情况下，团队最好持续集成依赖组件的新版本。当然，持续更新所有依赖组件，成本会更高一些，比如花费到集成（包括硬件和构建）上的资源以及修复bug或集成“未完成”版本引入的一些问题所带来的消耗。

​	你要在“应用程序集成时是否得到了快速反馈”和“很多你不关心的失败构建不断地打扰你”之间寻找一个平衡点。一个可能的解决方案是Alex Chaffee的一篇文章[d6tguh]提到的 “谨慎乐观主义”。

### 谨慎乐观主义

​	Chaffee的建议是，在依赖图中引入一些新的触发类型，即某个上游依赖的触发类型包括 “静止”（static）、“慎用”（guarded）和“活跃”（fluid）。“静止类型”的上游依赖发生变化后，不会触发新的构建。“活跃类型”的上游依赖发生变化后就一定会触发新的构建。如果某个“活跃类型”的上游依赖发生变更后触发了构建，并且这次构建失败了，那么就把上游依赖标记为“慎用类型”，并且将这个上游依赖的上一次成功的组件版本被标记为“好版本”。“慎用类型”的上游依赖可以按“静态类型”那样来使用，即它不会再拿上游依赖的最新修改[插图]，而这个状态的目的是提醒开发团队，这个上游依赖需要解决某个问题。

![](https://pic.imgdb.cn/item/60e424ed5132923bf86bb0ed.jpg)

​	然而，“谨慎乐观主义”可能导致复杂的行为。比如，将框架和报价引擎之间的触发类型设置为“活跃类型”，与CDS报价库一样。当CDS报价库和框架都更新以后，报价引擎就会有一次构建。如果报价引擎失败了，你就不知道到底是哪个依赖使这次构建失败了。是新版本的CDS报价库呢？还是框架的新版本呢？你就需要不断试验，找出到底是哪个。与此同时，这两个触发类型都变成了“慎用类型”。Chaffee提到，可以使用一种称作“告知悲观主义”（informed pessimism）的策略作为实现依赖追溯算法的起点。在这种策略中，每个触发都被设为“静止”的，但当上游依赖的某个新版本有效时，下游组件的开发团队会得到相应的通知。

### 循环依赖

​	令人惊奇的是，我们的确看到过这么做而且成功了的项目，在它的构建系统中的确存在循环依赖。你可能会对我们这里所说的“成功”提出质疑，但这些代码的确可以在生产环境中正常工作，对我们来说，这就足够了。关键在于，不要在项目一开始时就有循环依赖，这会导致蔓延。要是用组件A的某个版本来构建组件B，再用这个组件B再反过来构建组件A的一个新版本，这是完全可以做到的。但是，如果能够避免的话，我们还是建议不要这么做。因为这会导致一种“构建阶梯”，如图13-8所示。

![](https://pic.imgdb.cn/item/60e4256b5132923bf86e0164.jpg)

​	如前所述，我们不建议使用循环依赖。但是，如果你已经陷入其中，并且无法避免的话，那么上面所说的策略还是可行的。目前还没有哪种构建系统直接支持这种配置方式，所以你要自己打造一个工具来支持这种情况。另外，你还要非常小心地处理构建过程中各部分的交互关系：如果每个组件都自动触发依赖于它的组件的构建，由于是个死循环，所以这两个组件就会一直不断地做构建，不会停下来。一定要尽量避免循环依赖，但假如你发现你正在使用的代码库中有循环依赖，也别气馁，在完全解决这个问题之前，可以把“构建阶梯”作为一个临时解决方案。

## 管理二进制包

​	你不必自己管理制品库。目前市场上有几种相关的产品，包括开源项目Artifactory和Nexus。另外还有几种工具，比如AntHill Pro和Go就有它们自己的制品库。

### 制品库是如何运作的

​	为什么要删除二进制产物呢？因为这些产物很大（即使现在不大，将来也会变得很大）。考虑到存储空间，你最终也需要删除它们。因此，我们不建议将产物提交到版本控制库中。如果能重新生成它们，就不需要它们。当然，将已通过所有测试的这些产物和待发布的候选版本保存下来是非常值得的。已经发布过的东西也值得保存，因为可能会回滚到前面的版本，或都需要对使旧版本的用户提供一些技术支持。

​	无论这些产物能保存多长时间，都应该一直保存每个产物的散列值，以便可以验证生成二进制包的源代码是否正确。对于审计来说，这是非常重要的。比如，当你不确定某个具体环境中到底部署了哪个版本的应用程序时，可以使用该版本的MD5码找出版本库中对应的版本。你既可以使用构建系统来保存数据（一些持续集成服务器提供了这个功能），也可以使用版本控制系统。但无论怎样，管理散列码是配置管理策略的一部分。

### 部署流水线如何与制品库相结合

​	实现部署流水线需要做两件事：一是将构建过程中的产物放到制品库里；二是以后需要时能把它取出来。	

​	设想一下，某个流水线包括以下阶段：编译、单元测试、自动化验收测试、手工验收测试和生产环境部署。

* 编译阶段会创建需要放到制品库的二进制文件。
* 单元测试和验收测试阶段会取出这些二进制文件，并在其上运行单元测试和验收测试[插图]，将生成的测试报告放到制品库中，以便开发人员查看结果。
* 用户验收测试阶段是将二进制文件部署到UAT环境中，用于手工测试。
* 发布阶段是取出二进制文件，将其发布给用户或部署到生产环境中。

## 用Maven管理依赖

​	如前所述，项目中有两种依赖：外部库依赖（参见13.3.2节），以及应用程序的组件间依赖。Maven提供一种抽象，可以用同一种方式处理这两种依赖。所有的Maven领域对象，比如项目、依赖和插件，都能由一组元素来标识，它们是groupId、artifactId和version（有时这个三元组简称为GAV）。这三个元素唯一标识一个对象，还有packaging。一般以下面的格式书写，在Buildr中也是这么声明的：

![](https://pic.imgdb.cn/item/60e4f7265132923bf8f21bcf.jpg)

​	有时你并不想每次运行mvn install后都覆盖上次运行的产物。为了做到这一点，Maven提供了快照构建（snapshot build）的概念。只要在版本后面增加了-SNAPSHOT的后缀（在上面的例子中，它就是l.0.0-SNAPSHOT）。当运行mvninstall时，在该版本号的目录中，Maven会创建名为version-yyyymmdd-hhmmss-n的目录。需要这个快照的项目就可以仅指定l.0.0-SNAPSHOT（而不是完整的时间戳）取到本地库中的最新版本。

​	然而，使用快照时，你要当心点儿，因为这种方式让重现构建（reproducingbuild）更难了。较好的办法是让持续集成服务器生成一个权威版本，并把构建号作为该产物版本号的一部分，然后把它放到组织级的中央制品库里。然后，你就可以使用pom文件中Maven的版本设定方式来指定可使用的版本范围。如果的确需要在本地机器上做一些探索性的工作，那么可以临时修改一下本地的pom文件来使用快照方式。

​	Maven有一个非常有用的特性，就是它能分析项目中的依赖，并告诉你哪些是未清晰定义的依赖，哪些是没有用的依赖。只要运行命令mvn dependency:analyze即可得到这个报告。

## 小结

略

# 版本控制进阶

## 引言

​	分支的理由有三种。第一，为了发布应用程序的一个新版本，需要创建一个分支。这使开发人员能够不断开发新功能，而不会影响稳定地对外发布版本。当发现bug时，首先在相应的对外发布分支上修复，然后再把补丁合并到主干上。而该发布分支从来不会合并回主干。第二，当需要调研一个新功能或做一次重构时——调研分支最终会被丢弃并且从来不会被合并回主干。第三，当需要对应用程序做比较大的修改，但又无法使用上一章所描述的办法来避免分支时，也可以使用生命周期较短的分支（其实，如果代码基结构良好的话，这种情况也很少见）。分支的唯一目的就是可以对代码进行增量式或“通过抽象来模拟分支”方式的修改。

## 版本控制的历史

### CVS

​	CVS为版本控制和软件开发过程带来了很多创新。其中最重要的是，CVS默认不锁文件（即“并发”）。事实上，这也是开发CVS的主要动机。

​	虽然CVS有创新，但它也有很多问题（如下所述），其中有一些问题是由它从RCS中继承过来的单个文件变更追踪系统造成的。

* CVS的分支操作是将每个文件复制到该代码库的一个新副本里。如果代码库比较大的话，这可能会花较长时间，而且占用很多磁盘空间。
* 由于所有分支实际上都是通过复制方式得到的，所以做分支合并时，会引起很多奇怪的冲突，而且在一个分支上，刚刚新增的文件是无法自动合并到另一个分支上的。虽然有一些解决方法，但很耗时且易出错，总之非常不爽。
* 在CVS中打标签时，需要修改代码库中的每个文件，因此对于较大的代码库来说，这是另一个比较耗时的过程。
* CVS的提交操作并不是原子操作。也就是说，如果提交过程因故被打断的话，代码库将处于一种不确定状态。同样，如果两个人同时提交代码，两个人的提交可能是交错进行的。这让人很难看到谁到底做了哪些改动，回滚操作也很难做。
* 文件重命名不是一级操作：不得不先删除旧文件，然后再增加一个新的，这样就失去了该文件的版本历史。
* 建立和维护一个代码库是一项很艰难的工作。
* CVS的二进制文件是一个整块，所以无法对二进制文件做变更管理，因此，磁盘利用方式非常低效。

### SVN

​	SVN代码库模型最重要的特性之一就是，版本号是针对整个代码库的，而不是每个文件对应一个版本号。不必再说“某个文件从修订版本1变化到修订版本2”。相反，当代码库从修订版本1改变到修订版本2时，你可能想知道的是某个文件到底做了哪些修改。SVN用对待文件的方式对待目录、文件属性和元数据。也就是说，对这些内容的修改，其版本控制方式与对文件修改的版本控制方式相同。

缺点：

* 只能在线提交变更。这看似简单，但是分布式版本控制工作的主要优点之一就是对修改进行提交操作与将修改送到另一个代码库的操作分开。
* SVN在本地客户端用于跟踪变更的数据被保存在代码库每个文件夹的．svn目录中。可以将本地的不同目录更新到不同的修订版本上，甚至可以更新到不同的标签版本或分支版本上。尽管有时我们希望这么做，但是它很容易导致混乱，甚至错误。
* 尽管服务器端的操作是原子操作，但客户端的操作却不是。如果客户端的更新被打断，工作目录可能就会处于一种不确定状态。通常情况下，这很容易修复，但有时就不得不删除整个子目录，再重新签出一次。
* 修订号（revision number）在指定的代码库中是唯一的，但不同的代码库之间，这个修订号不是唯一的。比如，由于某种原则，一个代码库被分成了几个较小的代码库，新代码库中的修订号就与原代码库的修订号一点儿关系都没有。尽管这听起来是个小事情，但也意味着SVN的代码库无法支持那些DVCS的某些特性。

### 商业版本控制系统

​	Perforce。超好的性能、可扩展性和完美的工具支持。一些真正的大型软件开发组织都在使用Perforce。

* Perforce。超好的性能、可扩展性和完美的工具支持。一些真正的大型软件开发组织都在使用Perforce。
* AccuRev。它提供了像ClearCase那样进行基于流的开发能力，却没有ClearCase那么大的管理开销和那么差的性能。
* BitKeeper。第一个真正的DVCS，也是唯一的一个商业工具。

### 放弃悲观锁

​	如果版本控制系统支持乐观锁（即编辑本地工作副本的一个文件时，不会阻止别人在他们自己的工作区对其进行修改），那么就应该使用它。悲观锁是指，为了编辑某个文件，必须申请一个额外的锁，这看上去是阻止合并冲突的好方法。然而，事实上，它降低了开发流程的效率，尤其是在大型团队中。

## 分支与合并

团队使用分支有如下几种原因

* 物理上：因系统物理配置而分支，即为了文件、组件和子系统而分支。
* 功能上：因系统功能配置而分支，即为特性、逻辑修改、缺陷修复和功能增加，以及其他可交付的功能（比如补丁、发布或产品等）而分支。
* 环境上：因系统运行环境而分支，即由于构建和运行时平台[编译器、开窗口系统（windowing system）、库、硬件或操作系统等]的不同而分支或为整个平台而分支。
* 组织上：因团队的工作量而分支，即为活动/任务、子项目、角色和群组而分支。
* 流程上：因团队的工作行为而分支，即为支持不同的规章政策、流程和状态而分支。

### 合并

​	两次合并之间的间隔时间越长，在每个分支上工作的人越多，那么合并时的麻烦就越多。主要有如下两种办法可将这种痛苦最小化。

* 创建更多的分支来减少在每个分支上的修改。例如，可以每次开发新功能时就创建一个分支，这是“早分支”（early branching）的一个例子。然而，这也意味着需要更多的工作来跟踪所有的分支，更多合并的痛苦只是被向后推迟了而已。
* 很谨慎地创建分支，可能是每个发布才创建一个分支。这是“推迟分支”（deferred branching）的又一个例子。为了尽量减少合并的痛苦，就要经常做合并，这样在合并时就没有那么麻烦了。但你要有规律地进行，比如每天都做。

### 分支、流和持续集成

![](https://pic.imgdb.cn/item/60ec11655132923bf8065fd2.jpg)

​	在这个例子中，是为同一个应用程序创建了多个项目，并且每个项目对应一个分支。向主干（或者图14-1中所指的“集成分支”）合并活动相当不规律，而且当合并时，很容易对主干的应用程序造成破坏。因此，主干上的应用程序可能总是处于无法工作的状态，直到进入发布前的“集成阶段”，它才会被修复。

​	一个更可控的分支策略（我们强烈推荐的，可以说是业界标准）是：只为发布创建长周期的分支，如图14-2所示。

![](https://pic.imgdb.cn/item/60ec11a35132923bf807666b.jpg)

## DVCS

### 什么是DVCS

​	DVCS背后的根本性设计原则是，每个使用者在自己的计算机上都有一个自包含的一等（first-class）代码库，不需要一个专属的“主”代码库，尽管根据惯例，大多数团队都会指定一个（否则的话，持续集成就做不了了）。从这一设计原则出发，引入了很多有意思的特性，如下所述。

* 在几秒钟内就能开始使用DVCS了，即只要安装一下，并将修改提交到本地代码库就行了。
* 可以单独从别人那里拿到他们的最新更新，却不需要他们将其修改提交到中央代码库。
* 可以将自己的修改推送到一组人的代码库中，而不需要他们每个人自己来拿你的修改。
* 可以将自己的修改推送到一组人的代码库中，而不需要他们每个人自己来拿你的修改。
* 补丁可以通过网络用户更高效地传播，这让接受或拒绝个别补丁变得很容易，即被叫做“摘樱桃”（cherry-picking）的实践。
* 当没有联网的时候，也可以对修改的代码进行版本控制。
* 可以频繁地提交未完成的功能到本地代码库作为阶段检查点，而不会影响到其他人。
* 在将修改发送给其他人之前，可以很容易地在本地对这些提交进行修改，重排它们的顺序或将多次提交打包成一个，这种操作叫做“rebasing”。
* 很容易用本地代码库来尝试各种各样的解决方案或想法，而不需要在中央代码库创建一个分支。
* 由于能在本地把多次提交打包，所以就不需要经常修改中央代码库，这让DVCS具有更好的扩展性。
* 在本地建立和同步多个代理库很容易，因此可以提供更高的可用性。
* 因为全量代码库有很多份副本，所以DVCS有更好的容错性，尽管主代码库仍旧应该做备份。

![](https://pic.imgdb.cn/item/60ec12645132923bf80ab388.jpg)

### DVCS简史

略

### 企业环境中的DVCS

​	对于“在公司中使用DVCS”这件事来说，除了通常的“保守”原因以外，还有以下三个明显的反对意见。

* 集中式版本控制系统在用户的计算机中只保存唯一的一个版本，而DVCS则不同，只要有本地代码库的副本，就可以得到它的完整历史。
* 在DVCS的王国中，审核与工作流是更加不可靠的概念。集中式版本控制系统要求使用者将其所有的变更都提交到一个中央代码库。DVCS允许用户彼此交换变更集，甚至允许修改本地代码库中的历史，而不必让中央系统来跟踪这些修改。
* Git的确允许修改历史。而这在受制度监管的企业环境中可能就触及了“警戒底线”，而为了记录所有的内容，就要定期地对代码库进行备份。

### 使用DVCS

​	DVCS与集中式版本控制系统的主要区别在于，代码提交到本地的代码库中，相当于你自己的分支，而不是中央服务器中。为了与别人共享你的修改，需要执行一些额外的步骤。DVCS有两种新的操作：（1）从远程代码库把代码取回到本地库；（2）将本地修改推送到远程代码库。

## 基于流的版本控制系统

### 什么是基于流的版本控制系统

​	基于流的版本控制系统（比如ClearCase和AccuRev）可以把一系列修改一次性应用到多个分支上，从而减少合并时的麻烦。在这种“流”方式上，分支被更强大的概念“流”所代替。其中，最大的区别在于，流之间是可以相互继承的。所以，如果你把某次修改应用到一个指定的流上，它的所有子孙流都会继承那些修改。

​	想想这种方式对下面两种状况有什么样的帮助：（1）将某个缺陷的修复补丁应用到软件的多个版本上，（2）向代码基中添加第三方库的新版本。

​	当发布中有长生命周期分支时，第一种情况就很常见。假如在某个发布分支上做了一次缺陷修复，如何将这次修复同时应用到其他所有代码分支上呢？没有基于流的工具，答案是手工合并它。这是一个令人厌烦且易出错的工作，尤其是当有多个分支需要合并这次修改时。在使用基于流的版本控制后，只要将这次修改补丁合并到需要它的所有分支的共同祖先分支上即可。这些分支就会得到该补丁并更新，再触发一次包含该补丁的新构建。

![](https://pic.imgdb.cn/item/60ec32f65132923bf891db4e.jpg)

### 使用流的开发模型

​	基于流的版本控制系统鼓励开发人员在自己的工作区中进行开发。这样，开发人员可以在不影响其他人的情况下执行重构，试验不同的解决方案，并且开发新功能。当做好以后，他们就可以晋级这些修改，使其对其他人可见。

​	当然，现实生活中的事情不会这么简单。功能之间完全独立是不现实的，尤其是在团队遵循“需要做重构时就要不遗余力地做好重构”这种原则下，当把一大堆重构后的代码晋级到其他流上时，就时常会发生代码合并的问题。因此，当下列情况发生时，遇上集成问题是不足为奇的。

* 复杂的合并，由于不同的团队用不同的方式修改了共享代码。
* 依赖管理问题，某个新功能依赖于尚未被晋级的其他功能代码。
* 集成问题，比如，因为代码使用了一种新的配置，所以令集成和回归测试在发布流上失败了。

### 静态视图和动态视图

​	ClearCase有一个特性，叫做“动态视图”（dynamic view）。当某个文件合并到某个祖先流上时，它会立即更新对应的子孙流上工作的每个开发人员的视图。而在更传统一些的静态视图中，直到开发人员决定更新时，才会看到相应的修改。

### 使用基于流的版本控制系统做持续集成

​	基于流的版本控制系统的卖点之一就是：开发人员很容易在自己的私有流上工作，并且还承诺，之后再做合并也很容易。然而，在我们看来，这种方法有一个根本性的缺点：当代码修改被频繁向上晋级（比如每天多于一次）时，不会有什么问题，但是，如此频繁地晋级也会令这种方法所鼓吹的好处大打折扣。如果晋级频繁，较简单的解决方案没有什么问题，甚至效果更好。如果没做频繁晋级，那么当要发布版本时，很可能会遇到一些麻烦。因为你不知道会花费多长时间才能搞定所有的事情，例如将每个人认为应该可以工作的功能集成在一起，修复那些由于复杂的合并而引入的bug。可这些正是持续集成应该解决的问题。

## 主干开发

​	在这种模式中，开发人员几乎总是签入代码到主干，而使用分支的情况极少。主干开发有如下三个好处。

* 确保所有的代码被持续集成。
* 确保开发人员及时获得他人的修改。
* 避免项目后期的“合并地狱”和“集成地狱”。



​	主干开发的一个结果就是：每次向主干签入并不都是可发布状态。如果你使用分支方式做特性开发，或者使用基于流的开发通过多级直至发布级别来晋级变更，那么这可能看上去是对主干开发实践的一个“击倒性”反驳。如果每次都晋级到主干，那么如何管理一个有很多开发人员，且有多个版本发布的大型团队呢？这个问题的答案是：软件需要良好的组件化、增量式开发和特性隐藏（feature hiding）。这要求在架构和开发中更加细心，而它的收益是：不需要设定一个无法预期的较长的集成阶段将多个流合并到一起创建一个可发布的分支，因为这些工作的精力远比花在架构和开发上要多得多。

**不用分支也可以做复杂的修改**

​	当你想对代码基进行某种非常复杂的修改时，通常会创建一个分支，然后在该分支上进行修改，从而避免打断其他开发人员的工作，这么做看起来是最简单的方式。然而，事实上，这种方法会导致多个长生命周期的分支，与主干产生很多的代码分歧。每到发布时，分支合并几乎总是最复杂的过程，无法预期会花费多长时间。每次新的合并总会破坏某些原有功能，所以，做下一次合并之前，还要有个过程先让主干稳定下来。

## 按发布创建分支

​	有一种情况，“创建分支”是可以接受的，那就是在某个版本即将发布之前。一旦创建了这个分支，该发布版本的测试和验证全部在该分支上进行，而最新的开发工作仍旧在主干上进行。

​	为了发布而创建分支取代了“冻结代码”这种邪恶的做法，即几天内不许向版本控制库签入代码，有时甚至是几个星期。通过创建发布分支，开发人员仍旧可以向主干签入代码，而在发布分支上只做严重缺陷的修复。为了发布创建分支如图14-2所示。

在这种模式中，要遵循如下规则。

* 一直在主干上开发新功能。
* 当待发布版本的所有功能都完成了，且希望继续开发新功能时才创建一个分支。
* 在分支上只允许提交那些修复严重缺陷的代码，并且这些修改必须立即合并回主干。
* 当执行实际的发布时，这个分支可以选择性地打一个标签（如果版本控制系统仅支持文件级别的跟踪机制，比如CVS、StarTeam或ClearCase，那么打标签就是必须的操作了）。



​	这种分支方式在真正的大项目中效果并不太好，因为很难让一个大型团队或多个团队在同一个版本上同时完成他们所有的工作。在这种情况下，理想的方法是有一个组件化的架构，每个组件都有一个发布分支，以便在其他团队还在开发组件时，该团队可以在创建分支后继续在他们的组件上开发新的功能。如果做不到这一点，请参见本章的“按团队分支”模式，看看是否更可行。如果你想做到“可以挑选特性”的话，请参见“按功能特性分支”模式。

## 按功能特性分支

​	这种模式是为了让开发团队更容易在“特性”层次上并行工作，并保持主干的可发布状态。每个用户故事或特性在不同的分支上开发完成。一个故事只有通过测试人员验证无问题后，才会被合并到主干，以确保主干一直是可发布的。

要想让这种模式有效果，就要有如下一些前提条件。

* 每天都要把主干上的所有变更合并到每个分支上。
* 每个特性分支都应该是短生命周期的，理想情况下应该只有几天，绝对不能超过一个迭代周期。
* 活跃分支的数量在任意时刻都应该少于或等于正在开发当中的用户故事的数量。除非已经把开发的用户故事合并回主干，否则谁都不能创建新分支。
* 在合并回主干之前，该用户故事应该已经由测试人员验收通过了。只有验收通过的用户故事才能合并回主干。
* 重构必须即时合并，从而将合并冲突最小化。这个约束非常重要，但也可能非常痛苦，进而限制了这种模式的使用。
* 技术负责人的一部分职责就是保证主干的可发布状态。他应该检查所有的合并（可以通过查看补丁的方式进行检查）。他有权拒绝可能破坏主干代码的补丁。



​	DVCS就是为这种模式设计的，而且使它很容易与主干进行双向合并，并根据其Head[插图]创建补丁。DVCS让使用者能非常容易地创建一个分支代码库，并在其中增加特性，再开放给另一个提交者。这令那些使用GitHub的开源项目在开发速度方面收益颇丰。开源项目的一些关键特征使它们更适合这种模式，如下所述。

* 尽管可以有很多人向开源项目做贡献，但仅有一个由经验丰富的开发者组成的相对较小的团队来管理，他们对接受或拒绝补丁有最终的决定权。
* 发布日期相对灵活，这使得开源项目的提交者在拒绝次优的补丁方面有一定的回旋余地。尽管这对于商业产品来说也是适用的，但它并不是准则。



​	所以，在开源世界里，这种模式可能非常有效。它也适用于那些核心团队很小且由经验丰富者组成的商业项目中。它在大型项目中也能发挥作用，但需要应用下面这些条件：（1）代码基被合理分解成多个模块；（2）交付团队被分成几个小团队，每个团队都由一个有经验的开发者领导；（3）整个团队承诺频繁地向提交主干签入并集成；（4）交付团队不会屈从于交付压力而导致未达标准的发布决策。

​	值得强调的是，按特性分支的确与持续集成是对立的，我们关于“如何使这种模式能够工作起来”的所有建议只是为了确保在合并时情况不至于太糟糕。如果能在源头避免痛苦，那会更简单一些。当然，就像软件开发中的所有“原则”一样，也会有一些特例情况出现，比如像开源项目或使用DVCS且由丰富经验的开发者组成的小团队。可是，需要提醒你的是，当采纳这种模式时，就是在“刀尖上跳舞”（Runing with Scissors）。

## 按团队分支

​	这种模式试图解决如下状况：在一个大型团队里，有很多开发人员同时工作在多个工作单元流上，并且还要维持主干总是处于可发布状态。与按功能特性分支一样，这种模式的主要意图是确保主干一直是可发布的。为每个团队创建一个分支，并且只有当该分支稳定后才将其合并回主干。每次合并后，其他分支都要立即将这次变更与自己合并在一起。如图14-7所示。

![](https://pic.imgdb.cn/item/60ec36fc5132923bf8a3d956.jpg)

下面是按团队分支的工作流程[插图]。

* 创建多个小团队，每个团队自己都有对应的分支。
* 一旦某个特性或用户故事完成了，就让该分支稳定下来，并合并回主干。
* 每天都将主干上的变更合并到每个分支上。
* 对于每个分支，每次签入后都要运行单元和验收测试。
* 每次一个分支合并回主干时，在主干上都要运行所有的测试（包括集成测试）。



​	从持续集成的角度来说，这种策略有一些缺点。一个根本问题就是，这种策略上的工作单元是一个分支，而不是一次特定的修改。换句话说，无法将一次修改单独合并到主干，而只能将整个分支合并回去。否则无法知道是否破坏了主干上的规则。如果在合并之后，团队又发现了一个缺陷，而此时这个分支上又包含了其他修改的话，就不能只将这次修复合并回主干，团队要么让这个分支再次稳定下来，要么仅为这次修改创建另一个分支。

​	在这些问题里，有些问题可以利用DVCS来缓解。Linux内核开发团队使用了这种模式的一个变种，为操作系统的不同部分保持逻辑上的分支（比如调度程度和网络栈），也就是独立的代码库。DVCS能够从一个代码库上挑选一些变更发送给另一个代码库，这个过程叫做摘樱桃（cherry-picking）。也就是说，与其总是要合并整个分支，不如只合并想要的那些特性。现代的DVCS还有完善的rebasing工具，因此可以将补丁放在之前的变更集里，将它们捆绑在一起。因此，如果在补丁中发现了一个bug，那么完全可以为这个补丁再增加一个缺陷修复，并在部署流水线上运行该版本，在验证它没有破坏主干功能后，再合并这个新增的补丁。使用DVCS后，将这个模式从“不推荐使用”转为“在某种情况下”可以使用，因为团队可以做到有规律地合并到主干了。

## 小结

略。

# 持续交付管理

## 引言

​	本书的主要读者对象是实践者（practitioner）。然而，实现持续交付不仅仅是买些工具，做一些自动化的工作。它依赖于交付过程中所涉及的每个人的协作，来自行政管理层的支持，以及基层人员的改进意愿。

​	尽管在业务方面，每个人最终都是为了一个共同的目标，但在执行度和符合度方面经常会成为互相冲突的力量。这一点已经从有尽快交付压力的开发团队与把任何变更都看作风险的运维团队之间的关系看出来了。

​	我们认为，在组织中的这两部分人不要再进行零和博弈[插图]了。其实，执行度和符合度都可以满足。这一道理在持续交付中也是正确的。通过确保交付团队能得到应用程序在类生产环境上的不断反馈，是部署流水线达成“执行度”这个目标的方法和手段。

## 配置与发布管理成熟度模型

![](https://pic.imgdb.cn/item/60ecfe0f5132923bf856000a.jpg)

​	这个模型的最终目标是组织改进，想得到的结果如下。

* 缩短生产周期，以便能更快地为组织交付价值，增加利润。
* 减少缺陷，以便可以提高效率，在技术支持工作上花更少的成本。
* 提高软件交付生命周期的可预测性，让计划更有效。
*  具有采用和遵守任何必要的法律规章的能力。
* 具备有效发现和管理软件交付相关风险的能力。
* 通过更好的风险管理和交付更少缺陷的软件来减少成本。



​	我们相信，这个成熟度模型可以作为一个指南，帮助你收获所有这些产出。一如既往，我们推荐你使用戴明环，即计划-执行-检查-处理。

（1）使用这个模型来分析确定你所在组织的配置和发布管理模式。你会发现，组织中的不同部门在不同维度上处于不同的级别。

（2）选择一个领域集中发力，该领域是你的薄弱环节，你的痛点所在。价值流分析方法（value stream mapping）会帮你识别在组织中对哪个领域进行改进最有意义。本书会帮助你理解每个改进会带来什么，以及如何实施。你应该决定哪项改进对组织有意义，评估它的成本和收益，并排定优先级。你应该定义一些验收条件来将期望的结果具体化，以及这些验收条件如何度量。这样，才能确定这种变化是否能够成功。

（3）实施变革。首先，创建一个实施计划。可能最常见的方法就是先验证一下。比如，先选择组织中真正感到痛苦的那部分人，这些人会有强烈的动机去实施这种变革，而你将会看到最大的变化。

（4）一旦发生了变化，使用之前创建的验收条件来衡量这些变化是否达到了预期的效果。组织所有的干系人和参与者开一个回顾会议，找出这些变化是否够好，以及对哪个潜在领域还可以进行改进。

（5）重复上述步骤，积累知识。做增量式的改进，并将它推广到整个组织中。

## 项目生命周期

​	团队组建与磨合常常会经历五个阶段：创建期（forming）、风暴期（storming）、规范期（norming）、运转期（performing）和调整/重组期（mourning/reforming）[插图]。同样，软件也会经过几个阶段。初步可包含以下阶段：识别阶段（identification）、启动阶段（inception）、初始阶段（initiation）、开发和部署阶段（development and deployment）及运维阶段（operation）。在详细解释构建和部署工程如何融入这个蓝图之前，我们先简单地讲一下这些阶段。

​	ITIL（Information Technology Infrastructure Library，信息技术基础设施库）为软件服务交付提供了一个框架，它与本书中我们描述的交付方法相兼容。二者都是通过让IT部门成为业务的一个战略资产，从而提高向客户交付的价值。与ITIL一样，本书的方法也关注于有效用的、对目标适用的，以及有保证的、有用的服务，我们也在讨论满足明确定义的功能和非功能需求。

### 识别阶段

​	没有商务分析，需求收集工作就很难做，同时也无法客观地排列需求优先级（这对企业内部服务也是一样的）。即使做了，可能最后开发出来的应用程序或服务也与你在最初需求收集时所想的解决方案相差很大。

​	在开始需求收集之前，还有一样东西要准备好，即利益干系人列表，其中最重要的是业务主要负责人（business sponsor，在PRINCE2[插图]是高级责任人）。每个项目应该只有一个业务主要负责人。否则，在项目还没有完成之前，就会在政治内讧中失败。这个业务负责人在Scrum[插图]中叫做Product Owner，在其他敏捷形式中就是指客户。然而，除了业务负责人以外，每个项目都需要一个由该项目所涉及部门的成员组成的督导委员会——在一个公司中，这会包括其他高管和使用该服务的用户代表，而对于一个产品来说，可能是产品的高级主管或客户代表。IT项目的其他内部干系人包括运维、销售、市场和技术支持人员，以及开发和测试团队。这些干系人都要参与项目的下一个阶段——启动阶段。

### 启动阶段

​	这一阶段有很多种交付物。根据方法论和项目类型的不同，这些交付物也会有差异。然而，大多数启动阶段会有下列产出。

* 商务分析报告，包括该项目的价值评估。
* 概括性的功能与非功能需求列表（包括容量要求、可用性要求、服务连续性要求和安全性要求），需求的详细程度足以估算工作量和做项目计划即可。
* 发布计划，其中包括工作时间安排表和与项目相关的成本。为了得到这个信息，通常会评估需求的相对大小，所需的编码工作量，以及每个需求相关的风险和所需人力资源计划。
* 测试策略。
* 发布策略（详见后续内容）。
* 架构评估报告，决定使用什么样的平台和框架。
* 风险和问题列表。
* 开发生命周期的描述。
* 执行上述内容的计划描述。



​	这些交付物应包括足以启动项目的细节以及最多几个月后需交付的目标，如果可能的话，短点儿更好。根据我们的经验，合理的最长周期是三到六个月（倾向于三个月的期限）。在启动阶段结束前，应该根据对项目的估计收益、预计成本和已预期的风险等因素，给出该项目是否需要继续进行的明确决定。

​	该阶段中最重要的部分（也是决定项目成功概率的部分）是让所有项目干系人在一起面对面的工作，包括开发人员、客户、运营人员和管理层。这些人之间的对话才是真正的交付物，因为这会让所有人对需要解决的问题域，以及解决问题的方法有一个共同的理解。上面的清单用于设计和引导这种对话，以便能够讨论重要的问题，识别风险，并制定风险对策。

### 初始阶段

​	在启动阶段之后，就应该建立初始的项目基础设施了。这个初始阶段一般需要一周到两周的时间。下面是该阶段中的典型活动。

* 确保团队（分析师、经理和开发人员）可以得到开发所需的所有软硬件。
* 确保基本的基础设施都准备好了，比如因特网连接、白板、笔和纸、打印机、食物和饮料等。
* 创建电子邮件账号，为大家指派访问各类资源的权限。
* 建立好版本控制库。
* 建立一个基本的持续集成环境。
* 在角色、职责、工作时间和会议时间（比如站立会议、计划会议和演示会）上达成一致。
* 为第一周做准备工作，并在目标（不是指“最后期限”）上达成一致。
* 创建一个简单的测试环境和测试数据。
* 稍微更详细地研究一下预定的系统设计：在这一阶段探究它的可行性是真正的目标。
* 做一些调研（为了验证对某个具体需求的设计而做的实现，将来会被扔掉），识别和缓解任何分析、开发和测试风险。
* 开发用户故事或需求的待办列表（Backlog）
* 创建项目结构，使用最简单的用户故事（与hello world差不多），包括一个构建脚本和一些测试，从而验证持续集成环境可以正常工作。

### 开发与发布

​	当然，我们推荐以迭代增量式过程进行软件的开发与发布。不适用这种方式的唯一情况就是那种大型且涉及多个相关方的国防项目。然而，即便是太空船软件，也是利用迭代过程实现的[插图]。虽然很多人都认同迭代过程能带来益处，但是，我们经常看到的事实是，团队声称自己做的是迭代开发，但实际上完全不是那么回事。因此，有必要重申一下，我们认为下列信息是至关重要的，而且是迭代过程的最基本要求。

* 软件应该一直处于可工作状态，因为每次签入代码时，都会运行自动化测试套件，包括单元测试、组件测试以及端到端的验收测试。
* 每个迭代都能将软件部署到一个类生产环境中，并向用户演示（这样可以确保整个过程不但是迭代式的，而且是增量式的）。
* 迭代长度不超过两周。

使用迭代过程的理由有如下几个。

*  假如按业务价值来排功能优先级，你会发现，远在项目结束之前，软件就已经生效了。当然，即使我们已经完成了一些功能，也经常有一些很好的理由不发布新软件。然而，除了让人使用上可以工作的软件这种方式以外，没有哪种方式更能将人们对项目成功的担心转化成对看到新功能的兴奋。
* 可以从客户或者出资人那里得到关于软件的反馈，比如什么需求被满足了，什么需求还需要澄清或修改。这也意味着，正在开发的功能的确是有价值的。可在项目开始时，没有人知道，他们真正想要的是什么。
* 只有客户签收（sign off）后，需求才真正算完成了。而定期给客户做软件演示是跟踪进度唯一的可靠方式。
* 保持软件随时可工作（因为你不得不做演示），这会让团队更有纪律性，以避免集成阶段时间太长、破坏原有功能的重构和失去焦点及方向感。
* 可能最重要的是，强调每次迭代结束后都能得到可部署到生产环境的代码。在软件项目中，这是唯一真正有用的过程度量项，也只有迭代方法能提供这种方式。



​	“不做迭代开发”的一个常见理由就是：在很多功能没有完成之前，整个项目是无法交付价值的。虽然对于很多项目来说，这种限制可能是很现实的，但是，上面列表中的最后一项正好与这种情况相对应。当管理没有使用迭代开发方法的大型项目时，所有关于项目进展的度量都是主观的，根本没有办法确认项目的真正进度。在非迭代方法中看到的那些漂亮图表都是基于所剩时间的估算和对后续集成、部署和测试阶段中风险与成本的猜测。迭代开发提供的是项目进展情况的客观度量，它是用开发团队能够供给用户可工作的软件，并且该软件完成了多少被用户认可，满足用户目标的功能来衡量项目进度的。只有已准备好能够部署到生产环境的代码，那些你可以与之交互的代码（即便只是在一个UAT环境），才能保证指定的功能是真正完成了的。

​	迭代开发过程的关键在于划分优先级和并行化。工作应该被划分优先级，以便分析人员开始分析最有价值的特性，然后把工作交给开发人员，之后是测试人员，最后是给真正的用户或其代表做演示。利用来自精益制造的一些技术，这一工作可以并行进行，而且可以对每个任务上的工作人数进行调整，以便移除瓶颈。这样就会形成一个非常高效的开发过程。

​	进行迭代增量式开发有很多种方式。最流行方法之一就是Scrum，它是一种敏捷开发过程。Scrum在很多项目上成功了，但我们也看到过它失败。而失败的原因主要有如下三个。

* 缺乏承诺。向Scrum转变的过程是一个很容易出乱子的过程，尤其对于项目的领导力来说。确保每个人都能对项目的进度进行定期的面对面讨论，建立定期的回顾会议来分析项目运行情况，并寻找改进点。敏捷过程依赖于透明性、协作性、纪律性和持续改进。在实施敏捷过程中，会突然释放出一些有用信息，将原来隐蔽起来不便得到的真象推到聚光灯下。关键在于，要认识到这些问题本来就一直存在。现在只是暴露出来，让你知道了，这样就可以解决它们了。
* 忽视好的工程实践。Martin Fowler等人曾描绘过，假如Scrum执行者忽略了技术实践（比如测试驱动开发、重构和持续集成[99QFUz]），会出现什么情况。如果代码基被缺乏经验的初级开发人员搞坏了，任何开发过程都无法自动修复它。
* 将敏捷开发过程进行适应性调整，直到这个过程不再敏捷了。有些人对敏捷过程进行剪裁，令它成为他们认为能更好地适应他们组织需要的过程是很常见的现象。毕竟，敏捷过程就是要被裁剪的，以满足个别项目的需要。然而，敏捷过程中的各种要素常常以微妙的方式互相作用，很容易误解其价值所在，尤其是对于那些没有迭代过程实践背景的人来说，更是如此。所以，开始时应该坚信书中所写的都是正确的，并遵循书中所写的这个流程。在这一点上，怎么强调都不算过分。当你看到它是如何发挥作用以后，才能开始对其进行裁剪，以适应你的组织。



​	对诺基亚公司来说，这最后一点给它们带来了很多麻烦。因此，它建立了一个检查清单，用于评估它的团队是否在真正使用Scrum。这个检查分成如下两部分。

* 你在做迭代开发吗？
  * 迭代周期必须少于四周，而且要固定时长[插图]。
  * 在每个迭代结束时，软件的功能必须被测试完成，并能够正常工作。
  * 在规格说明书写完之前，迭代必须开始。
* 你在使用Scrum吗？
  * 你知道谁是Product Owner吗？
  * 产品待办列表是按业务优先级排列的吗？
  * 产品待办列表是由团队估算的吗？
  * 项目经理或其他人是否干扰了团队的工作？



### 运营阶段

​	一般来说，第一次发布不可能成为最后一次发布。接下来会发生什么，很大程度取决于项目本身。开发和发布过程可能会一直全速持续下去，也可能团队规模会减小。如果该项目原来是一个试验型项目，可能会发生相反的情况，团队规模会变大。

## 风险管理流程

风险管理是一个过程，它确保：

* 项目的主要风险已经被识别；
* 已有适当的缓解策略对这些风险进行管控；
* 在整个项目过程中，持续识别和管理风险。

风险管理过程应该有如下几个关键特征。

* 一个项目团队汇报项目状态的标准结构。
* 项目团队依赖标准，定期更新进度状态。
* 有一个信息展示板，让程序经理（program manager）可以跟踪当前状态，并查看所有项目的趋势。
* 项目外的人员定期对项目进行审计，确保风险被有效地管理起来了。

### 风险管理基础篇

​	风险管理的一个常见模型（参见Tom DeMarco和TimothyLister写的Dancing with Bears）是根据风险的影响（如果一旦发生，它们会引起多少损失）以及其可能性（风险有多大的可能成为事实）对风险进行分类的。二者结合在一起来评估每个风险的严重程度。从经济方面来考虑影响是最容易的：如果这个风险成为现实，会损失多少钱？然后再将该风险的可能性指定为0（不可能）到1（必然）之间的某个数值。把严重性产生的影响（就是损失的金钱）与可能性相乘，得到的金钱数就是对风险严重性的估值。这样，可以通过非常简单的计算决定使用什么策略来缓解风险：缓解策略的成本是否高于风险的严重度？如果回答是肯定的，那么缓解策略可能就没有必要实施了。

### 风险管理时间轴

**1．启动阶段结束时**

​	在这一阶段的最后，有两种重要的交付物。首先就是发布策略，它是在该阶段创建的。你应该验证我们在关于“创建发布策略”一节（10.2节）讨论过的所有方面都被考虑到了。如果没有考虑到的话，团队怎么做计划来管理这些相关的风险呢？

​	第二种交付物是对初始阶段的计划。有时候在启动和初始阶段间会有一段空闲期。此时，该计划在初始阶段的前几天做完就行。否则的话，初始阶段结束时就应该完成这个计划的制订。

**2．初始阶段结束时**

​	这里的关键是确保团队已经准备好开始开发软件了。持续集成服务器应该已经运行了，并能够编译代码和运行自动化测试套件。而且，还应该有一个类生产环境，可以用于产品代码的部署。用于描述应用程序的功能和非功能（尤其是容量）需求是如何通过部署流水线中的自动化测试套件的测试策略也应该到位了。

**3．开发和发布风险的缓解**

​	即便做了最充分的准备工作，开发和部署阶段也会有很多方式可能走向错误的一端，有时可能比你想的还要快。我们曾经历或听说过一些关于直到部署日期之后才交付代码，或者刚部署就由于容量问题而失败的项目。在整个阶段中，你要不断问自己一个问题：“有什么会出错？”假如你没有问自己这个问题，当事情发生时，你就会不知所措。

​	在该阶段，你的目标是识别、跟踪和管理所有你认为可以被管理的风险。有如下几种方法来识别风险。

* 查看部署计划。
* 每次演示之后都做一下简短的回顾会议。在这个会议上，让团队对项目风险进行头脑风暴。
* 让风险识别成为每日立会的一部分。

### 如何做风险管理的练习

​	分析任何项目，最好是从下面这些问题出发（对我们来说，这个列表在多个项目里都非常有效）。

* 如何跟踪项目进度？
* 如何防止缺陷？
* 如何发现缺陷？
* 如何跟踪缺陷？
* 怎么知道一个用户故事做完了？
*  如何管理环境？
* 如何管理配置项，比如测试用例、部署脚本、环境和应用程序配置、数据库脚本和外部库？
* 演示可工作功能的频率是怎样的？
* 做回顾会议的频率是怎样的？
* 运行自动化测试的频率是怎样的？
* 如何部署软件？
* 如何构建软件？
* 对运营团队来说，如何确保发布计划是可行的且可接受的？
* 如何确保风险问题列表是及时更新的？

## 常见的交付问题、症状和原因

### 不频繁的或充满缺陷的部署

**1．问题**

花很长时间部署某个构建版本，而且部署过程很脆弱。

**2．症状**

* 测试人员花很长时间才能将缺陷记录关闭。注意，这个症状可能并不是完全由不频繁的部署导致的，但它是可能的原因之一。
* 对用户故事的测试或者被客户验收需要花很长时间。
* 测试人员正在找的bug是开发人员很长时间之前修复的。
* 没有人信任UAT、性能或持续集成环境，当某个版本发布将要发布时，人们仍旧表示怀疑。
* 很少做演示。
*  应用程序很少被证明是可以工作的。
* 团队的速率（进度）比预期的慢。

**3．可能的原因**

​	有很多种可能的原因。下面是最常见的一些原因。

* 部署过程是非自动化的。
* 没有足够的硬件。
* 硬件和操作系统的配置没有被正确地管理。
* 部署过程依赖于团队无法掌控的系统。
* 没有足够多的人员理解构建和部署过程。
* 测试人员、开发人员、分析人员和运营人员在开发期间没有充分协作。
* 开发人员没有遵守纪律，通过小步增量方式的修改保证应用程序一直处于可工作状态，因此经常破坏原有功能。

### 较差的应用程序质量

**1．问题**

交付团队无法实施有效的测试策略。

**2．症状**

*  总出现回归bug。
* 缺陷数量持续增长，即使团队花很多时间修复它们（当然，这个症状只是表明你是否有一个有效的测试过程）。
* 客户抱怨产品质量低。
* 无论什么时候接到一个新的功能需求，开发人员都抱怨，看上去很害怕。
* 开发人员总是抱怨代码的可维护性，但却一直没有变好。
* 实现新功能的时间逐渐变长，并且团队进度开始落后。

**3．可能的原因**

​	本质上来说，这个问题有两个源头：测试人员与交付团队的其他成员的协作不畅以及自动化测试写得很差，或者不充分。

* 在特性的开发期间，测试人员没有与开发人员协作。
* 用户故事或特性被标记为“完成”，但没有写全面的自动化测试，也没有测试人员的验收，或者没有在类生产环境上给用户演示。
* 没有立刻修复已发现的缺陷，也没有写自动化测试用来检测回归问题，而是直接放到了待办列表中。
* 开发人员和测试人员在自动化测试套件开发方面没有足够的经验。
* 对于所用的技术或平台，团队并不了解写哪种类型的测试最有效。
* 没有足够的测试覆盖率，开发人员工作时无防护网，可能是因为他们的项目管理者没有给他们预留实现自动化测试的时间。
* 系统只是个会被放弃的原型（虽然我们遇到过好几个原来被当做会被放弃的原型开发而后来被直接当成重要的生产系统的事情）。

### 缺乏管理的持续集成工作流程

**1．问题**

​	不适当的构建过程管理。

**2．症状**

* 开发人员的签入不够频繁（应该至少一天一次）。
* 提交阶段总是处在失败状态。
* 缺陷的数量一直保持在较高水平。
* 在每次发布之前都有一个较长时间的集成阶段。

**3．可能的原因**

* 自动化测试运行时间太长。
* 提交阶段运行时间太长（理想情况下应该少于五分钟，超过十分钟是无法接受的）。
* 自动化测试有间歇性失败，还是误报。
* 没人得到许可就回滚别人的提交。
* 没有足够多的人理解持续集成过程，也没有足够的人做出改变。

### 较差的配置管理

**1．问题**

​	环境不是专属的，应用程序无法用自动化过程可靠安装。

**2．症状**

* 生产环境中总是有些莫名其妙的故障。
* 每次新版本部署都是紧张且令人担心的事情。
* 一个较大的团队专门对环境进行配置和管理。
* 部署到生产环境中的版本常常不得不回滚或打补丁。
* 生产环境中无法接受的当机时间。

**3．可能的原因**

* UAT和生产环境有差异。
* 没有对生产环境或试运行环境的变更管理流程，或者变更管理流程很差。
* 在运营、数据管理团队和交付团队之间协作不畅，沟通不充分。
* 对生产环境和试运行环境中的缺陷事件的监管有效性不足。
* 应用程序中的指南和日志不充分。
* 对应用程序非功能需求的测试不充分。



​	我们既没有篇幅也不想调查每个国家中各行业经常变化的那些规则。然而，我们要花一些时间讨论一般性的规则，尤其是那些在软件发布流程方面定义了严格控制的环境。许多这样的监管制度需要审计线索，让我们能够确定在生产环境中代码的每次修改来自哪里，谁碰过它们，以及在个流程中谁批准了哪些步骤。从金融业到健康医疗等很多行业中，常常会有这样的要求。

下面有一些常见的策略用来执行这类要求。

*  指定谁能够访问“特权”环境。
* 为特权环境中的修改制定并维护一个有效且高效的变更管理流程。
* 在执行部署之前，需要管理层的批准。
* 从构建到发布，每个过程都要文档记录。
* 创建一些授权的限制，以确保开发软件的人不能向生产环境部署，作为对潜在恶意干预的一种防护。
* 要求每次部署都要进行审计，以确切知道到底修改了哪些内容。

###  文档自动化

​	文档还有个问题，就是很容易过时。一个文档越详细，就可能越快过时。当这的确成为事实时，人们通常就不愿意再去更新它了。每个人可能都听过下面这样的对话。

​	自动化能解决这些问题。自动化脚本就是一份必须遵守的部署流程文档。强制使用这些脚本，你就要确保它们是时时更新的，并且部署流程是完全按照你指定的方式执行的。

### 加强可跟踪性

​	我们通常需要能够跟踪变更的历史，从生产环境中部署过哪些版本，到这些版本在源代码库中的版本号。我们想强调的是，有如下两种做法对这个过程有帮助。

* 二进制包仅创建一次，并且将在构建过程的第一个阶段产生的这个二进制包放到生产环境。同时，生成该二进制包的散列码（比如使用MD5或SHA1），用来确保拿到的是同一个二进制包，并将它们存在一个安全的数据库中。很多工具可以自动完成这件事。
* 使用全自动化的过程进行二进制包的部署、测试和发布流程，并自动记录谁在什么时间做了什么。目前市场上有几个工具可以做到这一点。



​	即使有了这些预防措施，还有一个地方有可能引入非授权的变更：当第一次用源代码创建二进制包的时候。比如，有权限登录到那台用于编译打包的机器上的人在编译打包过程中可能会做一些操作。解决这个问题的一个方法是在严格受控的机器上使用自动化过程一步创建二进制包。此时，关键在于能够自动地准备和管理这个环境，以便在创建过程中能够调试所有可能的问题。

### 在筒仓中工作

​	一些监管制度令我们很难建立这种跨功能团队。假如你是在一个壁垒更多的组织中，本书所描述的过程和技术，尤其是部署流水线的实现，会有助于防止这种孤立部门令交付过程低效的现象。然而，最重要的解决方法是：在项目一开始就让各部门进行沟通。这应该有以下几种形式。

* 每个参与到项目交付中的人，包括来自于各独立部门的人，都应该在每个项目开始时先碰一面。我们把这组人叫做发布工作组（release workinggroup），因为他们的工作就是保持发布流程一直正常运转。他们的任务应该是为项目建立一个发布策略，如第10章所述。
* 发布工作组应该在整个项目过程中定期开会，对过去工作做一次回顾，计划一下如何改进并执行计划。使用戴明环：计划、做、检查、改进（即PDCA）。
* 即使还没有用户，软件也应该尽可能频繁（至少每次迭代一次）地发布到类生产环境中。有些团队做持续部署，即每次修改通过部署流水线的所有阶段之后即发布。这里使用了一个原则“如果做一件事令你很痛苦，就更频繁地做这件事。”无论怎么强调这个实践的重要性都不算过分。
* 项目状态，包括15.4节提到过的信息指示板，应该对参与整个过程（包括构建、部署、测试和发布）的所有人都是可见的，可以让这些信息显示在每个人都能看到的一台大显示器上。

### 变更管理

​	在一个规范的环境中，对于构建、部署、测试和发布过程中的某些环境需要审批，这常常是必要的。尤其是，手工测试环境、试运行环境和生产环境总是在严格的访问控制之下，以便确保只能通过组织制定的变更管理流程对它们进行修改。这看上去像是不必要的官僚作风，但是实际研究表明，使用这种做法的组织中，其MTBF（Mean Time Between Failures，平均失败时间）和MTTR（mean time torepair，平均修复时间）更短（参见The Visible Ops Handbook第13页）。

​	如果你所在的组织因为对测试和生产环境进行未受控的变更令服务受到了影响，我们建议遵循下面的流程来管理审核事项。

* 由来自于开发团队、运营团队、安全团队、变更管理团队和业务部门的代表组成一个变更顾问委员会（Change Advisory Board，简称CAB）。
* 确定哪些环境属于变更管理流程控制的范围。确保这些环境都受到了访问控制，以便所有变更只能通过这个流程才能生效。
* 建立一个自动化变更管理系统，用来提出变更申请和管理审批。任何人都应该能够看到每个变更请求的状态以及由哪个人批准的。
* 无论任何人在任何时间想要对某个环境做修改，都必须通过变更请求来完成，比如，要部署某个应用程序的一个新版本，要创建一个新的虚拟环境或修改配置等。
* 每次变更都需要有一个补救策略，比如能够去除变更影响。
* 为每次变更的成功与否定义验收条件。理想情况下，可以创建一个自动化测试来验证。一旦变更成功，这个对应的自动化测试就会成功通过。在显示测试状态的运营管理信息展示板上设置一个对应的显示项（参见11.9.4节）。
* 使用一个自动化的过程来实施变更，以便某个变更无论何时被批准，都能够通过单击一个按钮就执行（或者一个链接也行）。



​	最后一点听上去有点儿难，但我们希望到目前为止，这听起来已经非常熟悉了，因为这一直是本书的一个重点。向生产环境部署被审计和授权的某个变更所使用的机制，应该与向其他环境部署相同变更完全相同，只是具体的授权不同而已——向部署流水线中增加访问控制非常方便，小事儿一桩。正是由于简单方便，以至于常常被扩大审计和授权的范围——所有变更都需要所属环境的所有者审批同意。可以使用为测试环境所创建的同样的自动化流程来管理受变更管理过程控制的环境。这样一来，也就顺便测试了所创建的自动化流程。

​	最后，当实现和管理一个变更审核流程时，还需要遵守如下三个原则。

* 对系统进行度量，并让其结果可见。一个变更需要多长时间才能被批准？有多少个变更正在等待审批？被回绝的变更比例有多大？
* 保持验证系统成功的度量项，并将其可视化。MTBF和MTTR是多少？一次变更的周期是多长？在ITIL中有一个更完整的度量项列表。
* 邀请各部门的代表，对系统进行定期回顾，基于这些回顾会议中的反馈对系统进行改进。

## 小结

略



# Jenkins 2.x实践指南-Start

​	https://jenkins.io

# 关于软件工程生产力

## 从另一个角度看"提高软件工程生产力"

​	如果将软件工程看成软件的生产过程，软件工程师是这个生产过程中的一种劳动者，知识是这个生产过程中的劳动对象，我们就会发现，这就是马克思的生产力理论三要素。

​	生产力三要素是劳动力、劳动资料、劳动对象，其中劳动资料和劳动对象构成生产资料。

​	生产力三要素分别指的是什么呢？

**劳动力**：一般意义，指工作人群，通常指在一家公司、各个产业乃至某个社会工作的人，多指体力劳动者，但通常不包括雇佣者（老板）和管理层。

**劳动资料**：也称劳动手段，是在劳动过程中所运用的物质资料或物质条件。

**劳动对象**：是指劳动本身所作用的客体，比如耕作的土地、纺织的棉花等。

​	在软件工程领域，生产力三要素又分别指的是什么呢？

**劳动力**：通常将软件开发工程师、测试工程师认为是劳动力。然而，当他们不在工作状态时，就不能称其为劳动力，只能称为劳动者

**劳动资料**：严格意义来说，办公场所、座椅、生产工具等都被称为劳动资料。本书主要讨论的是生产工具。笔者从硬件、软件的角度对生产工具进行了分类。****

* 硬件：开发时使用的电脑、机械键盘、灵敏的鼠标、网络速度等。
* 软件：IDE（如Eclipse、IntelliJ IDEA）、构建工具（如Webpack、Maven）、协作工具（如Jira）、部署工具（如Ansible、Puppet）等。

**劳动对象**：不像制造汽车，在开发软件时，劳动对象则是看不见、摸不着的知识。笔者将软件工程中的知识分为业务知识和技术知识。

### 从劳动力要素考虑提高软件工程生产力

​	如果能招到比一般程序员生产力高10倍的程序员，并好好利用，就可以提高生产力。如果这个程序员的生产力比一般程序员高10倍，那么通常意味着其工资也高10倍。

​	另外，不论招到什么样的程序员，管理者都要关心的是，如何帮助劳动者达到最佳工作状态，以产出更多的劳动力。不在工作状态，就不能称之为劳动力，只能称为劳动者。也许，那些经常随意打断程序员的管理者需要反思一下了。

### 从劳动对象要素考虑提高软件工程生产力

​	如果将软件生产过程看成是无形的知识具化成有形软件的过程，那么产品经理需要将想法（一种知识）具化成原型，美工和交互设计师理解产品经理的想法后，将自己的想法具化成设计稿，然后再将自己的理解及想法（又是一种知识）传达给前端开发人员。接着，前端开发人员和后端开发人员又沟通接口的设计（还是一种知识）……可以看出，要提高软件工程生产力，知识的流通效率起着很关键的作用。所谓知识的流通效率，指的是让知识从一个人的大脑流动到另一个（群）人的大脑的准确性和速度。

​	所以说，沟通能力在软件工程领域十分重要。

### 从生产工具要素考虑提高软件工程生产力

​	程序员笑话一则：程序员在椅子上打斗，经理叫他们回去，其中一位说：正在编译呢！

​	经理回答：哦，那你们继续。

​	我们算算账。假如一个20 000元/月工资的程序员，工作22天，每天8小时，那么每小时就是113.6元。假如程序员每天因为打开程序慢、网络慢、编译慢等而等待的时间总和为0.5小时，那么这0.5小时就属于浪费的，总共约57元。这意味着一个月会浪费1254元。

### 生产力三要素的意义

​	从生产力三要素的角度看，你要问平均编译时间是多久、为什么这么久，进而从三个要素发问：

* 生产工具：是电脑太慢了？是编译工具本身太慢了？
* 劳动力（程序员的能力）：是构建逻辑写得不合理？是编译过程中的某个阶段的问题影响了整体编译速度？
* 劳动对象：是不是缺少对当前构建工具（技术知识）的了解？

## Jenkins介绍

​	Jenkins是一款使用Java语言开发的开源的自动化服务器。我们通过界面或Jenkinsfile告诉它执行什么任务，何时执行。理论上，我们可以让它执行任何任务，但是通常只应用于持续集成和持续交付。

​	从生产力三要素来看，Jenkins属于劳动资料要素下的生产工具。

​	使用Jenkins能提升软件工程生产力的根本原因就在于它提供的是一个自动化平台。一个团队引入了Jenkins就像原来手工作坊式的工厂引入了生产流水线。由于知识的特殊性，它还能帮助我们将知识固化到自动化流水线中，在一定程度上解决了知识被人带走的问题。

​	我们使用Jenkins的过程，有如设计软件生产流水线的过程。

## Jenkins与DevOps

​	在行业内，DevOps的标杆Amazon WebServices（AWS）这样定义DevOps（https：//aws.amazon.com/cn/devops/what-is-devops/）：

​	DevOps集文化理念、实践和工具于一身，可以提高组织高速交付应用程序和服务的能力，与使用传统软件开发和基础设施管理流程相比，能够帮助组织更快地发展和改进产品。这种速度使组织能够更好地服务于客户，并在市场上更高效地参与竞争。

​	是不是可以理解为能帮助组织更快地发展和改进产品，可以提高组织高速交付应用程序和服务能力的都可以称自己为DevOps？

​	AWS给出的定义似乎没有可操作性。而维基百科（https：//zh.wikipedia.org/wiki/DevOps）给出的定义，可操作性或许多一些：

​	DevOps（Development和Operations的组合）是一种重视软件开发人员（Dev）和IT运维技术人员（Ops）之间沟通合作的文化、运动或惯例。通过自动化软件交付和架构变更的流程，使得构建、测试、发布软件能够更加快捷、频繁和可靠。

# Jenkins安装

https://www.jenkins.io/doc/book/installing/war-file/

```sh
wget https://get.jenkins.io/war-stable/2.289.1/jenkins.war --no-check-certificatejava -jar jenkins.war --httpPort=9090
```

https://www.cnblogs.com/ajianboke/p/10945522.html

**插件报错**

https://aflyun.blog.csdn.net/article/details/103338558

**SSL过不了**

​	调整时区，系统重新校验实践

**update site**

http://mirror.xmission.com/jenkins/updates/update-center.json

# pipeline入门

## pipline是什么

​	从某种抽象层次上讲，部署流水线（Deploymentpipeline）是指从软件版本控制库到用户手中这一过程的自动化表现形式。——《持续交付——发布可靠软件的系统方法》[1]（下称《持续交付》）

​	Jenkins 1.x只能通过界面手动操作来“描述”部署流水线。Jenkins 2.x终于支持pipeline as code了，可以通过“代码”来描述部署流水线。

* 更好地版本化：将pipeline提交到软件版本库中进行版本控制。
* 更好地协作：pipeline的每次修改对所有人都是可见的。除此之外，还可以对pipeline进行代码审查。
* 更好的重用性：手动操作没法重用，但是代码可以重用。



​	本书全面拥抱pipeline as code，放弃依赖手动操作的自由风格的项目（FreeStyle project）。

## Jenkinsfile又是什么

​	Jenkinsfile就是一个文本文件，也就是部署流水线概念在Jenkins中的表现形式。像Dockerfile之于Docker。所有部署流水线的逻辑都写在Jenkinsfile中。

​	Jenkins默认是不支持Jenkinsfile的。我们需要安装pipeline插件，本书使用的插件版本为2.27，其安装方式和普通插件的安装方式无异。安装完成后，就可以创建pipeline项目了，如图2-1所示。

![](https://pic.imgdb.cn/item/60d1fa2b844ef46bb29c6f2c.jpg)

## pipeline语法的选择

​	Jenkins团队在一开始实现Jenkins pipeline时，Groovy语言被选择作为基础来实现pipeline。所以，在写脚本式pipeline时，很像是（其实就是）在写Groovy代码。这样的确为用户提供了巨大的灵活性和可扩展性，我们还可以在脚本式pipeline中写try-catch。示例如下：

![](https://pic.imgdb.cn/item/60d3229a844ef46bb2cac661.jpg)

​	以上写法被称为脚本式（Scripted）语法。Jenkins pipeline还支持另一种语法：声明式（Declar-ative）语法。pipeline插件从2.5版本开始，才同时支持两种格式的语法。

​	脚本式语法的确灵活、可扩展，但是也意味着更复杂。再者，Groovy语言的学习成本对于（不使用Groovy的）开发团队来说通常是不必要的。所以才有了声明式语法，一种提供更简单、更结构化（more opinionated）的语法。示例如下：

​	![](https://pic.imgdb.cn/item/60d32528844ef46bb2d9e2ce.jpg)

​	本书所有的示例都将使用声明式语法。因为声明式语法更符合人类的阅读习惯、更简单。声明式语法也是Jenkins社区推荐的语法。

## 创建第一个pipeline

​	首先在Jenkins中新建一个pipeline项目，如图2-2所示。

![](https://pic.imgdb.cn/item/60d3265a844ef46bb2e0e63c.jpg)

​	在pipeline-hello-world项目的设置页面中，在Pipeline节点下填入pipeline的内容，如图2-3所示。

```
pipeline {    agent any    stages {        stage("build") {            steps {                echo "Hello World"            }        }    }}
```



![](https://pic.imgdb.cn/item/60d328c5844ef46bb2f00ac2.jpg)

执行后，结果如图2-4所示。

![](https://pic.imgdb.cn/item/60d32968844ef46bb2f43336.jpg)

​	和大多数Hello world示例一样，以上示例只是为了让大家对pipeline有一个感性的认识。

## 从版本控制库拉取pipeline

https://www.cnblogs.com/dotnet261010/p/12393917.html

​	在Hello world示例中，我们是直接在Jenkins界面上填入pipeline内容的。在试验时可以这么做，但是不推荐，因为这样无法做到pipeline的版本化。

​	首先需要安装Git插件，然后使用SSH的clone方式拉取代码。所以，需要将Git私钥放到Jenkins上，这样Jenkins才有权限从Git仓库拉取代码。

​	将Git私钥放到Jenkins上的方法是：进入Jenkins→Credentials→System→Globalcredentials页，然后选择Kind为“SSHUsername with private key”，接下来按照提示设置就好了，如图2-5所示。关于Credential的更多内容，我们会在第9章中进行详细介绍。目前只需要理解：Jenkins从Git仓库拉取代码时，需要SSH key就可以了，然后Jenkins本身提供了这种方式让我们设置。

![](https://pic.imgdb.cn/item/60d33852844ef46bb263d4da.jpg)

​	另外，需要注意的是，我们需要提前将SSH的公钥放到Git仓库中。关于这方面内容网络上有很多教程，本书不再赘述。现在，我们来看看项目结构，只有一个Jenkinsfile文件。

​	在Hello world示例中，在Pipeline节点下，在“Definition”中选择“Pipelinescript from SCM”，并在“SCM”中选择“Git”，然后根据选项填入信息内容就可以了，如图2-6所示。

​		现在，我们来看看项目结构，只有一个Jenkinsfile文件。

​		Jenkinsfile文件中的内容就是Hello world示例的内容。接下来，我们将项目推送到GitLab。

![](https://pic.imgdb.cn/item/60d33aad844ef46bb27599b5.jpg)

这里有两点需要注意：

* 在“Credentials”中选择我们刚刚创建的用于拉取代码的凭证。
* “Script Path”就是pipeline的文件名，默认是Jenkinsfile。保存并创建成功后，执行，在日志中除了Hello world被打印出来，git clone过程的日志也被打印出来。

![](https://pic.imgdb.cn/item/60d33d43844ef46bb28a16b0.jpg)

​	Maven是非常流行的一个Java应用构建工具。下面我们再来看一个使用Maven构建Java应用的例子。Jenkins默认支持Maven。首先在本地创建一个Maven项目，目录结构如下：

![](https://pic.imgdb.cn/item/60d341f7844ef46bb2b4d719.jpg)

​	接下来，需要在Jenkins上安装JDK和Maven。我们可以登录Jenkins服务器手动安装，也可以让Jenkins自动安装。这里选择后者。方法如下：

​	（1）进入Manage Jenkins→Global Tool Configuration→Maven页，设置如图2-7所示。

​		图略

​	留意Name输入框中的值，这里填的是mvn-3.5.4。在后面的pipeline中会使用到。

（2）进入Manage Jenkins→Global Tool Configuration→JDK页，设置如图2-8所示。

​	略

Jenkinsfile内容如下：

```
pipeline {    agent any    tools {        maven 'mvn-3.5.4'    }    stages {        stage("build") {            steps {                sh "mvn clean package spring-boot:repackage"                sh "printenv" // 将环境变量打印            }        }    }}
```

​	当Jenkins执行到tools时，就会根据Maven的设置自动下载指定版本的Maven，并安装。tools是pipeline中的一个指令，用于自动安装工具，同时将其路径放到PATH变量中。通过命令sh"printenv"，可以看到tools将MAVEN_HOME放到了当前任务的环境变量中。

​	关于tools的更多信息，我们会在第4章中进行详细介绍。单击构建后，通过Jenkins执行日志，我们看到指定版本的Maven被下载和安装，mvn执行打包。

​	![](https://pic.imgdb.cn/item/60d3439b844ef46bb2c5559d.jpg)

## 本章小结

​	由于历史原因，Jenkins pipeline支持两种语法。node为根节点的是脚本式语法，而pipeline为根节点的是声明式语法。本书使用的是Jenkins社区推荐的声明式语法。

# pipeline语法讲解

## 必要的Groovy知识

​	虽然学习Jenkins pipeline可以不需要任何Groovy知识，但是学习以下Groovy知识，对于我们写pipeline如虎添翼。

* 虽然Groovy同时支持静态类型和动态类型，但是在定义变量时，在Groovy中我们习惯使用def关键字，比如def x="abc"、def y=1。
* 不像Java，Groovy语句最后的分号不是必需的。
* Groovy中的方法调用可以省略括号，比如System.out.println "Hello world"。
* 支持命名参数，比如：

```groovy
def createName(Stirng giveName, Stirng familyName) {    return giveName + " " + familyName;}// 调用时可以这样createName familyName = "Lee", giveName = "Bruce"
```

* 支持默认参数值，比如：

```groovy
def sayHello(String name = "humans") {    print "hello ${name}"}sayHello() // 此时括号不能忽略
```

* 支持单引号、双引号。双引号支持插值，单引号不支持。比如：

```groovy
def name = 'world'print "hello ${name}" // 结果 hello worldprint 'hello ${name}' // 结果 hello ${name}
```

* 支持三引号。三引号分为三单引号和三双引号。它们都支持换行，区别在于只有三双引号支持插值。比如：

```groovy
def name = 'world'def aString = '''line oneline twoline three${name}'''def bString = """line oneline twoline three${name}"""
```

* 支持闭包。闭包的定义方法如下：

```groovy
// 定义闭包def codeBlock = { print "hello closure"}// 当成函数调用codeBlock()
```

还可以将闭包看作一个参数传递给另一个方法

```groovy
// 定义一个pipeline函数, 它接收一个闭包函数def pipeline(closure) {    closure()}// 在调用pipeline函数时，可以这样pipeline(codeBlock)// 如果把闭包定义的语句去掉pipeline({print "hello closure"})// 由于括号是非必需的, 所以pipeline {    print "hello closure"}
```

* 闭包的另类用法。我们定义一个stage函数

```groovy
def stage(String name, closure) {    println name    closure()}// 正常用法stage("stage name", {print "closure"})// 另外的写法stage("stage name") {    print "closure"}
```

​	这些知识点没有连贯性，读者浏览一遍后，大概有个印象就可以。等学习完本章后，再回头看就理解Jenkins pipeline的语法了。

## pipeline的组成

​	Jenkins pipeline其实就是基于Groovy语言实现的一种DSL（领域特定语言），用于描述整条流水线是如何进行的。流水线的内容包括执行编译、打包、测试、输出测试报告等步骤。

### pipeline最简结构

​	前文中，我们已经了解到：从软件版本控制库到用户手中这一过程可以分成很多阶段，每个阶段只专注处理一件事情，而这件事情又是通过多个步骤来完成的，这就是软件工程的pipeline。Jenkins对这个过程进行抽象，设计出一个基本的pipeline结构。

```groovy
pipeline {    agent any    stages {        stages('build') {            steps {                echo "hello world"            }        }    }}
```

* pipeline：代表整条流水线，包含整条流水线的逻辑。
* stage部分：阶段，代表流水线的阶段。每个阶段都必须有名称。本例中，build就是此阶段的名称。
* stages部分：流水线中多个stage的容器。stages部分至少包含一个stage。
* steps部分：代表阶段中的一个或多个具体步骤（step）的容器。steps部分至少包含一个步骤，本例中，echo就是一个步骤。在一个stage中有且只有一个steps。
* steps部分：代表阶段中的一个或多个具体步骤（step）的容器。steps部分至少包含一个步骤，本例中，echo就是一个步骤。在一个stage中有且只有一个steps。
* agent部分：指定流水线的执行位置（Jenkins agent）。流水线中的每个阶段都必须在某个地方（物理机、虚拟机或Docker容器）执行，agent部分即指定具体在哪里执行。我们会在第14章中进行详细介绍。

### 步骤

​	pipeline基本结构决定的是pipeline整体流程，但是真正“做事”的还是pipeline中的每一个步骤。步骤是pipeline中已经不能再拆分的最小操作。前文中，我们只看到两个步骤：sh和echo。sh是指执行一条shell命令；echo是指执行echo命令。这两个步骤只是Jenkins pipeline内置的大量步骤中的两个。

​	更好的设计是：步骤是可插拔的，就像Jenkins的插件一样。如果现有的插件不用修改或者只需要简单修改，就能在Jenkins pipeline中当成一个步骤来使用，该多好？这样就不用重新实现一遍已经存在的插件了。

​	已经有哪些插件适配了Jenkins pipeline呢？pipeline plugin的GitHub仓库给出了一个列表（https：//github.com/jenkinsci/pipeline-plugin/blob/master/COMPATIBILITY.md）方便大家检索，如图3-1所示（只截取了一部分）。

​	只要安装了这些适配了Jenkins pipeline的插件，就可以使用其提供的pipeline步骤。Jenkins官方还提供了pipeline步骤参考文档（https：//jenkins.io/doc/pipeline/steps/）。

![](https://pic.imgdb.cn/item/60d59eb7844ef46bb295e4ea.jpg)

## post部分

​	在上一章中，我们已经见过post部分，在pipeline执行失败后，发送邮件到指定邮箱中。

```
post {	failure {		mail to: 'team@example.com',		subject: 'The pipeline failed :('	}}
```

​	post部分包含的是在整个pipeline或阶段完成后一些附加的步骤。post部分是可选的，所以并不包含在pipeline最简结构中。但这并不代表它作用不大。根据pipeline或阶段的完成状态，post部分分成多种条件块，包括：

* always：不论当前完成状态是什么，都执行。
* changed：只要当前完成状态与上一次完成状态不同就执行。
* fixed：上一次完成状态为失败或不稳定（unstable），当前完成状态为成功时执行。
* regression：上一次完成状态为成功，当前完成状态为失败、不稳定或中止（aborted）时执行。
* aborted：当前执行结果是中止状态时（一般为人为中止）执行。
* failure：当前完成状态为失败时执行。
* success：当前完成状态为成功时执行。
* unstable：当前完成状态为不稳定时执行。
* cleanup：清理条件块。不论当前完成状态是什么，在其他所有条件块执行完成后都执行。post部分可以同时包含多种条件块。以下是post部分的完整示例。

```groovy
pipeline {    agent any    stages {        stage('build') {            steps {                echo "build stage"            }            post {                always {                    echo "stage post always"                }            }        }    }    post {        changed {            echo "pipeline post changed"        }        always {            echo "pipeline post always"        }        success {            echo "pipeline post success"        }        // 省略其他模块    }}
```

## pipeline支持的指令

​	显然，基本结构满足不了现实多变的需求。所以，Jenkins pipeline通过各种指令（directive）来丰富自己。指令可以被理解为对Jenkins pipeline基本结构的补充。

Jenkins pipeline支持的指令有：

* environment：用于设置环境变量，可定义在stage或pipeline部分。
* tools：可定义在pipeline或stage部分。它会自动下载并安装我们指定的工具，并将其加入PATH变量中。
* input：定义在stage部分，会暂停pipeline，提示你输入内容。
* options：用于配置Jenkins pipeline本身的选项，比如options {retry（3）}指当pipeline失败时再重试2次。options指令可定义在stage或pipeline部分。
* parallel：并行执行多个step。在pipeline插件1.2版本后，parallel开始支持对多个阶段进行并行执行。
* parameters：与input不同，parameters是执行pipeline前传入的一些参数。
* triggers：用于定义执行pipeline的触发器。
* when：当满足when定义的条件时，阶段才执行。



​	在使用指令时，需要注意的是每个指令都有自己的“作用域”。如果指令使用的位置不正确，Jenkins将会报错。

## 配置pipeline本身

​	options指令用于配置整个Jenkins pipeline本身的选项。根据具体的选项不同，可以将其放在pipeline块或stage块中。以下例子若没有特别说明，options被放在pipeline块中。

* buildDiscarder：保存最近历史构建记录的数量。当pipeline执行完成后，会在硬盘上保存制品和构建执行日志，如果长时间不清理会占用大量空间，设置此选项后会自动清理。此选项只能在pipeline下的options中使用。示例如下：

```
options {	buildDiscrader(logRotator(numToKeepStr: '10'))}
```

* checkoutToSubdirectory：Jenkins从版本控制库拉取源码时，默认检出到工作空间的根目录中，此选项可以指定检出到工作空间的子目录中。示例如下：

```groovy
options {    checkoutToSubdirectory('subdir')}
```

* disableConcurrentBuilds：同一个pipeline，Jenkins默认是可以同时执行多次的，如图3-2所示。此选项是为了禁止pipeline同时执行。示例如下：

```
options {	disableConcurrentBuilds()}
```

![](https://pic.imgdb.cn/item/60d5a224844ef46bb2a3401a.jpg)

在某些pipeline存在抢占资源或调用冲突的场景下，此选项非常有用。设置此选项后，如图3-3所示。

![](https://pic.imgdb.cn/item/60d5a236844ef46bb2a3859a.jpg)

* newContainerPerStage：当agent为docker或dockerfile时，指定在同一个Jenkins节点上，每个stage都分别运行在一个新的容器中，而不是所有stage都运行在同一个容器中。

```groovy
options {    newContainerPerStage()}
```

* retry: 当失败发生时进行重试, 可以指定整个pipeline的重试次数需要注意的是，这个次数是指总次数，包括第1次失败。以下例子总共会执行4次。当使用retry选项时，options可以被放在stage块中。

```groovy
pipeline {    agent any    options {        retry(4)    }    stages {        stage('build') {            steps {                echo "ok"                error("emm..")            }        }    }}
```

* timeout：如果 pipeline 执行时间过长，超出了我们设置的 timeout 时间，Jenkins 将中止pipeline。以下例子中以小时为单位，还可以以 SECONDS（秒）、MINUTES（分钟）为单位。当使用timeout选项时，options可以被放在stage块中。

```groovy
options {    timeout(time: 10, unit: "HOURS")}
```

​	设置此选项后，强迫团队去处理执行时间过长的pipeline，从而优化pipeline的反馈周期。通常将timeout设置为10分钟就可以了。

## 在声明式pipeline中使用脚本

​	在使用声明式pipeline一段时间后，你会发现直接在steps块中写if-else，或者定义一个变量，Jenkins都会报错。也就是不能直接在steps块中写Groovy代码。

​	Jenkins pipeline专门提供了一个script步骤，你能在script步骤中像写代码一样写pipeline逻辑。比如分别在不同的浏览器上跑测试。

```groovy
pipeline {    agent any    stages {        stage('Example') {            steps {                script {                    def browsers = ['chrome', 'firefox']                    for (int i = 0; i < browsers.size(); i++) {                        echo "Testing the ${browsers[i]} browser"                    }                }            }        }    }}
```

​	可以看出，在script块中的其实就是Groovy代码。大多数时候，我们是不需要使用script步骤的。如果在script步骤中写了大量的逻辑，则说明你应该把这些逻辑拆分到不同的阶段，或者放到共享库中。共享库是一种扩展Jenkins pipeline的技术，我们会在后面的章节中讲到。

​	另外，细心的读者可能已经注意到，这样串行的测试方法是低效的，而应该在不同的浏览器上并行跑测试。

## pipeline内置基础步骤

### 文件目录相关步骤

**deleteDir：删除当前目录**

​	deleteDir是一个无参步骤，删除的是当前工作目录。通常它与dir步骤一起使用，用于删除指定目录下的内容。

**dir：切换到目录**

​	默认pipeline工作在工作空间目录下，dir步骤可以让我们切换到其他目录。使用方法如下：

```groovy
dir("/var/logs") {    deleteDir()}
```

**fileExists：判断文件是否存在**

​	fileExists（'/tmp/a.jar'）判断/tmp/a.jar文件是否存在。如果参数是相对路径，则判断在相对当前工作目录下，该文件是否存在。结果返回布尔类型。

**isUnix：判断是否为类UNIX系统**

​	如果当前pipeline运行在一个类UNIX系统上，则返回true。

**pwd：确认当前目录**

​	pwd与Linux的pwd命令一样，返回当前所在目录。它有一个布尔类型的可选参数：tmp，如果参数值为true，则返回与当前工作空间关联的临时目录。

**writeFile：将内容写入指定文件中**

*  file：文件路径，可以是绝对路径，也可以是相对路径。
*  text：要写入的文件内容。
*  encoding（可选）：目标文件的编码。如果留空，则使用操作系统默认的编码。如果写的是Base64的数据，则可以使用Base64编码。

**readFile：读取文件内容**

​	读取指定文件的内容，以文本返回。readFile支持的参数有：

* file：路径，可以是绝对路径，也可以是相对路径。
* encoding（可选）：读取文件时使用的编码。

示例如下：

![](https://pic.imgdb.cn/item/60d736b1844ef46bb2b5b237.jpg)

### 制品相关步骤

**stash：保存临时文件**

​	stash步骤可以将一些文件保存起来，以便被同一次构建的其他步骤或阶段使用。如果整个pipeline的所有阶段在同一台机器上执行，则stash步骤是多余的。所以，通常需要stash的文件都是要跨Jenkins node使用的。关于Jenkins node的相关概念，我们会在第14章中进行介绍。

​	stash步骤会将文件存储在tar文件中，对于大文件的stash操作将会消耗Jenkinsmaster的计算资源。Jenkins官方文档推荐，当文件大小为5∼100MB时，应该考虑使用其他替代方案。

​	stash步骤的参数列表如下：

*  name：字符串类型，保存文件的集合的唯一标识。
*  allowEmpty：布尔类型，允许stash内容为空。
*  excludes：字符串类型，将哪些文件排除。如果排除多个文件，则使用逗号分隔。留空代表不排除任何文件。
*  includes：字符串类型，stash哪些文件，留空代表当前文件夹下的所有文件。
*  useDefaultExcludes：布尔类型，如果为true，则代表使用Ant风格路径默认排除文件列表。



​	除了name参数，其他参数都是可选的。excludes和includes使用的是Ant风格路径表达式。在3.7.5节中将简单介绍该表达式写法。

**unstash：取出之前stash的文件**

​	unstash步骤只有一个name参数，即stash时的唯一标识。通常stash与unstash步骤同时使用。以下是完整示例。

![](https://pic.imgdb.cn/item/60d7387c844ef46bb2c6236e.jpg)

### 命令相关步骤

​	与命令相关的步骤其实是Pipeline：Nodes and Processes插件提供的步骤。由于它是Pipeline插件的一个组件，所以基本不需要单独安装。

**sh：执行shell命令**

sh步骤支持的参数有：

* script：将要执行的shell脚本，通常在类UNIX系统上可以是多行脚本。
* encoding：脚本执行后输出日志的编码，默认值为脚本运行所在系统的编码。
* returnStatus：布尔类型，默认脚本返回的是状态码，如果是一个非零的状态码，则会引发pipeline执行失败。如果returnStatus参数为true，则不论状态码是什么，pipeline的执行都不会受影响。
* returnStdout：布尔类型，如果为true，则任务的标准输出将作为步骤的返回值，而不是打印到构建日志中（如果有错误，则依然会打印到日志中）。除了script参数，其他参数都是可选的。



returnStatus与returnStdout参数一般不会同时使用，因为返回值只能有一个。如果同时使用，则只有returnStatus参数生效。

**bat、powershell步骤**

* bat步骤执行的是Windows的批处理命令。powershell步骤执行的是PowerShell脚本，支持3+版本。这两个步骤支持的参数与sh步骤的一样，这里就不重复介绍了。

###  其他步骤

**error：主动报错，中止当前pipeline**

​	error 步骤的执行类似于抛出一个异常。它只有一个必需参数：message。通常省略参数：error（"there's an error"）。
**tool：使用预定义的工具**

​	如果在Global Tool Configuration（全局工具配置）中配置了工具，如图3-4所示，比如配置了Docker，那么可以通过tool步骤得到工具路径。

![](https://pic.imgdb.cn/item/60d738fd844ef46bb2cac4ed.jpg)

tool步骤支持的参数有：

* name：工具名称。
* type（可选）：工具类型，指该工具安装类的全路径类名。



​	每个插件的type值都不一样，而且绝大多数插件的文档根本不写type值。除了到该插件的源码中查找，还有一种方法可以让我们快速找到type值，就是前往Jenkinspipeline代码片段生成器中生成该tool步骤的代码即可，如图3-5所示。

![](https://pic.imgdb.cn/item/60d73923844ef46bb2cc246a.jpg)

**timeout：代码块超时时间**

​	为timeout步骤闭包内运行的代码设置超时时间限制。如果超时，将抛出一个org.jenkinsci.plugins.workflow.steps.FlowInterruptedException异常。timeout步骤支持如下参数：

*  time：整型，超时时间。
*  unit（可选）：时间单位，支持的值有NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS、MINUTES（默认）、HOURS、DAYS。
*  activity（可选）：布尔类型，如果值为true，则只有当日志没有活动后，才真正算作超时

**waitUntil：等待条件满足**

​	不断重复waitUntil块内的代码，直到条件为true。waitUntil不负责处理块内代码的异常，遇到异常时直接向外抛出。waitUntil步骤最好与timeout步骤共同使用，避免死循环。示例如下：



![](https://pic.imgdb.cn/item/60d73956844ef46bb2cdd6e2.jpg)

**retry：重复执行块**

执行N 次闭包内的脚本。如果其中某次执行抛出异常，则只中止本次执行，并不会中止整个retry的执行。同时，在执行retry的过程中，用户是无法中止pipeline的。
![](https://pic.imgdb.cn/item/60d73971844ef46bb2ceb34b.jpg)

**sleep：让pipeline休眠一段时间**

sleep步骤可用于简单地暂停pipeline，其支持的参数有：

*  time：整型，休眠时间。
*  unit（可选）：时间单位，支持的值有NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS（默认）、MINUTES、HOURS、DAYS。

![](https://pic.imgdb.cn/item/60d7398e844ef46bb2cfaa03.jpg)

### 小贴士

**使用pipeline代码片段生成器学习**

​	对于初学Jenkins pipeline的新人来说，如何开始写pipeline是一个坎儿。好在Jenkins提供了一个pipeline代码片段生成器，通过界面操作就可以生成代码。

​	对于初学Jenkins pipeline的新人来说，如何开始写pipeline是一个坎儿。好在Jenkins提供了一个pipeline代码片段生成器，通过界面操作就可以生成代码。

​	进入“Pipeline Syntax”页面后，在右边的“Sample Step”下拉框中选择需要生成代码的步骤，并根据提示填入参数，然后单击“Generate Pipeline Script”按钮，就可以生成代码了，如图3-7所示。

![](https://pic.imgdb.cn/item/60d739be844ef46bb2d15867.jpg)

![](https://pic.imgdb.cn/item/60d739c7844ef46bb2d1b2cd.jpg)

**使用VS Code扩展校验Jenkinsfile**

​	不像Java语言有各种开发工具支持，Jenkinsfile从诞生以来就没有很好的工具支持，无奈只能使用VS Code文本编辑器+Groovy语法高亮进行开发。对语法的校验全凭自己对Jenkinsfile的熟悉程度。

​	2018年11月初，Jenkins官方博客介绍了一个VS Code扩展：Jenkins PipelineLinter Connector，实现了对Jenkinsfile的语法校验。

​	在VS Code应用市场搜索“Jenkins Pipeline Linter Connector”并安装，然后对该扩展进行设置，如图3-8所示。

![](https://pic.imgdb.cn/item/60d739e8844ef46bb2d2d9fe.jpg)

​	然后，进入Jenkins的Manage Jenkins→Manage Configure Global Security页，确认Jenkins启用了“CSRF Protection”，如图3-9所示。

接下来，打开一个Jenkinsfile文件，调用扩展命令，如图3-10所示

![](https://pic.imgdb.cn/item/60d739fe844ef46bb2d3b039.jpg)

最后，在OUTPUT中可以看到校验结果，如图3-11所示。

![](https://pic.imgdb.cn/item/60d73a13844ef46bb2d472b1.jpg)

​	值得注意的是，该扩展只能利用Jenkins API进行语法校验。比如将input步骤写成nput，校验同样通过。

**使用Workspace Cleanup插件清理空间**	

​	通常，当pipeline执行完成后，并不会自动清理空间。如果需要（通常需要）清理工作空间，则可以通过Workspace Cleanup插件实现。

（1）安装Workspace Cleanup插件（地址为https：//plugins.jenkins.io/ws-cleanup）。

（2）在pipeline的post部分加入插件步骤。

![](https://pic.imgdb.cn/item/60d73a3a844ef46bb2d5e021.jpg)

**Ant风格路径表达式简介**

​	Ant是比Maven更老的Java构建工具。Ant发明了一种描述文件路径的表达式，大家都习惯称其为Ant风格路径表达式。Jenkins pipeline的很多步骤的参数也会使用此表达式。

​	Ant路径表达式包括3种通配符。

* ？：匹配任何单字符。
* *：匹配0个或者任意数量的字符。
* **：匹配0个或者更多的目录



我们通过以下例子来学习。

* `**/CVS/*`：匹配CVS文件夹下的所有文件，CVS文件夹可以在任何层级。

# 环境变量与构建工具

## 环境变量

​	环境变量可以被看作是pipeline与Jenkins交互的媒介。比如，可以在pipeline中通过BUILD_NUMBER变量知道构建任务的当前构建次数。环境变量可以分为Jenkins内置变量和自定义变量。接下来我们分别讨论。

### Jenkins内置变量

​	在pipeline执行时，Jenkins通过一个名为env的全局变量，将Jenkins内置环境变量暴露出来。其使用方法有多种，示例如下：

```groovy
pipeline {    agent any    stages {        stage('Example') {            steps {                echo "Runing ${evn.BUILD_NUMBER} on ${evn.JENKINS_URL}"                echo "Runing $evn.BUILD_NUMBER on $evn.JENKINS_URL"                echo "Runing ${BUILD_NUMBER} on ${JENKINS_URL}"            }        }    }}
```

​	默认env的属性可以直接在pipeline中引用。所以，以上方法都是合法的。但是不推荐方法三，因为出现变量冲突时，非常难查问题。

​	那么，env变量都有哪些可用属性呢？通过访问＜Jenkins master的地址>/pipeline-syntax/globalsenv来获取完整列表。在列表中，当一个变量被声明为“For a multibranchproject”时，代表只有多分支项目才会有此变量。

* BUILD_NUMBER：构建号，累加的数字。在打包时，它可作为制品名称的一部分，比如server-2.jar。
* BRANCH_NAME：多分支pipeline项目支持。当需要根据不同的分支做不同的事情时就会用到，比如通过代码将release分支发布到生产环境中、master分支发布到测试环境中。
* BUILD_URL：当前构建的页面URL。如果构建失败，则需要将失败的构建链接放在邮件通知中，这个链接就可以是BUILD_URL。
* GIT_BRANCH：通过git拉取的源码构建的项目才会有此变量。



​	在使用env变量时，需要注意不同类型的项目，env变量所包含的属性及其值是不一样的。比如普通pipeline任务中的GIT BRANCH变量的值为origin/master，而在多分支pipeline任务中GITBRANCH变量的值为master。

​	小技巧：在调试pipeline时，可以在pipeline的开始阶段加一句：sh'printenv'，将env变量的属性值打印出来。这样可以帮助我们避免不少问题。

### 自定义pipeline环境变量

​	当pipeline变得复杂时，我们就会有定义自己的环境变量的需求。声明式pipeline提供了environment指令，方便自定义变量。比如：

```groovy
pipeline {    agent any    environment {        CC = 'clang'    }    stages {        stage('Example') {            environment {                DEBUG_FLAGS = '-g'            }            steps {                sh "${CC} ${DEBUG_FLAGS}"                sh 'printenv'            }        }    }}
```

​	另外，environment指令可以在pipeline中定义，代表变量作用域为整个pipeline；也可以在stage中定义，代表变量只在该阶段有效。

​	在实际工作中，还会遇到一个环境变量引用另一个环境变量的情况。在environment中可以这样定义：

![](https://pic.imgdb.cn/item/60d7f0905132923bf87c5d2f.jpg)

​	值得注意的是，如果在environment中定义的变量与env中的变量重名，那么被重名的变量的值会被覆盖掉。比如在environment中定义PATH变量（PATH也是env中的一个变量）。

### 自定义全局环境变量

​	进入Manage Jenkins→ConfigureSystem→Global properties页，勾选“Environment variables”复选框，单击“Add”按钮，在输入框中输入变量名和变量值即可。

​	自定义全局环境变量会被加入 env 属性列表中，所以，使用自定义全局环境变量与使用Jenkins内置变量的方法无异：${env.g name}。

## 构建工具

​	构建是指将源码转换成一个可使用的二进制程序的过程。这个过程可以包括但不限于这几个环节：下载依赖、编译、打包。构建过程的输出——比如一个zip包，我们称之为制品（有些书籍也称之为产出物）。而管理制品的仓库，称为制品库。

### 构建工具的选择

​	对构建工具的选择，很大一部分因素取决于你所使用的语言。比如构建 Scala 使用 SBT，JavaScript的Babel、Browserify、Webpack、Grunt以及Gulp等。当然，也有通用的构建工具，比如Gradle，它不仅支持Java、Groovy、Kotlin等语言，通过插件的方式还可以实现对更多语言的支持。

​	对构建工具的选择，还取决于团队对工具本身的接受程度。笔者的建议是，团队中同一技术栈的所有项目都使用同一个构建工具。

### tools指令介绍

​	tools指令能帮助我们自动下载并安装所指定的构建工具，并将其加入PATH变量中。这样，我们就可以在sh步骤里直接使用了。但在agent none的情况下不会生效。

​	tools指令默认支持3种工具：JDK、Maven、Gradle。通过安装插件，tools指令还可以支持更多的工具。接下来，我们介绍几种常用的构建环境的搭建。

### JDK环境搭建

**自动安装JDK**

​	设置自动安装Oracle JDK时有一些特殊，因为下载Oracle JDK时需要输入用户名和密码。进入Manage Jenkins→Global ToolConfiguration→JDK页，单击“Add JDK”按钮，就可以设置自动安装JDK，如图4-2所示。

![](https://pic.imgdb.cn/item/60d7fb1f5132923bf8b272d7.jpg)

​	单击图4-2中所示的“Please enter yourusername/password”链接，在弹出的对话框中输入你在Oracle官网上的用户名和密码。

​	基于安全的考虑，公司的网络可能无法直接访问外网，所以无法使用自动下载。这时就需要在Jenkins agent上自行安装JDK，然后在ManageJenkins→Global Tool Configuration→JDK页中指定名称和JAVA_HOME路径，如图4-3所示。

![](https://pic.imgdb.cn/item/60d8001d5132923bf8cec151.jpg)

### Maven

​	Jenkins pipeline的tools指令默认就支持Maven。所以，使用Maven只需要两步。

​	（1）进入Manage Jenkins→Global ToolConfiguration→Maven页，设置如图4-4所示。请注意Name的值为mvn-3.5.4。接下来会用到这个值。“Install from Apache”下的Version可以选择Maven版本。

​	（2）在Jenkinsfile中指定Maven版本，并使用mvn命令。

![](https://pic.imgdb.cn/item/60d800675132923bf8d07b75.jpg)

​	这样，当执行到tools指令时，Jenkins会自动下载并安装Maven。将mvn命令加入环境变量中，可以使我们在pipeline中直接执行mvn命令。

### 使用Managed files设置Maven

​	Maven默认使用的是其官方仓库，国内下载速度很慢。所以，我们通常会使用国内的Maven镜像仓库。这时就需要修改 Maven 的配置文件settings.xml。settings.xml 文件的默认路径为${M2 HOME}/conf/settings.xml。但是，我们是不可能登录上Jenkins的机器，然后手动修改这个文件的。

​	Config File Provider插件（https：//plugins.jenkins.io/config-file-provider）能很好地解决这个问题。只需要在Jenkins的界面上填入settings.xml的内容，然后在pipeline中指定settings.xml就可以了。也就是说，对于不同的pipeline，可以使用不同的settings.xml。

​	具体实现方法如下：

1. 安装Config File Provider插件。
2. 进入Manage Jenkins页面，就可以看到多出一个“Managed files”菜单，如图4-5所示。
3. 单击“Managed files”进入，在左侧菜单栏中选择“Add a new Config”，就会看到该插件支持很多种配置文件的格式及方式，如图4-6所示。

![](https://pic.imgdb.cn/item/60d805d15132923bf8f2a8f0.jpg)

​	我们看到列表中有多个重复的选项，看来ConfigFile Provider插件2.18版本在Jenkins 2.121.1下有Bug。但是依然可以设置，不会报错。

​	（4）选择“Global Maven settings.xml”选项。因为我们的设置是全局的。填写“ID”字段，Jenkins pipeline会引用此变量名。假如使用的ID为maven-global-settings。

​	（5）单击“Submit”按钮提交后，就看到编辑页了。将自定义的Maven settings.xml的内容粘贴到“Content”字段中，单击“Submit”按钮即添加完成，如图4-7所示。

![](https://pic.imgdb.cn/item/60d81e3e5132923bf8a1604e.jpg)

（6）在Jenkins pipeline中使用的方法如下

```groovy
configfileProvider([configFile(fileId: 'maven-global-settings', variable: 'MAVEN_GLOBAL_ENV')]) {    sh 'mvn -s $MAVEN_GLOBAL_ENV clean install'}
```

### Go语言环境搭建

Jenkins支持Golang的构建，只需要以下几步。

（1）安装Go插件（https：//plugins.jenkins.io/golang）。

（2）进入Manage Jenkins→Global ToolConfiguration→Go页，设置如图4-8所示。

（3）在pipeline中加入tools部分。

```groovy
tools {    go 'go1.10'}
```

此时，在环境变量中会增加一个GOROOT变量。

![](https://pic.imgdb.cn/item/60d81f5e5132923bf8a9b358.jpg)

（4）设置GOPATH。了解Go语言开发的读者都会知道，编译时需要设置GOPATH环境变量。直接在environment指令中添加就可以了。

​	完整代码如下：

![](https://pic.imgdb.cn/item/60d81f8e5132923bf8ab0f5c.jpg)

​	Python环境很容易产生Python版本冲突、第三方库冲突等问题。所以，Python开发通常会进行工程级别的环境隔离，也就是每个Python工程使用一个Python环境。

​	在Jenkins环境下，我们使用Pyenv Pipeline插件（https：//plugins.jenkins.io/pyenv-pipeline）可以轻松地实现。

​	首先，准备Python基础环境。

（1）在Jenkins机器上安装python、pip、virtualenv。

* pip：Python的包管理工具。
* virtualenv：Python中的虚拟环境管理工具。

（2）安装Pyenv Pipeline插件。

​	然后，在pipeline中使用Pyenv Pipeline插件提供的withPythonEnv方法。

![](https://pic.imgdb.cn/item/60d820395132923bf8aff3dc.jpg)

​	withPythonEnv方法会根据第一个参数——可执行python路径——在当前工作空间下创建一个virtualenv环境。

​	withPythonEnv方法的第二个参数是一个闭包。闭包内的代码就执行在新建的virtualenv环境下

## 利用环境变量支持更多的构建工具

​	平时，开发人员在搭建开发环境时做的就是：首先在机器上安装好构建工具，然后将这个构建工具所在目录加入PATH环境变量中。

​	平时，开发人员在搭建开发环境时做的就是：首先在机器上安装好构建工具，然后将这个构建工具所在目录加入PATH环境变量中。

![](https://pic.imgdb.cn/item/60d821b95132923bf8bad035.jpg)

还可以有另一种写法：

![](https://pic.imgdb.cn/item/60d8222e5132923bf8be24ab.jpg)

## 利用tools作用域实现多版本编译

​	在实际工作中，有时需要对同一份源码使用多个版本的编译器进行编译。tools指令除了支持pipeline作用域，还支持stage作用域。所以，我们可以在同一个pipeline中实现多版本编译。代码如下：

![](https://pic.imgdb.cn/item/60d822e25132923bf8c359ed.jpg)

​	在打印出来的日志中，会发现每个stage下的JAVA_HOME变量的值都不一样。

# 代码质量

## 静态代码分析

### 代码规范检查

​	写代码时大括号该不该换行？对于这样的问题，很容易在团队里引发“战争”。在笔者看来，像该不该换行这类代码风格的优缺点问题，不是关键问题。关键问题在于整个团队甚至整个公司所有人是否采用同一套规范。

​	2017年阿里巴巴发布了《阿里巴巴Java开发手册》（https：//github.com/alibaba/p3c），在行业内引起了不小的轰动。《阿里巴巴Java开发手册》（下文以p3c简称）内容包括：命名风格、常量定义等。有了阿里巴巴的“光环”，公司内所有人就“代码规范”达成共识，变得更容易了。至于p3c里的规范是否真的是最好的，这是相对次要的一个问题。

### 使用PMD进行代码规范检查

​	PMD（https：//pmd.github.io/）是一款可扩展的静态代码分析器，它不仅可以对代码风格进行检查，还可以检查设计、多线程、性能等方面的问题。

​	Maven的PMD插件（https：//pmd.github.io/）使我们能在Maven上使用PMD。

使用步骤如下：

（1）在Maven项目的pom.xml中加入PMD插件。

![](https://pic.imgdb.cn/item/60d82b1d5132923bf8ff4dde.jpg)

​	maven-pmd-plugin插件并不会自动使用p3c-pmd，需要在引入dependencies部分手动加入p3c-pmd依赖，然后在rulesets属性中引入p3c的规则。

（2）安装Jenkins PMD插件（https：//pmd.github.io/）。

​	Jenkins PMD插件的作用是将PMD报告呈现在任务详情页中

（3）在Jenkinsfile中加入pmd步骤。

![](https://pic.imgdb.cn/item/60d833245132923bf83a25bd.jpg)

![](https://pic.imgdb.cn/item/60d833335132923bf83a9cdf.jpg)

### 各静态代码分析器之间的区别

​	目前每种语言基本上都有自己的静态代码分析器，比如Java语言，除PMD外，还有Check-style、FindBugs等。但是没有一款能“大一统”，实现对所有语言、所有场景的支持。

​	另外，同一种语言下的不同分析器，它们在功能上既有区别，又有重叠，读者需要根据自己团队的情况进行选择。但是不论选择哪款分析器，所有进行静态代码分析的地方都必须统一分析规则。比如我们决定使用阿里巴巴的开发规范，那么Maven插件、IDE插件以及后面说到的SonarQube都必须使用；否则，分析结果可能会不一致，进而影响分析结果的可信度。

## 单元测试

### JUnit单元测试报告

​	Maven会执行测试阶段（包括单元测试），然后生成测试报告。

​	收集并展示JUnit测试报告的步骤如下：

（1）安装Jenkins JUnit插件（https：//plugins.jenkins.io/junit）。

（2）在Jenkins中加入junit步骤。通常将junit步骤放在post always中，因为当测试不通过时，我们依然可以收集到测试报告。写法如下：

![](https://pic.imgdb.cn/item/60d83a5d5132923bf86bff30.jpg)

单击“Test Result”进入，可以看到测试报告的详细信息，如图5-4所示。

![](https://pic.imgdb.cn/item/60d83a755132923bf86c9d26.jpg)

### JaCoCo实现代码覆盖率

​	JUnit只是方便我们写单元测试的一个框架，但是并没有告诉我们有多少代码被测试覆盖到了。而JaCoCo填补了这一空白。JaCoCo是一个免费的Java代码覆盖率的库，能帮助我们检测出代码覆盖率，并输出覆盖率报告。

JaCoCo提供了以下几个维度的覆盖率分析。

* 指令覆盖率（Instruction Coverage）
* 分支覆盖率（Branch Coverage）
* 圈复杂度覆盖率（Cyclomatic Complexity Coverage）
* 行覆盖率（Line **Coverage**)
* 方法覆盖率（Method Coverage）

以下是JaCoCo插件的使用步骤。

（1）安装JaCoCo插件（https：//plugins.jenkins.io/jacoco）。

（2）在Maven项目中引入JaCoCo插件，执行maven jacoco生成代码覆盖率报告。

![](https://pic.imgdb.cn/item/60d8410e5132923bf8951521.jpg)

（3）使用jacoco步骤。jacoco步骤在mvn命令之后执行，写法如下：

![](https://pic.imgdb.cn/item/60d841a55132923bf898ae76.jpg)

为了更好地理解 jacoco 步骤的参数，我们看看 JaCoCo 插件在自由风格项目中的UI，如图5-5所示。

![](https://pic.imgdb.cn/item/60d846005132923bf8b1dc21.jpg)

​	pipeline运行完成后，我们可以在任务详情页的下方看到报告，如图5-6所示。

​	buildOverBuild和changeBuildStatus参数都能影响Jenkins任务的结果状态，那么当这两个参数的值都为true时，结果是什么呢？由其共同决定。以下是它们的判断逻辑。

![](https://pic.imgdb.cn/item/60d8461a5132923bf8b271eb.jpg)

​	最后，各个维度的覆盖率应该设置多少呢？没有标准答案。

​	笔者的经验是先要确定项目是遗留的还是新建的。遗留的就以当前覆盖率为基线，新建的则设置相对高一些的要求。再看项目的紧急程度，如果非常紧急的话，则可以考虑放低要求。最后看项目的重要程度。如果这个项目在整个架构中起着非常重要的作用，那么覆盖率要求会高一些。

### 代码覆盖率越高，软件的质量就越高吗

​	代码覆盖率最好不要单独使用，而是需要与其他指标，如代码变动率、复杂度等一并考虑。

## 性能测试

​	Taurus是一个开源的自动化框架，用于运行各种开源负载测试工具和功能测试工具。其支持最流行的开源负载测试工具Apache JMeter、Selenium、Gatling、The Grinder等。Taurus的关键特性有：

* 我们可以使用YAML或JSON来描述性能测试。这也正是我们想要的test ascode。
* 它会根据我们选择的性能测试类型自动下载相应的工具。比如在下例中会使用JMeter，那么Taurus会自动下载JMeter并安装。



​	Jenkins的Performance插件就是使用Taurus来进行性能测试的。在进行性能测试之前，首先要准备环境。

### 准备性能测试环境

（1）在运行性能测试环境的机器上，按照4.2.6节介绍的步骤准备Python环境。

（2）安装Performance插件（https：//plugins.jenkins.io/performance）。

（3）安装Taurus？不需要自行安装，Performance插件如果发现机器上没有安装Taurus，它会自动运行pip install bzt命令进行安装。

### 运行JMeter测试

​	假设平时你都是手动执行JMeter测试的，现在希望将它自动化。这很简单，只需要两步。

（1）在现有的项目中加入Jenkinsfile。

![](https://pic.imgdb.cn/item/60d847095132923bf8b7da62.jpg)

（2）在项目中加入blaze_exist_jmeter_config.yml文件。

![](https://pic.imgdb.cn/item/60d8471a5132923bf8b83dd8.jpg)

​	blaze_exist_jmeter_config.yml是Taurus的配置文件，用于描述如何进行性能测试。以上配置很简单，就是执行一个名为simple的场景（scenario），这个场景就是执行现有的JMeter脚本。modules配置了JMeter的下载地址及版本。上例中，我们指定了国内的下载链接，避免从国外下载。

​	在Jenkinsfile中，bzt是Performance插件提供的一个步骤。其参数如下：

* params：字符串类型，Taurus配置文件的路径。
* alwaysUseVirtualenv：布尔类型，如果为false，则不使用virtualenv进行环境隔离。默认值为true。
* bztVersion：字符串类型，bzt版本。
* generatePerformanceTrend：布尔类型，是否在Jenkins项目详情页生成性能趋势图。默认值为true。
* useBztExitCode：布尔类型，是否使用bzt步骤的退出码作为Jenkins项目的构建结果。默认值为true。
* useSystemSitePackages：布尔类型，是否为virtualenv加上“--system-site-packages”参数。默认值为true。
* workingDirectory：字符串类型，指定bzt的工作目录。
* workspace：字符串类型，已经废弃，请使用workingDirectory。



​	至此，以上用法可以满足大部分人在Jenkins上使用JMeter的需求。关于Taurus配置文件的更多语法，大家可以前往Taurus官网学习。

![](https://pic.imgdb.cn/item/60d847605132923bf8b9c77b.jpg)

## SonarQube：持续代码质量检查

​	关于更详细的区别，可前往官方网站（https：//www.sonarsource.com/plans-and-pricing/）进行了解。本书使用的是开源版6.7.5 LTS，假设读者已经安装此版本。

### Maven与SonarQube集成

​	为方便起见，我们就不自己写例子了，而是直接使用JUnit 4源码来做示例。将JUnit 4从GitHub克隆下来后，在pom.xml中加入SonarQube插件依赖。

![](https://pic.imgdb.cn/item/60d848755132923bf8bfd39f.jpg)

执行命令：
![](https://pic.imgdb.cn/item/60d8488d5132923bf8c06501.jpg)

​	sonar.host.url参数用于指定SonarQube服务的地址。这时，就可以在SonarQube的“Projects”中看到JUnit 4的分析结果，如图5-9所示。

![](https://pic.imgdb.cn/item/60d848a35132923bf8c0e838.jpg)

​	可以看到JUnit 4有11个Bug。SonarQube服务默认允许任何人执行源码分析，因此在生产环境中使用会有安全隐患。以下几步可以提高其安全性：

​	（1）设置SonarQube禁止非登录用户使用，如图5-10所示。

![](https://pic.imgdb.cn/item/60d848b95132923bf8c15e61.jpg)

（2）为用户生成Token，Jenkins只能通过Token与SonarQube集成。登录SonarQube，进入个人设置页面中的Security tab页，如图5-11所示。

![](https://pic.imgdb.cn/item/60d848cc5132923bf8c1c408.jpg)

（3）在执行mvn命令时加入相应的sonar.login参数。

![](https://pic.imgdb.cn/item/60d848da5132923bf8c20eaa.jpg)

### Jenkins与SonarQube集成

​	在上一节中，我们将Maven与SonarQube集成。这时，SonarQube对于Jenkins来说还是透明的，Jenkins并不知道代码质量如何。本节我们将集成Jenkins与SonarQube，以实现当代码质量不合格时，Jenkins pipeline失败。

（1）Jenkins：安装SonarQube Scanner插件（https：//plugins.jenkins.io/sonar），本书使用的版本是2.8。

（2）Jenkins：配置SonarQube Scanner插件，如图5-12所示。

![](https://pic.imgdb.cn/item/60d8490a5132923bf8c31866.jpg)

（3）SonarQube：设置Webhooks。不同代码规模的源码，分析过程的耗时是不一样的。所以，当分析完成时，由SonarQube主动通知Jenkins。设置方法就是进入SonarQube的Adminstration→Configuration→Webhooks页，加入＜Jenkins的地址>/sonarqube-webhook/，如图5-13所示。

![](https://pic.imgdb.cn/item/60d849195132923bf8c36a80.jpg)

​	＜Jenkins的地址>/sonarqube-webhook/接口由Jenkins SonarQube插件提供。

（4）在Jenkinsfile中加入SonarQube的stage。

![](https://pic.imgdb.cn/item/60d849305132923bf8c3f14e.jpg)

​	withSonarQubeEnv是一个环境变量包装器，读取的是我们在图5-12中所配置的变量。在它的闭包内，我们可以使用以下变量。

*  SONAR_HOST_URL：SonarQube服务的地址。
*  SONAR_AUTH_TOKEN：SonarQube认证所需要的Token。



​	waitForQualityGate 步骤告诉 Jenkins 等待 SonarQube 返回的分析结果。当它的abortPipeline参数为true时，代表当质量不合格时，将pipeline的状态设置为UNSTABLE。

​	我们同时使用了timeout包装器来设置waitForQualityGate步骤的超时时间，避免当网络出问题时，Jenkins任务一直处于等待状态。

（5）设置Quality Gates（质量阈值）。在SonarQube的“Quality Gates”下，我们可以看到系统自带的质量阈值，如图5-14所示。可以看出它是针对新代码的。所以，在初次及没有新代码加入的情况下，执行代码分析是不会报出构建失败的。

![](https://pic.imgdb.cn/item/60d8496e5132923bf8c54570.jpg)

### 使用SonarQube Scanner实现代码扫描

​	上文中，我们是使用Maven插件实现代码扫描的，也就是利用构建工具本身提供的插件来实现。在构建工具本身不支持的情况下，我们使用SonarQube本身提供的扫描工具（Scanner）进行代码扫描。

​	具体步骤如下：

（1）在安装SonarQube Scanner插件后，设置扫描工具自动下载并安装（推荐），如图5-15所示。

![](https://pic.imgdb.cn/item/60d8498a5132923bf8c5e571.jpg)

也可以取消自动安装，改成手动安装后指定目录，如图5-16所示。

![](https://pic.imgdb.cn/item/60d849965132923bf8c627f3.jpg)

​	请注意，这里的Name值与图5-12中所设置的值是两码事。此处设置的是SonarScanner工具本身的名称与路径
（2）在代码项目根目录下放入sonar-project.properties文件，sonar-scanner会读取其配置，内容如下：

![](https://pic.imgdb.cn/item/60d849aa5132923bf8c6975a.jpg)

（3）pipeline部分代码如下：

![](https://pic.imgdb.cn/item/60d849b65132923bf8c6d53a.jpg)

### SonarQube集成p3c

​	前文中，我们已经交待，必须在所有做代码规范检查的地方使用同一套规范。而SonarQube默认使用的是它自带的规范（SonarQube称为规则），所以也需要设置SonarQube使用p3c的规范。

​	有好心的朋友开源了SonarQube的p3c PMD插件（https：//github.com/mrprince/sonar-p3c-pmd），我们可以拿来直接使用。

具体步骤如下：

（1）从GitHub下载p3c PMD插件，编译打包。

2）将上一步打包好的JAR包放到SonarQube所在服务器的＜SonarQube的home目录>/ext ensions/plugins目录下。

（3）SonarQube：创建p3c profile。单击SonarQube顶部的“QualityProfiles”，然后单击页面右上角的“Create”按钮，输入新profile名称，选择Java语言，如图5-17所示。

（4）SonarQube：在profile列表中找到刚刚创建的p3c profile，单击其最右边的下三角按钮，选择“Set as Default”，如图5-18所示。

创建p3c profile成功，如图5-19所示。

![](https://pic.imgdb.cn/item/60d849f25132923bf8c8166c.jpg)

![](https://pic.imgdb.cn/item/60d849fb5132923bf8c84c18.jpg)

​	（5）SonarQube：为p3c profile激活p3c规则。新创建的profile是没有激活任何规则的，需要手动激活。单击下三角按钮，选择“Activate More Rules”，如图5-20所示。

（6）跳转到激活页面，激活所有的p3c规则，如图5-21所示。

这样，当SonarQube分析Java代码时，就会使用p3c规则了。

![](https://pic.imgdb.cn/item/60d84a115132923bf8c8c151.jpg)

### 将分析报告推送到GitLab

​	如果希望对每一次代码的commit都进行分析，并将分析结果与该commit关联起来，那么SonarQube的GitLab插件就是一个不错的选择。SonarQube GitLab插件的功能就是将SonarQube的分析结果推送到GitLab。

​	（1）在SonarQube上安装GitLab插件（https：//github.com/gabrie-allaigre/sonar-gitlab-plugin），如图5-22所示。

![](https://pic.imgdb.cn/item/60d84a295132923bf8c94309.jpg)

​	如果因为网络原因安装失败，则可进行手动安装。

（2）配置SonarQube GitLab插件，如图5-23所示。

![](https://pic.imgdb.cn/item/60d84a3c5132923bf8c9ab45.jpg)

置好SonarQube GitLab插件后，需要为sonar-scanner添加几个参数，以告诉SonarQube将分析结果关联到GitLab的相应commit上。

![](https://pic.imgdb.cn/item/60d84a4b5132923bf8c9f8e8.jpg)

* -Dsonar.analysis.mode：分析报告模式，值为preview，代表将结果推送到GitLab。此参数虽然官方标注SonarQube 6.6后被废弃，但是笔者使用6.7版本依然需要加上它。

* -Dsonar.gitlab.ref_name：分支名称。
* -Dsonar.gitlab.project_id：GitLab对应的项目路径。
* -Dsonar.projectName：对应SonarQube上的项目名称。
* -Dsonar.gitlab.commit_sha：代码的commit ID。

当SonarQube分析完成后，我们就可以在GitLab的相应commit页面上的代码行内或commit评论区看到分析结果了，如图5-24所示。

分析结果是显示在行内还是评论区，由SonarQube GitLab插件的配置决定。关于该插件的更多参数本书就不做更多介绍了。

## Allure测试报告：更美观的测试报告

### Allure测试报告介绍

​	是不是觉得JUnit输出的测试报告不美观。不只是JUnit，很多其他编程语言的测试框架的测试报告也差不多。Allure测试报告是一个框架，能将各种测试报告更美观地呈现出来。

###  集成Allure、Maven、Jenkins

​	接下来，我们将Allure、Maven、Jenkins集成。Allure与其他编程语言及构建工具的集成与此类似。

具体步骤如下：

（1）安装Allure Jenkins插件（https：//plugins.jenkins.io/allure-jenkins-plugin），进入Jenkins的Manage Jenkins→Global ToolConfiguration→Allure Commandline页，配置Allure自动下载并安装的版本，如图5-25所示。

![](https://pic.imgdb.cn/item/60d84abb5132923bf8cc587c.jpg)

（2）在pom.xml文件中加入依赖。

![](https://pic.imgdb.cn/item/60d84ac85132923bf8cc9fcf.jpg)

（3）在pom.xml文件中加入Allure插件（https：//github.com/allure-framework/allure-maven）。

![](https://pic.imgdb.cn/item/60d84ad95132923bf8ccf8b9.jpg)

（4）在Jenkinsfile中的post阶段加入allure步骤。

![](https://pic.imgdb.cn/item/60d84ae75132923bf8cd41be.jpg)

构建完成后，我们看到在构建历史记录中出现了Allure的logo，如图5-26所示。单击Allure的logo，就可以进入优美的测试报告页面了，如图5-27所示。Allure测试报告是不是美观了很多？不要小看这点视觉上的改善，它可能会让你的领导对你刮目相看。

![](https://pic.imgdb.cn/item/60d84af75132923bf8cd9612.jpg)

![](https://pic.imgdb.cn/item/60d84b005132923bf8cdc6a9.jpg)

## 当我们谈质量时，谈的是什么

​	质量是什么？温伯格（Gerald M.Weinberg）在《质量·软件·管理（第1卷）》中给出了一个可操作性很强的定义：

​	质量就是对某个（某些）人而言的价值。

​	回到工作中，我们在谈质量前，是不是应该先讨论质量是对“谁”而言的，再谈如何提高质量。

## 总结

略

# 触发pipeline执行

##  什么是触发条件

​	前文中，我们都是在推送代码后，再切换到Jenkins界面，手动触发构建的。显然，这不够“自动化”。自动化是指pipeline按照一定的规则自动执行。而这些规则被称为pipeline触发条件。

​	对于pipeline触发条件，笔者从两个维度来区分：时间触发和事件触发。

## 时间触发

​	时间触发是指定义一个时间，时间到了就触发pipeline执行。在Jenkins pipeline中使用trigger指令来定义时间触发。

​	tigger指令只能被定义在pipeline块下，Jenkins内置支持cron、pollSCM，upstream三种方式。其他方式可以通过插件来实现。

### 定时执行：cron

​	定时执行就像cronjob，一到时间点就执行。它的使用场景通常是执行一些周期性的job，如每夜构建。

![](https://pic.imgdb.cn/item/60d883985132923bf808a36c.jpg)

​	Jenkins trigger cron语法采用的是UNIX cron语法（有些细微的区别）。一条cron包含5个字段，使用空格或Tab分隔，格式为：MINUTE HOUR DOMMONTH DOW。每个字段的含义为：

* MINUTE：一小时内的分钟，取值范围为0∼59。
* HOUR：一天内的小时，取值范围为0∼23。
* DOM：一个月的某一天，取值范围为1∼31。
* MONTH：月份，取值范围为1∼12。
* DOW：星期几，取值范围为0∼7。0和7代表星期天。

还可以使用以下特殊字符，一次性指定多个值。

* *：匹配所有的值
* M-N：匹配M 到N 之间的值。
* M-N/X or*/X：指定在M 到N 范围内，以X值为步长。
* A，B，· · ·，Z：使用逗号枚举多个值。



​	在一些大型组织中，会同时存在大量的同一时刻执行的定时任务，比如N 个半夜零点（0 0***）执行的任务。这样会产生负载不均衡。在Jenkins trigger cron语法中使用“H”字符来解决这一问题，H代表hash。对于没必要准确到零点0分执行的任务，cron可以这样写：H 0***，代表在零点0分至零点59分之间任何一个时间点执行。



​	需要注意的是，H应用在DOM（一个月的某一天）字段时会有不准确的情况，因为10月有31天，而2月却是28天。

​	Jenkins trigger cron还设计了一些人性化的别名：@yearly、@annually、@monthly、@weekly、@daily、@midnight和@hourly。例如，@hourly与H****相同，代表一小时内的任何时间；@midnight实际上代表在半夜12：00到凌晨2：59之间的某个时间。其他别名很少有应用场景。

### 轮询代码仓库：pollSCM

​	轮询代码仓库是指定期到代码仓库询问代码是否有变化，如果有变化就执行。有读者会问：那多久轮询一次？笔者的回答是：越频繁越好。因为构建的间隔时间越长，在一次构建内就可能会包含多次代码提交。当构建失败时，你无法马上知道是哪一次代码提交导致了构建失败。总之，越不频繁集成，得到的“持续集成”的好处就越少。笔者通常会在Jenkinsfile中这样写：

![](https://pic.imgdb.cn/item/60d884115132923bf80c4b28.jpg)

​	事实上，如果代码有变化，最好的方式是代码仓库主动通知Jenkins，而不是Jenkins频繁去代码仓库检查。那这种方式存在的意义是什么？

​	在一些特殊情况下，比如外网的代码仓库无法调用内网的Jenkins，或者反过来，则会采用这种方式。

