# 2. Hadoop大数据关键技术

## 2.2 大数据采集技术

数据采集可以分为内部采集与外部采集两个方面。

​	（1）离线数据采集技术，首先要是基于文件的数据采集系统、日志收集系统等，代表性的工具有Facebook公司开发的Scribe、Cloudera公司开发的Flume和Apache基金会支持的Chukwa等；其次是基于数据库和表的数据采集技术，基于数据库的数据采集系统中代表性工具有GoldenGate公司的TMD、迪思杰公司而数据采集软件、IBM公司的CDC（InfoSphere Change Data Capture，CDC）、MySQL支持的Binlog采集工具等；在基于表的批量抽取软件中，广泛应用的是Sqoop和其他ETL工具。

​	（2）在线数据采集技术，主要是基于消息的采集、数据流采集等。基于消息采集的技术，如性能数据采集等，代表性的产品有Linkedin的Kafka，以及开源的ActiveMQ、RabbitMQ、RocketMQ等。基于数据流的采集技术，如信令数据采集等，代表性的产品有IBM StreamBase、Twitter公司的Storm等。这些工具或组件一般会根据场景选择压缩算法。

​	外部数据采集主要是指互联网数据的采集，相关技术主要分为两类。

​	（1）网络爬虫类，即按照一定的规则，自动抓取互联网信息的程序框架，例如，用于搜索引擎的网络爬虫属于通用网络爬虫，商用的代表性产品有Google、Baidu公司开发的系统等，其网络搜索技术已经非常成熟，但是并不对外开放技术。开源的技术有Apache Nutch、Scrapy、Heritrix、WebMagic、WebCollector等网络爬虫框架。

​	（2）开放API类，即数据源提供者开放的数据采集接口，可以用来获取限定的数据。在外部数据中，除了互联网数据采集技术，也有基于传感器应用的采集技术，这种技术在物联网中用得较多。此外，还有电信公司特有的探针技术，例如，我们在打电话、利用手机上网时，电信公司的路由器、交换机等设备中都会有数据交换，探针就是从这些设备上采集数据的技术。

### 2.2.1 结构化数据采集工具

​	在Hadoop大数据应用生态系统中，Sqoop作为Apache的顶级项目，主要用来在Hadoop和关系数据库之间传递数据。通过Sqoop可以方便地将数据从关系数据库导入HDFS、HBase或Hive中，或者将数据从HDFS导出到关系数据库中。图2-2是Sqoop系统架构示意图。

![](https://pic.imgdb.cn/item/61ee02072ab3f51d91e3c494.jpg)

### 2.2.2 日志收集工具与技术

**1. 日志收集**

​	日志收集模块需要使用一个分布式的、具有高可靠性和高可用性、能够处理海量日志数据的框架，并且应该能够支持多源采集和集中存储。目前常用的开源日志收集系统有Flume、Scribe等。Flume是由Cloudera开发的一个分布式、高可靠性和高可用性的海量日志收集系统，支持在系统中定制各类数据发送方，用于收集数据；同时，Flume也可对数据进行简单处理，并写入各种数据接收方（可定制）。

​	Flume传输数据的基本单位是事件（Event），如果是文本文件，则通常是一行记录，这也是事件的基本单位。事件（Event）从源（Source）传输到通道（Channel），再从通道传输到目的地（Sink），事件本身是一个字节数组，并可携带消息头（Headers）信息。

![](https://pic.imgdb.cn/item/61ee034a2ab3f51d91e4d66c.jpg)

**2. 数据分发工具Kafka**

​	Flume收集的数据和进行日志处理的系统之间可能存在多对多的关系，为了解耦和保证数据的传输延迟，可以选用Kafka作为消息中间层进行日志中转分发。Flume发送源数据流的速度不太稳定，有时快有时慢，当Flume的数据流发送速度过快时（这种情况很常见），会导致下游的消费系统来不及处理，这样可能会丢弃一部分数据。Kafka在这两者之间可以扮演一个缓存的角色，而且数据是写入到磁盘上的，可保证在系统正常启动/关闭时不会丢失数据。

![](https://pic.imgdb.cn/item/61ee03782ab3f51d91e4fec9.jpg)

（1）Topic：消息的基本单位。一个Topic可以看成一类消息，Kafka在保存消息时会按照Topic进行归类，每个Topic具体的数据存放位置由配置文件来决定，可能会存放到不同的分区（Partition）上。每条消息在文件中的位置称为偏移量（Offset），偏移量是一个数据类型为长整型（long）的数字。

（2）Broker：Kafka集群的基本单位，Kafka集群中包含多个Broker。Kafka集群中可能存在一个或多个代理（Broker）服务器，负责Producer和Consumer二者之间的消息处理与交互。

（3）Producer：生产者，生产（发布）Topic的进程。

（4）Consumer：消费者，消费（订阅）Topic的进程，同时若干个消费者还可以组成一个消费组，这样当生产者发布Topic时可以实现以下两种常用功能。

![](https://pic.imgdb.cn/item/61ee03c92ab3f51d91e53e4a.jpg)

## 2.3 大数据存储技术

### 2.3.1 相关概念

**1. 列存储**

​	传统的关系型数据库行存储（Row Storage）的方式，存储的下一个对象是同条记录的下一个属性通常采用。传统的行存储数据排列方式如表2-1所示，传统的关系型数据库，如DB2、Oracle、Sybase、SQLServer、Greenplum、Netezza和Teradata等都采用行存储。

​	（1）大数据应用往往需要批量访问列数据（当用户主要关心同一属性的统计特性时），这时列存储方式的优势就会体现出来，列存储方式对属性的访问比行存储方式快很多，据有关报道，它的读取速度比行存储方式要快50～100倍。

​	（2）有利于提高数据的压缩比，同类数据存储在一起有助于提高数据之间的相关性，从而有利于实施高效压缩算法（如行程压缩算法等）。

**表2-3　行存储方式与列存储方式的比较**

![](https://pic.imgdb.cn/item/61ee04702ab3f51d91e5c9bf.jpg)

**2. Key-Value存储**

​	Google在其分布式数据库技术产品BigTable中，为了存储Web页面，创造性地提出了Key-Value这种Map数据结构，并广泛应用到Google的多种应用中。

![](https://pic.imgdb.cn/item/61ee049d2ab3f51d91e5eecb.jpg)

​	Key-Value数据结构本质上就是一个映射，Key是查找数据地址的唯一关键字，而Value则是实际存储的内容。Key-Value数据结构使用哈希函数实现关键字到值的快速映射，这种数据结构可以提高数据的存储能力和并发读写能力，适合通过主键快速查询。

**3. NoSQL（Not only SQL）数据库**

​	NoSQL数据库泛指非关系型的数据库，其兴起的原因是传统的关系型数据库应对大规模、高并发数据的能力有限，NoSQL数据库能够弥补传统的关系型数据库在这方面的不足。相比于传统的关系型数据库，以云平台为基础的NoSQL数据库系统具有以下特点：

* NoSQL数据库去掉了传统的关系型数据库的关系特征，易于扩展；
* 由于NoSQL数据库结构简单，所以在大数据中的读写性能很好；
* NoSQL数据库可以随时存储自定义的数据格式；
* NoSQL数据库可以方便地实现高可用性的架构。

常见的几类NoSQL数据库如下。

（1）KV存储数据库（如Memcached、Redis）。这类NoSQL数据库在互联网中应用范围最广。Memcached提供具备LRU淘汰策略的KV内存存储；而Redis提供支持复杂结构（如List、Hash等）的内存及持久化存储。Redis适用于数据变化快且数据库大小可预见（适合内存容量）的应用程序，如股票价格、数据分析、实时数据收集、实时通信。

（2）列存储型数据库（如HBase、Cassandra）。HBase是基于列存储方式的分布式数据库集群系统。由于列存储方式以列为单位来存储数据，适合对某一列进行随机查询处理。采用列存储方式的数据库具有高扩展性，即使数据增加也不会降低处理速度，因此，采用列存储方式的数据库主要适合应用于需要处理大量数据的情况。

​	HBase是Hadoop大数据应用生态系统中的重要一员，实现了对海量数据的随机实时读写访问。从逻辑上讲，HBase将数据按照表、行和列进行存储。HBase的主要目标是依靠横向扩展，通过不断增加廉价的商用服务器来增加计算和存储能力。HBase提供了命令行管理和丰富的API接口，通过调用这些接口，可以使用多种程序语言对HBase进行访问。

​	HBase适用于偏好BigTable，并且需要对大数据进行随机实时访问的场合，如Facebook的消息数据库。HBase是一个写快读慢的系统（当然，这里的慢是相对于写而言的）。对于读数据比较多的情况，可对HBase进行读优化，主要方法是增强系统的IO能力（HDFS层面）、增大BlockCache、调整主压缩（MajorCompaction）策略等。若随机读较多，还可以减小BlockSize。

​	（3）文档存储型数据库（如MongoDB、CouchDB）。文档存储数据库不需要定义表结构，存储格式多样化，适合存储非结构化的数据。它可以通过复杂的查询条件来获取数据，是非常容易使用的NoSQL数据库。CouchDB的最佳应用场景是：适用于数据变化较少，执行预定义查询，进行数据统计的应用程序；适用于需要支持数据版本的应用程序，如CRM和CMS系统。

​	MongoDB的最佳应用场景是：需要动态查询；需要使用索引而不是MapReduce功能；对数据库有性能要求；需要使用CouchDB，但因为数据改变太频繁而占满内存的应用程序。

**4. 图存储数据库**

​	图存储数据库是基于图理论构建的，使用节点、属性和边的概念。节点代表实体，属性用来保存与节点相关的信息，边用来表示实体之间的关系。图存储数据库在存储某些数据集时速度非常快，可以把图直接映射到面向对象的应用程序中。

### 2.3.2 分布式存储系统

**1. HDFS**

​	Hadoop的出现解决了传统的单机处理模式受内存、计算能力限制的问题，利用集群的存储和计算能力为海量数据提供可靠的存储和处理。

HDFS设计思路有以下几点。

​	（1）硬件异常是常态。在一个大数据环境下，HDFS集群由大量物理机器构成，每台机器由很多硬件组成，因为某一个硬件异常而使HDFS集群出错的概率是很高的，因此HDFS集群的一个核心设计目标就是能够快速检测硬件异常并快速从异常中恢复工作。

​	（2）访问流式数据。在HDFS集群上运行的应用要求访问流式数据，为适用于批处理而非交互式处理，因此在设计HDFS集群时更加强调高吞吐量而非低延迟。

​	（3）大数据集。在HDFS中，典型的文件大小是GB级甚至TB级的，因此HDFS设计的重点是支持大文件，并且可以通过扩展物理机器的数量来支持更大的集群。

​	（4）简单的一致性模型。HDFS提供的访问模型是一次写入多次读取的模型。文件在完成写入操作后就不需要再修改了，采用这种简单的一致性模型，可以支持更高的吞吐量，以及文件追加。

​	（5）移动计算比移动数据的代价更低。HDFS利用了计算机系统的数据本地化原理，认为数据离CPU越近，性能更高。HDFS提供的接口可以让应用感知到数据的物理存储位置。

​	（6）异构软硬件平台兼容。HDFS集群应该被设计成能够方便地从一个平台迁移到另外一个平台。



​	如图2-7所示，HDFS集群由一个Master（NameNode）和多个Slave（DataNode）组成。

![](https://pic.imgdb.cn/item/61ee075d2ab3f51d91e836c2.jpg)

综合上述的设计假设和架构分析，HDFS特别适合以下场景：

（1）要求顺序访问的场景，如提供流媒体服务等大文件存储。

（2）要求大文件全量访问的场景，如要求对海量数据进行全量访问、OLAP等。

（3）整体预算有限的场景，想利用分布式计算的便利，但又不打算购买昂贵的HPC（高性能计算机群）、高性能小型机等。



但是，HDFS在如下场景中的性能还是不尽如人意。

（1）要求低延迟数据访问的场景。低延迟数据访问意味着要求快速定位数据，如10 ms级的响应，系统若忙于响应此类要求，则有悖于快速返回大量数据的假设。

（2）存在大量小文件的场景。大量小文件将占用大量的块，不仅会造成较大的浪费，对NameNode也是严峻的挑战。Hadoop适用于较大的文件，原因在于Map任务每次会处理一个输入的小文件（FileInputFormat通常是被分割的文件）。如果文件太小（这里指的是小于HDFS的块大小），并且有很多这样的小文件，那么就会增加打开文件的性能开销；同时，大量的小文件也会增加NameNode元数据的存储开销。

（3）多用户进行并发写入的场景。并发写入违背数据一致性模型，数据可能会出现不一致。

（4）要求实时更新的场景。HDFS支持文件追加（Append），但实时更新会降低数据吞吐量，以及增加维护数据一致的模型代价。

**2. 分布式内存文件存储**

​	“内存为王”这句话现在很流行，大数据处理对速度的追求是无止境的。由于内存的速度和磁盘的速度不是一个数量级，同时，内存的价格越来越低、内存的容量越来越大，这就使得数据存储在内存中有了可行性。伴随着这种趋势，大量的基于内存的计算框架也研制出来了，如Spark，就是优秀的基于内存的计算框架。但是，现有的计算框架还面临一些挑战。Tachyon的出现解决了内存中的垃圾回收（Garbage Collection，GC）开销大、缓存数据丢失等问题。

​	Tachyon是一个分布式内存文件系统，可以在集群里以访问内存的速度来访问存储在Tachyon里的文件。Tachyon是安装在底层的分布式文件存储和上层的各种计算框架之间的一种中间件，主要职责是将那些不需要存储到HDFS中的文件存储到分布式内存文件系统中，以此实现共享内存，从而大幅提高访问效率。Tachyon可以在不同的计算框架内共享内存，同时可以减少内存冗余和基于JVM（Java虚拟机）内存计算框架的GC时间。

​	Tachyon采用传统的主从结构，和Hadoop类似。在Tachyon中，Master里的WorkflowManager是Master进程，为了防止单点问题，可以部署多台StandbyMaster（备用主机）。Slave是由Worker Daemon（工作守护进程）和Ramdisk（内存盘）构成的，Worker Daemon是基于JVM的，Ramdisk是一个Off HeapMemory（堆外内存）。Master和Worker之间的通信协议是Thrift。

![](https://pic.imgdb.cn/item/61ee08ac2ab3f51d91e939ad.jpg)

### 2.3.3 数据库（HBase）与数据仓库（Hive）

**1. HBase**

​	HBase（Hadoop Database）是一个高可靠、高性能、基于列存储方式、可伸缩的分布式存储系统，利用HBase技术可在廉价计算机上搭建起大规模的结构化存储集群。

![](https://pic.imgdb.cn/item/61ee08d72ab3f51d91e95ae8.jpg)

**表2-4　HBase集群中各组件中的功能说明**

![](https://pic.imgdb.cn/item/61ee090d2ab3f51d91e98a84.jpg)

​	（1）HBase的查询过程。在HDFS中，HBase上的数据是以HFlie二进制的形式存储在Block中的，所以对于HDFS来说，HBase是完全透明的。HBase的数据访问流程如图2-10所示。

![](https://pic.imgdb.cn/item/61ee09392ab3f51d91e9abbd.jpg)

​	HBase的响应速度快是因为其特殊的存储模型和访问机制，HBase中有两张表：Meta表和Root表，Meta表记录了用户的Region信息，包含了多个Region及其所在的RegionServer服务器地址，Root表则记录了Meta表的Region信息。因此，Root只有一个Region。图2-10中，客户端可以快速定位到要查找的数据所在的Region Server。当要对HBase进行增删改查等数据操作时，HBase的客户端首先访问分布式协调服务器ZooKeeper，通过ZooKeeper可以访问Root表的地址，因为Root表里面记录了Meta表的地址，通过Meta表就可以找到数据所在的位置，并将数据操作命令发送给RegionServer，该RegionServer接收并执行该命令从而完成本次数据操作。

​	（2）基于HBase的二级索引机制。HBase具有扩展性强、实时查询效率高等特点，在大数据实时处理中应用十分广泛。但是，因为HBase的Key-Value存储特性，所以只支持少量的SQL查询操作，不支持二级索引。对于非主键的查询，只能通过全表扫描和过滤的方式获取数据，效率非常低，使得HBase存在很大的局限性。即使通过Hive、Pig等组件对全表进行MapReduce计算，依然会占用大量的资源，也会大大增加延迟。

​	如果在HBase存储的基础上对表中一列的Value进行索引，而主键RowKey作为该索引的值，则通过对Value的索引可快速定位到符合要求的RowKey，再通过RowKey进行二次查找即可将结果数据取出来。虽然这种方式会损失部分查询效率，但能保证实时性，而且可以极大地提高查询的便捷性，这就是HBase二次索引机制的基本思想。

​	（3）ITHBase拓展项目。ITHBase（Indexed Transactional HBase）是在HBase0.19.3版本中的第三方带索引的独立拓展项目。在HBase写入数据时，如果MemStore写满后发出写磁盘的请求，则ITHBase会拦截请求并为MemStore中的数据创建索引，索引会在表中以列簇（Column Family，CF）的形式存在，而且ITHBase只支持Region级别的操作。当ITHBase读取数据时，会通过表中的索引列来加速扫描数据。

​	ITHBase对HBase的源码进行了修改拓展，并在其基础上重新设计了HBase中的RegionServer模块，而Client只负责处理逻辑。HBase版本更新迅速，但ITHBase的源码几年未更新，是否具有工业强度的稳定性成为用户选择它的主要障碍。 

（4）Phoenix项目。Phoenix起源于Saleforce社区的一个开源项目，后来发展成为Apache的顶级项目。Phoenix是为了解决HBase SQL查询有限、不支持二级索引等问题开发的一个Java中间层，并提供可嵌入的JDBC（Java DatabaseConnectivity）驱动供客户端使用。通过发送JDBC请求给HBase，Phoenix自定义的HBase协处理器将查询语言转化为多个HBase扫描操作和服务器端过滤，Phoenix带来了更快的开发效率。

​	Phoenix会将一个聚合查询分成多个扫描操作，然后将扫描操作分配给Phoenix自定义的HBase协处理器，进行扫描操作并执行生成JDBC标准的查询结果。这些协处理器可以在服务器中并行工作，从而提高查询性能。平衡地拆分表是Phoenix能否获得高效查询的最重要因素之一，例如，将相等大小的分区平均分配到不同的RegionServer上，表中的数据在各个RegionServer上均匀分布可以保证每一个Phoenix线程处理的数据量相当，这样就可以减少查询的等待时间。

​	Phoenix对于大数据集的查询可以达到毫秒级性能。当查询条件同时存在RowKey主键索引和二级索引时，会自动选择最优的索引。Phoenix维护一个系统表（System Table）作为Scheme元数据的存储，支持对多列进行动态索引的创建、删除和修改，并且不限制列数。当索引可变时，列数越多，写入速度受影响就越大；当索引不可变时，不影响写入速度，且Phoenix提供了对RowKey分析的特性，可以让数据均匀分布在各个RegionServer上。Phoenix还具备其他值得关注的特性，例如，通过客户端（Client）批处理可支持有限的事务、支持版本化的模式仓库、优化扫描等。目前，Phoenix对HBase的支撑比较完善，包括索引更新、增量识别等功能。

**2. Hive**

​	Hive是基于HDFS和MapReduce架构的数据仓库，提供了类似SQL的HiveQL语言来操作结构化数据，其基本原理是将HiveQL语言自动转换成MapReduce任务，从而对Hadoop集群中存储的海量数据进行查询和分析。图2-11所示为Hive的系统架构。

![](https://pic.imgdb.cn/item/61ee0b5a2ab3f51d91eb885f.jpg)

​	Hive支持海量结构化数据分析汇总，可将复杂的MapReduce任务简化为SQL语句，具有灵活的数据存储格式，支持JSON、CSV、TEXTFILE、RCFILE、SEQUENCEFILE几种存储格式。

​	Hive采用HDFS作为文件存储系统。Hive数据库中的所有数据文件都可以存储在HDFS中，Hive所有的数据操作也都是通过HDFS的接口进行的。

​	Hive所有的数据计算都依赖于MapReduce。在进行数据分析时，Hive会将用户提交的HiveQL语句解析成相应的MapReduce任务并提交给MapReduce执行。

​	Hive的MetaStore可用来处理Hive的数据库、表、分区等结构和属性信息，这些信息需要存放在一个关系型数据库（如MySQL）中，从而对MetaStore进行维护和处理。表2-5给出了Hive中各个组件的说明。

**表2-5　Hive中各个组件说明**

![](https://pic.imgdb.cn/item/61ee0b902ab3f51d91ebb0f5.jpg)

* 支持索引，可加快数据查询；
* 支持不同的存储类型，如纯文本文件、HBase中的文件；
* 可将元数据保存在关系型数据库中，大大减少了在查询过程中执行语义检查的时间；
* 可以直接使用存储在HDFS中的数据；内置大量用户函数（UDF）来操作时间、字符串和其他的数据挖掘工具，支持用户扩展UDF函数来完成内置函数无法实现的操作；
* 类似于SQL的查询方式，将SQL查询转换为MapReduce任务后在Hadoop集群上执行，Hive与SQL相似促使其成为Hadoop与其他BI工具结合的理想交集。

## 2.4 分布式计算框架

### 2.4.1 离线计算框架

**1. MapReduce**

​	MapReduce是出现最早、知名度最大的分布式计算框架，主要用于大批量的集群任务。MapReduce致力于解决大规模数据处理的问题，在设计之初就考虑了数据的局部性原理。MapReduce利用局部性原理将整个问题分而治之。MapReduce指的是Map（映射）和Reduce（归约）两种函数，也是计算过程的两个阶段。Map函数对类型为Key-Value的数据进行映射处理，之后交给Reduce函数进行归约处理。简单地说，MapReduce就是将一个规模比较大的任务拆分成多个规模比较小的任务同时并行进行作业，在完成各个规模比较小的任务后，再将它们进行归约聚合，这也是并行分布式计算思想的体现。当一个任务失败时，调度器会挑选另一个节点重新开始作业，从而保证系统的可靠性、容错性。利用该模型，使用者只需关注想要执行的运算逻辑规则，而不必关心分布式执行中的容错、数据和计算分布、负载均衡等复杂的细节，MapReduce框架会自动处理这些问题。图2-12给出了MapReduce的执行流程。

![](https://pic.imgdb.cn/item/61ee0ded2ab3f51d91edb0a0.jpg)

**2. Yarn（MRv2）**

​	Hadoop的发展并没有因为MRv1的不足而停止。从MapReduce 0.23.0版本开始，Hadoop开发团队摒弃了原有框架，从根本上进行了改变。新的MapReduce框架命名为Yarn或MRv2。

**3. Spark**

​	随着Yarn的出现，MapReduce的使用者不需要担心任务的并行性和容错问题，只需要使用一些基本的操作就能并行地读写数据。但是，由于MapReduce框架并没有很好地使用分布式内存，每个MapReduce任务均需要读写磁盘，这使得MapReduce对于某些需要重用中间结果的应用效率很低。在很多迭代式的机器学习和数据挖掘算法中，使用中间结果是非常常见的。如果使用MapReduce框架来处理这类应用，那么数据副本、磁盘I/O和数据序列化将会花费大量时间。因为内存的读写速度远远高于磁盘。在理想状况下，如果所有的工作数据都能放入内存，那么大部分的任务就能在很短的时间内完成。

​	为了避免MapReduce框架中多次读写磁盘的消耗，更充分地利用内存，加州大学伯克利分校AMP Lab提出了一种新的、开源的、类似于MapReduce的内存编程模型—Spark。Spark是基于MapReduce算法实现分布式计算的，拥有MapReduce的优点；不同于MapReduce的是Job Tracker中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等场景。Spark掀起了内存计算之先河，引领了大数据技术的发展。

​	Spark是基于内存的分布式计算框架。在迭代计算的场景下，数据处理过程中的数据都会存储在内存中，从而避免了MapReduce计算框架中的问题。Spark能够使用HDFS，使用户能够快速地从MapReduce切换到Spark，并且提供比MapReduce高10～100倍的性能。作为计算引擎，Spark还支持小批量流式处理、离线批处理、SQL查询、数据挖掘，避免用户在这几类不同的系统中加载同一份数据带来的存储和性能上的开销。Spark能够融入Hadoop的生态系统。

​	Spark也已经结合Hadoop等大数据处理工具发展出了自己的生态系统。Spark的生态系统如图2-13所示，读者可以参考相关文献了解其中涉及的组件，本书将在后面的章节中重点介绍Spark MLlib的应用。

![](https://pic.imgdb.cn/item/61ee14352ab3f51d91f36f92.jpg)

**4. Flink**

​	Flink是一个高效、分布式、基于Java实现的通用大数据分析引擎，它具有类似于MapReduce的高效性、灵活性和扩展性，以及并行数据库查询优化方案，支持批量和基于流式数据的分析，且提供了基于Java和Scala的API。Flink已升级成为Apache基金会的顶级项目，其设计思想主要来源于Hadoop、MPP数据库、流式计算系统等，支持增量迭代计算。

Flink具有如下主要特征：

* 数据集DataSet的API支持Java、Scala和Python等语言；
* 数据流DataStream的API支持Java和Scala等语言；
* 表（Table）的API支持类似于SQL的查询；
* 具有机器学习和图处理（Gelly）的各种库；
* 具有自动优化迭代的功能，如有增量迭代；支持高效序列化和反序列化。



​	Spark和Flink都支持实时计算，且都可基于内存进行计算。Flink对流式计算和迭代计算的支持力度更强。无论Spark还是Flink，它们的发展重点都是将数据和平台API化，除了传统的统计算法，还包括学习算法，同时使其生态系统越来越完善。

![](https://pic.imgdb.cn/item/61ee14f82ab3f51d91f437c2.jpg)

### 2.4.2 实时流计算平台

​	在Storm出现之前，对于需要实现计算的任务，开发者需要手动维护一个消息队列和消息处理者所组成的实时处理网络。消息处理者先从消息队列中获取消息后进行处理，然后更新数据库，发送消息给其他队列，所有这些操作都需要开发者自己实现。

**1. Storm**

​	Storm是Twitter的一个类似于Hadoop的开源实时数据处理框架（原来是由BackType开发的，后BackType被Twitter收购，将Storm作为Twitter的实时数据分析系统）。Storm能处理高频交易数据和大规模数据的实时流计算，可应用于实时搜索、高频交易和社交网络等。金融机构的交易系统是一个非常典型的流计算处理系统，对实时性和一致性有很高的要求。

​	（1）Storm的设计思想。在Storm中也有对于流（Stream）的抽象。Storm将流中元素抽象为Tuple（元组），一个Tuple就是一个值列表（Value List），值列表中的每个Value都有一个Name，并且该Value可以是基本类型、字符类型、字节数组等，当然也可以是其他可序列化的类型。Storm流如图2-15所示。

![](https://pic.imgdb.cn/item/61ee155f2ab3f51d91f49743.jpg)

​	形象地说，Spout就好像是一个个水龙头，并且每个水龙头里流出的水是不同的，我们想用哪种水就拧开哪个水龙头，然后使用管道将水龙头中的水导向到一个处理器（Bolt），经Bolt处理后再使用管道导向另一个处理器或者存入容器中。Spout流的概念如图2-16所示。

![](https://pic.imgdb.cn/item/61ee15922ab3f51d91f4cb8f.jpg)

​	Storm对数据输入的来源和数据输出的去向没有做任何限制，在Storm中，可以使用任意来源的数据输入和任意的数据输出，只要编写对应的代码来获取/写入这些数据即可。在典型的场景下，数据输入的来源和数据输出的去向是类似于Kafka或者ActiveMQ这样的消息队列，也可以是数据库、文件系统或者Web服务。

​	图2-17是Storm的拓扑。拓扑是Storm最高层次的一个抽象概念，它可以被提交到Storm集群执行，一个拓扑就是一个流转换图，图中每个节点都是一个Spout或者Bolt，图中的边表示Bolt订阅了哪些流，当Spout或者Bolt发送元组到流时，Spout就会发送元组到每个订阅了该流的Bolt（只要预先订阅，Spout就会将流发到适当的Bolt上）。

![](https://pic.imgdb.cn/item/61ee15c62ab3f51d91f50383.jpg)

**2. Spark Streaming**

​	Spark Streaming是大规模流式计算并行处理框架，它将流式计算分解成一系列短小的批处理作业。Spark Streaming是在2013年被添加到Spark中的，它可以实时处理来自Kafka、Flume和Amazon Kinesis等多种数据。Spark Streaming提供了一套高效、可容错的准实时大规模流式计算并行处理框架，它能和批处理、即时查询放在同一个软件栈中，这种对不同数据的统一处理能力正是Spark Streaming被迅速采用的关键原因之一。Spark Streaming的用户包括Uber、Netflix和Pinterest等公司。

​	（1）Spark Streaming框架。Spark Streaming将流式计算分解成一系列短小的批处理作业，这里的批处理引擎是Spark，也就是把Spark Streaming的输入数据按照Batch Size分成一段一段的数据，即离散流（Discretized Stream，DStream），每一段DStream都转换成Spark中的弹性分布式数据集（ResilientDistributed Dataset，RDD），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算可以根据业务的需求对中间的结果进行迭加，或者存储到外部设备中。图2-18显示了Spark Streaming流程。

![](https://pic.imgdb.cn/item/61ee16d82ab3f51d91f5fe1b.jpg)

​	相比于传统的处理框架，Kafka+Spark Streaming的框构具有以下几个优点。

​	Spark Streaming框架的高效和低延迟保证了操作的准实时性。利用Spark Streaming框架提供的丰富API和高灵活性，可以简捷地实现较为复杂的算法。

​	编程模型的高度一致使得上手Spark Streaming相当容易，同时也可以保证业务逻辑在实时处理和批处理上的复用。在基于Kafka+Spark Streaming的流量统计应用运行过程中，有时会遇到内存不足、垃圾回收（GC）阻塞等各种问题。下面介绍一下如何对Spark Streaming应用程序进行调优来减少甚至避免这些问题的影响。

## 2.5 数据分析平台与工具

**1. 基础分析工具——Excel、SPSS Statistics、SAS**

​	（1）Excel。作为电子表格软件，Excel适合进行简单的统计（如分组、求和等），由于其方便好用，功能也能满足很多场景需要，所以实际已成为最常用的数据处理软件工具。其缺点在于功能单一，且可处理数据规模小。近几年Excel在大数据方面（如地理可视化和网络关系分析）也进行了一些增强，但应用能力仍然有限。

​	（2）SPSS Statistics。SPSS Statistics作为商业统计软件，可进行经典的统计分析（如回归、方差、因子、多变量等）处理。SPSS Statistics是轻量级的，易于使用，但功能相对较少，适合常规统计分析。

​	（3）SAS。SAS也是商业统计软件，功能丰富而强大（包括绘图能力），且可以通过编程来扩展其分析能力，适合复杂与高要求的统计性分析。目前，有些企业，特别是银行会选择使用SAS进行大数据挖掘和分析。

**2. 基于机器学习的分析工具——SPSS Modeler、MATLAB、Weka**

​	

**3. 基于可视化的大数据分析工具——Tableau、Gephi、NanoCubes**

​	近两年来出现了许多面向大数据、具备可视化能力的分析工具，在商业研究领域，Tableau就是卓越代表。Tableau的优势主要在于支持多种大数据源和格式、可视化图表类型，加上拖曳式的使用方式，非常适合研究人员使用，能够涵盖大部分分析研究的场景。Tableau的应用如图2-19所示。但是，由于Tableau不能提供经典统计和机器学习算法支持，因此它不能代替统计和数据挖掘软件。另外，就实际处理速度而言，当面对较大的数据（超过3000万记录）时，并没有官方介绍得那么迅速。

**4. 基于编程的大数据分析的编程语言——R、Java、Python**

## 2.5.2 机器学习

​	复杂的大数据分析主要依靠机器学习，机器学习包括监督学习、非监督学习、强化学习等，监督学习又包括分类学习、回归学习、排序学习、匹配学习等。

**1. 机器学习基础**

​	机器学习（Machine Learning，ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科，专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识使之不断改善自身的结构和性能。机器学习是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域。机器学习主要使用归纳、综合，而不是演绎。

​	综合考虑各种学习方法出现的历史渊源、知识表示、推理策略、结果评估的相似性、研究人员交流的相对集中性，以及应用领域等因素，可以将机器学习方法分为以下六类。

​	（1）经验性归纳学习：经验性归纳学习（Empirical Inductive Learning）采用一些数据密集的经验方法（如版本空间法、ID3法、定律发现方法）对实例进行归纳学习，其实例和学习结果一般都采用属性、谓词、关系等符号表示，它相当于基于学习策略分类中的归纳学习，但扣除了联接学习、遗传算法、增强学习的部分。

​	（2）分析学习：分析学习（Analytic Learning）方法是从一个或少数几个实例出发，运用领域知识进行分析。其主要特征为：推理策略主要是演绎，而非归纳；使用过去的问题求解经验（实例）指导新的问题求解，或产生能更有效地运用领域知识的搜索控制规则。分析学习的目标是改善系统的性能，而不是新概念的描述，分析学习包括应用解释学习、演绎学习、多级结构组块及宏操作学习等技术。

​	（3）类比学习：它相当于基于学习策略分类中的分类学习。在这一类型的学习中比较引人注目的研究是通过与过去经历的具体实例进行类比来学习，也称为基于范例的学习（Case Based Learning），或简称范例学习。

​	（4）遗传算法：遗传算法（Genetic Algorithm）用于模拟生物繁殖的突变、交换和达尔文的自然选择（在每一生态环境中适者生存），它把问题可能的解编码为一个向量，称为个体，向量的每一个元素称为基因，并利用目标函数（相应于自然选择标准）对群体（个体的集合）中的每一个个体进行评价，根据评价值（适应度）对个体进行选择、交换、变异等遗传操作，从而得到新的群体。遗传算法适用于非常复杂和困难的环境，例如，带有大量噪声和无关数据、事物不断更新、问题目标不能明显和精确地定义，以及通过很长的执行过程才能确定当前行为的价值等。同神经网络一样，遗传算法的研究已经发展为人工智能的一个独立分支，其代表人物为霍勒德（Holland）。

​	（5）联接学习：典型的联接模型实现为人工神经网络，其由称为神经元的一些简单计算单元以及单元间的加权联接组成。近年来，基于卷积神经网络（CNN）、循环神经网络（RNN）的深度学习受到广泛的关注。

​	（6）增强学习：增强学习（Reinforcement Learning）的特点是通过与环境的试探性（Trial And Error）交互来确定和优化动作的选择，以实现所谓的序列决策任务。在这种任务中，学习机制通过选择并执行动作，导致系统状态发生变化，并有可能得到某种强化信号（立即回报），从而实现与环境的交互。强化信号就是对系统行为的一种标量化的奖惩。系统学习的目标是寻找一个合适的动作选择策略，即在任一给定的状态下选择哪种动作的方法，使产生的动作序列可获得某种最优的结果（如累计立即回报最大）。

​	在机器学习分类中，经验性归纳学习、遗传算法、联接学习和增强学习均属于归纳学习，其中经验性归纳学习采用符号表示方式，而遗传算法、联接学习和增强学习则采用亚符号表示方式；分析学习属于演绎学习。

​	分类是最常见的机器学习应用问题，如垃圾邮件过滤、人脸检测、用户画像、文本情感分析、网页归类等，本质上都是分类问题。分类学习也是机器学习领域研究最彻底、使用最广泛的一个分支。最近，Fernández-Delgado等人在JMLR（Journal of Machine Learning Research，机器学习顶级期刊）杂志发表了一篇有趣的论文：他们让179种不同的分类学习方法（分类学习算法）在UCI 121个数据集上进行了“大比武”（UCI是机器学习公用数据集，每个数据集的规模都不大），结果发现Random Forest（随机森林）和SVM（支持向量机）分别名列第一、第二，但两者差异不大；在84.3%的数据上，Random Forest压倒了其他90%的方法；也就是说，在大多数情况下，只用Random Forest或SVM即可。

**2. Mahout算法库**

​	机器学习算法库包括朴素贝叶斯分类器、KNN分类器、SVM、决策树、Boosting、梯度下降Boosted树、随机森林、EM算法、神经网络。

​	Mahout是Apache Software Foundation（ASF）旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便、快捷地创建智能应用程序，并且在Mahout的最近版本中还加入了对Hadoop的支持，使这些算法可以更高效地运行在云计算环境中。Mahout提供机器学习的各种算法，主要包括聚类、分类和协同过滤等算法，表2-6是Mahout算法库的算法列表。

![](https://pic.imgdb.cn/item/61ee3da32ab3f51d9119e957.jpg)

​	Mahout最大的优点就是基于Hadoop实现，它把很多以前运行于单机上的算法，转化为MapReduce模式，大大提升了算法可处理的数据量和处理性能。通过和Hadoop分布式框架相结合，Mahout可以有效地使用分布式系统来实现高性能计算。

**3. Spark MLlib库**

​	Spark MLlib（Machine Learnig lib）是基于Spark的一个机器学习库，它提供了各种各样的算法，这些算法用来在集群上实现分类、回归、聚类、协同过滤等。其中一些算法也可以应用到流数据上，例如使用普通最小二乘法或者K均值算法（还有更多）来计算线性回归。值得注意的是，Mahout已经脱离MapReduce，转而加入Spark MLlib。

​	Spark之所以在机器学习方面具有得天独厚的优势，主要有以下两点原因。

​	（1）机器学习算法一般都有很多个步骤迭代计算的过程，机器学习的计算需要在多次迭代后获得足够小的误差或者足够收敛才会停止。在迭代时如果使用MapReduce计算框架，则每次计算都要读写磁盘，以及启动任务等，这会导致非常大的I/O和CPU消耗。而Spark基于内存的计算模型天生就擅长迭代计算，多个步骤计算直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说，Spark正是机器学习的理想平台。

​	（2）从通信的角度讲，如果使用MapReduce计算框架，由于JobTracker和TaskTracker之间是通过心跳消息的方式来进行通信和传递数据的，因此执行速度非常慢，而Spark具有出色而高效的通信系统，效率很高。

# 4. HDFS安装与基本应用

## 4.1 HDFS概述

### 4.1.2 主要组件与架构

​	HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HDFS是以Master-Slave（主从）模式运行的，其中NameNode、SecondaryNameNode运行在Master上节点，DataNode运行Slave节点上。

![](https://pic.imgdb.cn/item/61ee43e72ab3f51d9120853c.jpg)

## 4.2 HDFS架构分析

### 4.2.1 数据块

​	磁盘数据块是磁盘读写的基本单位，与普通文件系统类似，HDFS也是把文件分块来存储的。HDFS默认数据块大小为64 MB，而磁盘数据块一般为512 B。HDFS的数据块为何如此之大呢？增大数据块可以减少寻址时间与数据传输时间的比例，若寻址时间为10 ms，磁盘的数据传输速率为100 MB/s，那么寻址时间与传输时间比约为1%。当然，磁盘数据块太大也不好，因为一个MapReduce通常以一个数据块作为输入，数据块过大会导致整体任务数量过小，降低作业处理速度。

HDFS按数据块存储还有如下好处。

（1）文件可以任意大，无须担心单个节点磁盘容量小于文件的情况。

（2）简化了文件子系统的设计，子系统只存储数据块，而文件的元数据则交由其他系统（如NameNode）管理。

（3）有利于备份和提高系统可用性，这得益于以数据块为单位进行备份的机制，HDFS默认的副本数量为3。

（4）有利于负载均衡。

### 4.2.2 NameNode

**1. NameNode中的元数据**

​	当一个客户端请求一个文件或者存储一个文件时，它首先需要知道要到哪个DataNode上去存取，获得这些信息后，客户端再直接和这个DataNode进行交互，而这些信息的维护者就是NameNode。

​	NameNode管理着文件系统的命名空间，它维护文件系统树，以及树中的所有文件和目录，也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。对于实际文件的保存与操作，都是由DataNode负责的。当一个客户端请求文件时，它仅仅是从NameNode中获取文件的元数据，而具体的文件数据传输不需要经过NameNode，是由客户端直接与相应的DataNode进行交互的。

​	NameNode保存的元数据种类有：

* 文件名目录名及它们之间的层级关系；
* 文件目录的所有者及其权限；
* 每个文件的数据块名称及文件有哪些数据块组成。

**2. 元数据的持久化**

​	在NameNode中存放元数据的文件是fsimage。在系统运行期间，所有对元数据的操作都将保存在内存中并被持久化到另一个文件edits中，并且edits文件和fsimage文件会被SecondaryNameNode周期性地合并（合并过程会在SecondaryNameNode中详细介绍）。

​	为了简化系统的设计，Hadoop只有一个NameNode，这也就导致了Hadoop集群的单点故障问题。因此，对NameNode节点的容错尤其重要，Hadoop提供了以下两种机制来解决。

（1）将Hadoop的元数据写入本地文件系统的同时再实时同步到一个远程挂载的网络文件系统（NFS）。

（2）运行一个SecondaryNameNode，它的作用是与NameNode进行交互，定期通过编辑日志文件合并命名空间镜像。当NameNode发生故障时，可通过SecondaryNameNode合并的命名空间镜像副本来恢复。需要注意的是，SecondaryNameNode保存的状态总是滞后于NameNode，所以这种方式难免会丢失部分数据。

### 4.2.3 DataNode

​	DataNode是HDFS中的Worker节点，它负责存储数据块，也负责为客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳消息定期向NameNode发送所存储数据块的列表信息。当对HDFS文件系统进行读写时，NameNode告知客户端每个数据块保存在哪个DataNode上，客户端直接与DataNode进行通信，DataNode还会与其他DataNode通信，用于复制这些数据块以实现冗余。

### 4.2.4 SecondaryNameNode

​	需要注意，SecondaryNameNode并不是NameNode的备份。我们从前面的介绍已经知道，所有HDFS的元数据都保存在NameNode的内存中。在启动NameNode时，它首先会将fsimage加载到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断被持久化到本地edits文件中。

​	SecondaryNameNode的角色就是定期合并edits和fsimage文件，我们来看一下合并的步骤。

（1）合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。

（2）SecondaryNameNode从NameNode请求fsimage和edits文件。

（3）SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。

（4）NameNode从SecondaryNameNode获取合并好的新的fsimage并将旧的替换掉，并用创建的edits.new文件替换掉原来的edits文件。

（5）更新fstime文件中的检查点。

### 4.2.5 数据备份

​	HDFS是通过备份数据块的形式来实现容错的，除了文件的最后一个数据块，其他所有数据块的大小都是一样的。数据块的大小和备份因子都是可以配置的。NameNode负责各个数据块的备份，DataNode会通过心跳消息定期向NameNode发送自己节点上的Block报告，这个报告中包含了DataNode节点上的所有数据块的列表。

### 4.2.6 通信协议

​	HDFS的通信协议都是基于TCP/IP的，一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client协议与NameNode交互。而DataNode则通过DataNode协议与NameNode进行沟通。HDFS的RCP（远程过程调用）对Client协议和DataNode协议做了封装。按照HDFS的设计，NameNode不会主动发起任何请求，只会被动地接收来自客户端或DataNode的请求。

### 4.2.7 可靠性保证

​	HDFS可以允许DataNode失败。DataNode会定期（默认为3 s）向NameNode发送心跳消息，若NameNode在指定时间间隔内没有收到心跳消息，它就认为此节点已经失败。此时，NameNode会把失败节点的数据块（从另外的副本节点获取）备份到另外一个健康的节点，这就保证了集群始终维持指定的副本数。

## 4.3 文件操作过程分析

### 4.3.1 读文件

​	HDFS有一个FileSystem实例，客户端通过调用FileSystem实例的open()方法可以打开系统中要读取的文件。HDFS通过RPC调用NameNode获取数据块的位置信息，对于文件的每一个数据块，NameNode会返回含有该数据块副本的DataNode的节点地址。另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就存储在客户端所在的节点上。

​	HDFS会返回一个FSDataInputStream对象给客户端（Client），由FSDataInputStream类封装的DFSDataInputStream对象负责管理与DataNode、NameNode的I/O，具体过程是：

① 客户端发起读请求。

② 客户端与NameNode得到文件的数据块及位置信息列表。

③ 客户端直接和DataNode交互读取数据。

④ 读取完成关闭连接。

![](https://pic.imgdb.cn/item/61ee4bb32ab3f51d9128f65d.jpg)

### 4.3.2 写文件

​	HDFS有一个DistributeFiledSystem实例，客户端通过调用该实例的create()方法可以创建文件。DistributeFiledSystem实例会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，看看文件是否存在、客户端是否有创建权限等。若通过检查，NameNode则会为创建文件写一条记录到本地磁盘的EditLog；若不通过则会向客户端抛出IOException。创建成功之后DistributeFiledSystem实例会返回一个DFSDataOutputStream对象，客户端由此开始写入数据。

​	同读文件过程一样，由FSDataOutputStream类封装的DFSDataOutputStream对象负责管理与DataNode、NameNode的I/O，具体过程是：

① 客户端在向NameNode请求之前先将文件数据写入本地文件系统的一个临时文件。

② 待临时文件达到数据块大小时开始向NameNode请求DataNode信息。

③ NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址）。

④ 客户端通过得到的信息把创建临时数据块Flush到表中的第一个DataNode。

⑤ 当文件关闭时，NameNode会提交这次文件创建，此时文件在文件系统中可见。

![](https://pic.imgdb.cn/item/61ee4ceb2ab3f51d912a4a8b.jpg)

### 4.3.3 删除文件

​	HDFS中删除文件过程一般需要如下几步。

（1）在开始删除文件时，NameNode只是重命名被删除的文件到“/trash”目录下，因为重命名操作只是元数据的变动，所以整个过程非常快。在“/trash”目录下，文件会被保留一定的时间（可配置，默认是6小时），在这期间，文件可以很容易被恢复，恢复时只需要将文件从“/trash”移出即可。

（2）当指定的时间到达时，NameNode将会把文件从命名空间中删除。

（3）标记删除的数据块释放空间，HDFS文件系统显示空间增加。

